# -*- coding: utf-8 -*-
"""Keras-Embedding-Cnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pox-H0mV2Hs-d7bVQ5vlKV6057MqJwMH
"""

#https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html

from google.colab import drive
import os
drive.mount('/content/gdrive')

TEXT_DATA_DIR = os.path.join('/content/gdrive/My Drive/AI/keras-newsgroup')
print("TEXT_DATA_DIR: ", TEXT_DATA_DIR)

import sys

texts = []  # list of text samples
labels_index = {}  # dictionary mapping label name to numeric id
labels = []  # list of label ids
for name in sorted(os.listdir(TEXT_DATA_DIR)):
    path = os.path.join(TEXT_DATA_DIR, name)
    if os.path.isdir(path):
        label_id = len(labels_index)
        labels_index[name] = label_id
        for fname in sorted(os.listdir(path)):
            if fname.isdigit():
                fpath = os.path.join(path, fname)
                if sys.version_info < (3,):
                    f = open(fpath)
                else:
                    f = open(fpath, encoding='latin-1')
                t = f.read()
                i = t.find('\n\n')  # skip header
                if 0 < i:
                    t = t[i:]
                texts.append(t)
                f.close()
                labels.append(label_id)

print('Found %s texts.' % len(texts))
print('Len: ', len(texts[0]))
textArr = texts[0].split('\n')
print(textArr[0:10])
print('labels:', labels)

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
import numpy as np

MAX_NB_WORDS = 10000
MAX_SEQUENCE_LENGTH = 200
VALIDATION_SPLIT = 0.2 

tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)

word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
labels = to_categorical(np.asarray(labels))
print('Shape of data tensor:', data.shape)
print('Shape of label tensor:', labels.shape)
print('labels[0]', labels[0])

# split the data into a training set and a validation set
indices = np.arange(data.shape[0])
print('indices: ', indices)

np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]

nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])
print('data.shape[0]: ', data.shape[0])
print('VALIDATION_SPLIT: ', VALIDATION_SPLIT)
print('nb_validation_samples: ', nb_validation_samples)

x_train = data[:-nb_validation_samples]
y_train = labels[:-nb_validation_samples]
x_val = data[-nb_validation_samples:]
y_val = labels[-nb_validation_samples:]

print(x_train.shape)
print(y_train.shape)
print(x_val.shape)
print(y_val.shape)

GLOVE_DIR = os.path.join('/content/gdrive/My Drive/AI')
print("GLOVE_DIR: ", GLOVE_DIR)


embeddings_index = {}
f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Found %s word vectors.' % len(embeddings_index))

EMBEDDING_DIM = 100

embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector

from keras.layers import Embedding

embedding_layer = Embedding(len(word_index) + 1,
                            EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=MAX_SEQUENCE_LENGTH,
                            trainable=False)

from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten
from keras.models import Model
from keras.layers import TimeDistributed


sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
print('sequence_input.shape: ', sequence_input.shape)

embedded_sequences = embedding_layer(sequence_input)
print('embedded_sequences.shape: ', embedded_sequences.shape)

x = Conv1D(128, 5, activation='relu')(embedded_sequences)
print('x.shape: ', x.shape)
x = MaxPooling1D(5)(x)
print('m x.shape: ', x.shape)

x = Conv1D(128, 5, activation='relu')(x)
print('x.shape: ', x.shape)
x = MaxPooling1D(5)(x)
print('m x.shape: ', x.shape)

#x = Conv1D(128, 5, activation='relu')(x)
#print('x.shape: ', x.shape)
#x = MaxPooling1D(35)(x)  # global max pooling
#print('m x.shape: ', x.shape)

x = Flatten()(x)
print('f x.shape: ', x.shape)

x = Dense(128, activation='relu')(x)
print('d x.shape: ', x.shape)

preds = Dense(len(labels_index), activation='softmax')(x)
print('preds.shape: ', preds.shape)

model = Model(sequence_input, preds)
model.summary()
model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['acc'])

print('x_train: ', x_train[0:1])
print('y_train: ', y_train[0])
print('x_val: ', x_val[0:1])
print('y_val: ', y_val[0])

# happy learning!
model.fit(x_train, y_train, validation_data=(x_val, y_val),
          epochs=2, batch_size=32)