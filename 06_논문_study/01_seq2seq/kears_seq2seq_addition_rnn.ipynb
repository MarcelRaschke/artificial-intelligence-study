{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kears-seq2seq-addition-rnn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZyJTrAcmih1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "7d6c60b1-dcfe-43b2-ea2f-fe2dc35b2906"
      },
      "source": [
        "#https://github.com/likejazz/jupyter-notebooks/blob/master/deep-learning/addition_rnn.py\n",
        "\n",
        "# %%\n",
        "\"\"\"\n",
        "An implementation of sequence to sequence learning for performing addition\n",
        "<https://github.com/keras-team/keras/blob/master/examples/addition_rnn.py>\n",
        "Input: \"535+61\"\n",
        "Output: \"596\"\n",
        "Padding is handled by using a repeated sentinel character (space)\n",
        "Input may optionally be reversed, shown to increase performance in many tasks in:\n",
        "\"Learning to Execute\"\n",
        "http://arxiv.org/abs/1410.4615\n",
        "and\n",
        "\"Sequence to Sequence Learning with Neural Networks\"\n",
        "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
        "Theoretically it introduces shorter term dependencies between source and target.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, RepeatVector, Dense, Activation, TimeDistributed\n",
        "import numpy as np\n",
        "from six.moves import range\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6UMe8zmmy_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharacterTable(object):\n",
        "    def __init__(self, chars):\n",
        "        \"\"\"Initialize character table.\n",
        "        # Arguments\n",
        "            chars: Characters that can appear in the input.\n",
        "        \"\"\"\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
        "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "\n",
        "    def encode(self, C, num_rows):\n",
        "        \"\"\"One hot encode given string C.\n",
        "        # Arguments\n",
        "            num_rows: Number of rows in the returned one hot encoding. This is\n",
        "                used to keep the # of rows for each data the same.\n",
        "        \"\"\"\n",
        "        x = np.zeros((num_rows, len(self.chars)))\n",
        "        for i, c in enumerate(C):\n",
        "            x[i, self.char_indices[c]] = 1\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, calc_argmax=True):\n",
        "        if calc_argmax:\n",
        "            x = x.argmax(axis=-1)\n",
        "        return ''.join(self.indices_char[x] for x in x)\n",
        "\n",
        "\n",
        "class colors:\n",
        "    ok = '\\033[92m'\n",
        "    fail = '\\033[91m'\n",
        "    close = '\\033[0m'\n",
        "\n",
        "\n",
        "# --"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMr4TsvRmzCM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "e0989775-4752-461c-8b95-b7e589eeb92f"
      },
      "source": [
        "# Parameters for the model and dataset.\n",
        "TRAINING_SIZE = 50000\n",
        "DIGITS = 3\n",
        "REVERSE = True\n",
        "\n",
        "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of int is DIGITS.\n",
        "MAXLEN = DIGITS + 1 + DIGITS\n",
        "\n",
        "# All the numbers, plus sign and space for padding.\n",
        "chars = '0123456789+ '\n",
        "ctable = CharacterTable(chars)\n",
        "\n",
        "questions = []\n",
        "expected = []\n",
        "seen = set()\n",
        "print('Generating data...')\n",
        "while len(questions) < TRAINING_SIZE:\n",
        "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
        "                            for _ in range(np.random.randint(1, DIGITS + 1))))\n",
        "    a, b = f(), f()\n",
        "    # Skip any addition questions we've already seen\n",
        "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
        "    key = tuple(sorted((a, b)))\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    # Pad the data with spaces such that it is always MAXLEN.\n",
        "    q = '{}+{}'.format(a, b)\n",
        "    query = q + ' ' * (MAXLEN - len(q))\n",
        "    ans = str(a + b)\n",
        "    # Answers can be of maximum size DIGITS + 1.\n",
        "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
        "    if REVERSE:\n",
        "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
        "        # space used for padding.)\n",
        "        query = query[::-1]\n",
        "    questions.append(query)\n",
        "    expected.append(ans)\n",
        "print('Total addition questions:', len(questions))\n",
        "\n",
        "print('Vectorization...')\n",
        "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sentence in enumerate(questions):\n",
        "    x[i] = ctable.encode(sentence, MAXLEN)\n",
        "for i, sentence in enumerate(expected):\n",
        "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
        "\n",
        "# Shuffle (x, y) in unison as the later parts of x will almost all be larger digits.\n",
        "indices = np.arange(len(y))\n",
        "np.random.shuffle(indices)\n",
        "x = x[indices]\n",
        "y = y[indices]\n",
        "\n",
        "# Explicitly set apart 10% for validation data that we never train over.\n",
        "split_at = len(x) - len(x) // 10\n",
        "(x_train, x_val) = x[:split_at], x[split_at:]\n",
        "(y_train, y_val) = y[:split_at], y[split_at:]\n",
        "\n",
        "print('Training Data:')\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print('Validation Data:')\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)\n",
        "\n",
        "# --"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating data...\n",
            "Total addition questions: 50000\n",
            "Vectorization...\n",
            "Training Data:\n",
            "(45000, 7, 12)\n",
            "(45000, 4, 12)\n",
            "Validation Data:\n",
            "(5000, 7, 12)\n",
            "(5000, 4, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFbQSxjNmzEl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "78d5eadd-1ca0-4a13-cf0e-ebc238507327"
      },
      "source": [
        "# %%\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "# \"Encode\" the input sequence using an RNN.\n",
        "model.add(LSTM(64, input_shape=(MAXLEN, len(chars))))\n",
        "# As the decoder RNN's input, repeatedly provide with the last hidden state of\n",
        "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
        "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
        "model.add(RepeatVector(DIGITS + 1))\n",
        "model.add(LSTM(32, return_sequences=True))\n",
        "\n",
        "model.add(TimeDistributed(Dense(len(chars))))\n",
        "model.add(Activation('softmax'))\n",
        "# <https://stackoverflow.com/a/46004661/3513266>\n",
        "# from keras.metrics import categorical_accuracy\n",
        "# model.compile(loss='binary_crossentropy',\n",
        "#               optimizer='adam', metrics=[categorical_accuracy])\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 64)                19712     \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 4, 64)             0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 4, 32)             12416     \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 4, 12)             396       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 4, 12)             0         \n",
            "=================================================================\n",
            "Total params: 32,524\n",
            "Trainable params: 32,524\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH3TU8I7mzGt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5464ef72-490c-44d7-a54b-eeae8c246cd4"
      },
      "source": [
        "# %%\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=50,\n",
        "          validation_data=(x_val, y_val))\n",
        "\n",
        "# %%\n",
        "# Select 10 samples from the validation set at random so we can visualize errors.\n",
        "for i in range(10):\n",
        "    ind = np.random.randint(0, len(x_val))\n",
        "    rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
        "    preds = model.predict_classes(rowx)\n",
        "    q = ctable.decode(rowx[0])\n",
        "    correct = ctable.decode(rowy[0])\n",
        "    guess = ctable.decode(preds[0], calc_argmax=False)\n",
        "    print('%5d' % ind, end=': ')\n",
        "    print('Q', q[::-1] if REVERSE else q, end=' ')\n",
        "    print('T', correct, end=' ')\n",
        "    if correct == guess:\n",
        "        print(colors.ok + '☑' + colors.close, end=' ')\n",
        "    else:\n",
        "        print(colors.fail + '☒' + colors.close, end=' ')\n",
        "    print(guess)\n",
        "\n",
        "# %% Print final results.\n",
        "i = 2975\n",
        "output_final = model.predict(np.array([x_val[i]]))\n",
        "\n",
        "print(ctable.decode(x_val[i]),\n",
        "      ctable.decode(y_val[i]),\n",
        "      ctable.decode(output_final[0]))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 45000 samples, validate on 5000 samples\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "45000/45000 [==============================] - 30s 670us/step - loss: 1.8885 - acc: 0.3227 - val_loss: 1.8009 - val_acc: 0.3474\n",
            "Epoch 2/50\n",
            "45000/45000 [==============================] - 24s 540us/step - loss: 1.7598 - acc: 0.3522 - val_loss: 1.7255 - val_acc: 0.3633\n",
            "Epoch 3/50\n",
            "45000/45000 [==============================] - 24s 540us/step - loss: 1.6749 - acc: 0.3778 - val_loss: 1.6308 - val_acc: 0.4017\n",
            "Epoch 4/50\n",
            "45000/45000 [==============================] - 24s 534us/step - loss: 1.5542 - acc: 0.4266 - val_loss: 1.4756 - val_acc: 0.4667\n",
            "Epoch 5/50\n",
            "45000/45000 [==============================] - 25s 547us/step - loss: 1.4254 - acc: 0.4780 - val_loss: 1.3738 - val_acc: 0.4951\n",
            "Epoch 6/50\n",
            "45000/45000 [==============================] - 24s 540us/step - loss: 1.3314 - acc: 0.5122 - val_loss: 1.3000 - val_acc: 0.5276\n",
            "Epoch 7/50\n",
            "45000/45000 [==============================] - 24s 540us/step - loss: 1.2611 - acc: 0.5376 - val_loss: 1.2342 - val_acc: 0.5462\n",
            "Epoch 8/50\n",
            "45000/45000 [==============================] - 24s 542us/step - loss: 1.1957 - acc: 0.5633 - val_loss: 1.1663 - val_acc: 0.5771\n",
            "Epoch 9/50\n",
            "45000/45000 [==============================] - 25s 559us/step - loss: 1.1345 - acc: 0.5861 - val_loss: 1.1110 - val_acc: 0.5969\n",
            "Epoch 10/50\n",
            "45000/45000 [==============================] - 25s 547us/step - loss: 1.0759 - acc: 0.6079 - val_loss: 1.0550 - val_acc: 0.6159\n",
            "Epoch 11/50\n",
            "45000/45000 [==============================] - 25s 562us/step - loss: 1.0276 - acc: 0.6252 - val_loss: 1.0089 - val_acc: 0.6294\n",
            "Epoch 12/50\n",
            "45000/45000 [==============================] - 25s 545us/step - loss: 0.9817 - acc: 0.6423 - val_loss: 0.9723 - val_acc: 0.6432\n",
            "Epoch 13/50\n",
            "45000/45000 [==============================] - 25s 552us/step - loss: 0.9423 - acc: 0.6568 - val_loss: 0.9306 - val_acc: 0.6610\n",
            "Epoch 14/50\n",
            "45000/45000 [==============================] - 24s 542us/step - loss: 0.9030 - acc: 0.6714 - val_loss: 0.8911 - val_acc: 0.6753\n",
            "Epoch 15/50\n",
            "45000/45000 [==============================] - 24s 543us/step - loss: 0.8679 - acc: 0.6836 - val_loss: 0.8488 - val_acc: 0.6944\n",
            "Epoch 16/50\n",
            "45000/45000 [==============================] - 25s 551us/step - loss: 0.8317 - acc: 0.6971 - val_loss: 0.8237 - val_acc: 0.7006\n",
            "Epoch 17/50\n",
            "45000/45000 [==============================] - 25s 550us/step - loss: 0.7974 - acc: 0.7069 - val_loss: 0.7941 - val_acc: 0.7049\n",
            "Epoch 18/50\n",
            "45000/45000 [==============================] - 24s 542us/step - loss: 0.7604 - acc: 0.7184 - val_loss: 0.7702 - val_acc: 0.7116\n",
            "Epoch 19/50\n",
            "45000/45000 [==============================] - 25s 552us/step - loss: 0.7222 - acc: 0.7301 - val_loss: 0.7130 - val_acc: 0.7291\n",
            "Epoch 20/50\n",
            "45000/45000 [==============================] - 24s 541us/step - loss: 0.6799 - acc: 0.7437 - val_loss: 0.6600 - val_acc: 0.7510\n",
            "Epoch 21/50\n",
            "45000/45000 [==============================] - 24s 525us/step - loss: 0.6323 - acc: 0.7604 - val_loss: 0.6212 - val_acc: 0.7627\n",
            "Epoch 22/50\n",
            "45000/45000 [==============================] - 24s 538us/step - loss: 0.5866 - acc: 0.7782 - val_loss: 0.5816 - val_acc: 0.7780\n",
            "Epoch 23/50\n",
            "45000/45000 [==============================] - 24s 542us/step - loss: 0.5470 - acc: 0.7935 - val_loss: 0.5275 - val_acc: 0.8002\n",
            "Epoch 24/50\n",
            "45000/45000 [==============================] - 24s 541us/step - loss: 0.5061 - acc: 0.8116 - val_loss: 0.4916 - val_acc: 0.8158\n",
            "Epoch 25/50\n",
            "45000/45000 [==============================] - 24s 540us/step - loss: 0.4681 - acc: 0.8281 - val_loss: 0.4648 - val_acc: 0.8286\n",
            "Epoch 26/50\n",
            "45000/45000 [==============================] - 24s 539us/step - loss: 0.4325 - acc: 0.8453 - val_loss: 0.4257 - val_acc: 0.8434\n",
            "Epoch 27/50\n",
            "45000/45000 [==============================] - 25s 548us/step - loss: 0.3938 - acc: 0.8637 - val_loss: 0.3987 - val_acc: 0.8585\n",
            "Epoch 28/50\n",
            "45000/45000 [==============================] - 25s 545us/step - loss: 0.3555 - acc: 0.8812 - val_loss: 0.3496 - val_acc: 0.8830\n",
            "Epoch 29/50\n",
            "45000/45000 [==============================] - 24s 534us/step - loss: 0.3156 - acc: 0.8989 - val_loss: 0.3105 - val_acc: 0.9006\n",
            "Epoch 30/50\n",
            "45000/45000 [==============================] - 25s 552us/step - loss: 0.2738 - acc: 0.9167 - val_loss: 0.2756 - val_acc: 0.9121\n",
            "Epoch 31/50\n",
            "45000/45000 [==============================] - 25s 545us/step - loss: 0.2357 - acc: 0.9333 - val_loss: 0.2286 - val_acc: 0.9346\n",
            "Epoch 32/50\n",
            "45000/45000 [==============================] - 25s 552us/step - loss: 0.2079 - acc: 0.9429 - val_loss: 0.2263 - val_acc: 0.9335\n",
            "Epoch 33/50\n",
            "45000/45000 [==============================] - 24s 536us/step - loss: 0.1799 - acc: 0.9534 - val_loss: 0.1855 - val_acc: 0.9494\n",
            "Epoch 34/50\n",
            "45000/45000 [==============================] - 25s 552us/step - loss: 0.1605 - acc: 0.9600 - val_loss: 0.1736 - val_acc: 0.9522\n",
            "Epoch 35/50\n",
            "45000/45000 [==============================] - 25s 552us/step - loss: 0.1447 - acc: 0.9642 - val_loss: 0.1568 - val_acc: 0.9576\n",
            "Epoch 36/50\n",
            "45000/45000 [==============================] - 25s 551us/step - loss: 0.1255 - acc: 0.9713 - val_loss: 0.1429 - val_acc: 0.9619\n",
            "Epoch 37/50\n",
            "45000/45000 [==============================] - 25s 555us/step - loss: 0.1201 - acc: 0.9711 - val_loss: 0.1253 - val_acc: 0.9669\n",
            "Epoch 38/50\n",
            "45000/45000 [==============================] - 24s 533us/step - loss: 0.1040 - acc: 0.9764 - val_loss: 0.1199 - val_acc: 0.9661\n",
            "Epoch 39/50\n",
            "45000/45000 [==============================] - 24s 543us/step - loss: 0.0978 - acc: 0.9776 - val_loss: 0.1050 - val_acc: 0.9738\n",
            "Epoch 40/50\n",
            "45000/45000 [==============================] - 25s 550us/step - loss: 0.0855 - acc: 0.9815 - val_loss: 0.0947 - val_acc: 0.9742\n",
            "Epoch 41/50\n",
            "45000/45000 [==============================] - 25s 550us/step - loss: 0.0811 - acc: 0.9816 - val_loss: 0.0907 - val_acc: 0.9764\n",
            "Epoch 42/50\n",
            "45000/45000 [==============================] - 24s 544us/step - loss: 0.0731 - acc: 0.9839 - val_loss: 0.1185 - val_acc: 0.9629\n",
            "Epoch 43/50\n",
            "45000/45000 [==============================] - 24s 537us/step - loss: 0.0720 - acc: 0.9835 - val_loss: 0.0814 - val_acc: 0.9780\n",
            "Epoch 44/50\n",
            "45000/45000 [==============================] - 25s 545us/step - loss: 0.0608 - acc: 0.9871 - val_loss: 0.0749 - val_acc: 0.9788\n",
            "Epoch 45/50\n",
            "45000/45000 [==============================] - 25s 546us/step - loss: 0.0578 - acc: 0.9874 - val_loss: 0.0617 - val_acc: 0.9851\n",
            "Epoch 46/50\n",
            "45000/45000 [==============================] - 25s 546us/step - loss: 0.0650 - acc: 0.9840 - val_loss: 0.0585 - val_acc: 0.9860\n",
            "Epoch 47/50\n",
            "45000/45000 [==============================] - 24s 532us/step - loss: 0.0441 - acc: 0.9920 - val_loss: 0.0680 - val_acc: 0.9815\n",
            "Epoch 48/50\n",
            "45000/45000 [==============================] - 24s 536us/step - loss: 0.0502 - acc: 0.9888 - val_loss: 0.0612 - val_acc: 0.9830\n",
            "Epoch 49/50\n",
            "45000/45000 [==============================] - 25s 547us/step - loss: 0.0506 - acc: 0.9882 - val_loss: 0.0476 - val_acc: 0.9882\n",
            "Epoch 50/50\n",
            "45000/45000 [==============================] - 24s 537us/step - loss: 0.0378 - acc: 0.9929 - val_loss: 0.0441 - val_acc: 0.9892\n",
            " 2929: Q 928+262 T 1190 \u001b[92m☑\u001b[0m 1190\n",
            " 1617: Q 126+5   T 131  \u001b[92m☑\u001b[0m 131 \n",
            " 1453: Q 99+123  T 222  \u001b[92m☑\u001b[0m 222 \n",
            " 3824: Q 275+887 T 1162 \u001b[92m☑\u001b[0m 1162\n",
            "  828: Q 925+9   T 934  \u001b[92m☑\u001b[0m 934 \n",
            " 3780: Q 244+848 T 1092 \u001b[92m☑\u001b[0m 1092\n",
            " 1680: Q 22+870  T 892  \u001b[92m☑\u001b[0m 892 \n",
            " 2098: Q 582+202 T 784  \u001b[92m☑\u001b[0m 784 \n",
            " 4166: Q 538+197 T 735  \u001b[92m☑\u001b[0m 735 \n",
            " 2094: Q 83+465  T 548  \u001b[92m☑\u001b[0m 548 \n",
            " 42+686 710  710 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4ZkhTbMmzKj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "43d68cd6-03b2-4121-acc2-4a4e46b57603"
      },
      "source": [
        "# %%\n",
        "from keras.models import Model\n",
        "\n",
        "intermediate_layer_model = Model(inputs=model.input,\n",
        "                                 outputs=model.layers[4].output)\n",
        "output = intermediate_layer_model.predict(np.array([x_val[i]]))\n",
        "print(output.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.clf()\n",
        "# h = np.reshape(output[0], (-1, 8))\n",
        "# sns.heatmap(h)\n",
        "sns.heatmap(output[0])\n",
        "plt.show()\n",
        "\n",
        "# %% Print weights.\n",
        "names = [weight.name for layer in model.layers for weight in layer.weights]\n",
        "weights = model.get_weights()\n",
        "\n",
        "# suppress scientific notation\n",
        "np.set_printoptions(suppress=True)\n",
        "for name, weight in zip(names, weights):\n",
        "    print(name, weight.shape)\n",
        "    print(weight)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 4, 12)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAD8CAYAAADUv3dIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nV5IaMulKUkMmXUlq6P8AnxCL/4tvNsoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "lstm_1/kernel:0 (12, 256)\n",
            "[[-0.34842145 -0.6015014  -0.28206107 ... -1.4924046  -0.26111946\n",
            "  -0.49554265]\n",
            " [ 0.24461417 -0.6161232  -0.13870609 ... -1.371071    1.3007562\n",
            "   0.36224097]\n",
            " [-1.3327584   0.6924768   0.00528515 ... -0.40645933  0.21285152\n",
            "  -1.0268382 ]\n",
            " ...\n",
            " [-0.17469373  0.39099395 -0.18956205 ...  0.61584127 -0.60158926\n",
            "  -0.1220063 ]\n",
            " [ 0.21798176  0.5702935   0.06164977 ...  0.3464856  -0.6345232\n",
            "  -0.71967953]\n",
            " [-0.26164317  0.73763293  0.5375261  ... -0.20976159 -0.38023445\n",
            "  -0.6704178 ]]\n",
            "lstm_1/recurrent_kernel:0 (64, 256)\n",
            "[[-0.07259123  0.01725388 -0.15761138 ... -0.9583497  -0.16796358\n",
            "  -0.27304217]\n",
            " [ 0.49339336  0.2136172  -0.0417339  ... -0.13079697  0.6654904\n",
            "   0.07428343]\n",
            " [ 0.49809474  0.39910772  0.58097243 ... -0.05935094  0.03431176\n",
            "   0.20428336]\n",
            " ...\n",
            " [-0.19726746  0.03590891  0.223036   ... -0.6504134   0.06130451\n",
            "  -0.03242338]\n",
            " [ 0.5032811   0.32537842  0.614183   ...  0.40874565 -0.06267741\n",
            "   0.06588752]\n",
            " [-0.414325   -0.19861636 -0.14891197 ... -0.20794232 -0.14742874\n",
            "  -0.39059493]]\n",
            "lstm_1/bias:0 (256,)\n",
            "[ 0.04548742  0.3351709   0.01006642  0.23924346  0.11993498  0.37817818\n",
            "  0.1531789   0.33438662  0.29018325 -0.0755607   0.54103875 -0.30951458\n",
            "  0.43362394  0.02147881  0.13256209 -0.1955238  -0.02154175  0.11753283\n",
            " -0.11279559  0.1401447   0.13634562  0.11346066  0.0820205   0.14038172\n",
            "  0.02637159  0.06219678  0.19036768  0.42730513  0.4413077   0.03562014\n",
            "  0.12865596  0.03747944  0.46616092 -0.02278277  0.35219166  0.20424795\n",
            "  0.15258636  0.11984836 -0.13134922 -0.20065406  0.07551105  0.140314\n",
            " -0.08630662  0.17827998 -0.29282707 -0.2911252   0.04541819 -0.06782017\n",
            "  0.15918604  0.2263745   0.44569597  0.23668437  0.14557803 -0.0147362\n",
            "  0.09934042  0.20398921  0.2390028  -0.21542272  0.19534002  0.07958072\n",
            "  0.56513554 -0.02105875  0.17778899  0.22513232  0.29966217  0.9856153\n",
            "  1.0087981   0.8298903   1.0988868   0.97261554  0.8295611   1.3351234\n",
            "  0.89106727  1.1001881   0.94219416  0.49740788  1.1831359   1.030365\n",
            "  0.8095689   0.53650725  0.99311876  0.81707543  0.9829442   1.1708152\n",
            "  0.74087524  1.0328994   1.045352    1.2132624   1.0694082   1.026873\n",
            "  0.7820115   0.8085001   0.6302532   0.9138344   0.90495336  0.9978873\n",
            "  0.97001106  0.9163632   0.95080245  0.8859442   1.071443    0.911974\n",
            "  0.85285515  0.8464859   1.2455056   0.7892277   0.91974413  0.8322363\n",
            "  0.6011184   0.7747708   0.7264783   1.1851974   0.8556718   0.9513261\n",
            "  1.120459    0.9523372   0.7398641   1.188357    1.0774337   0.8873001\n",
            "  1.0959942   0.78186834  0.87625676  1.1542743   0.85839874  0.89345425\n",
            "  0.9890958   1.216737   -0.10950972  0.3391877  -0.07111184  0.17843062\n",
            "  0.06042025  0.11056887 -0.02938914  0.02324991  0.06974158  0.08349852\n",
            "  0.08099341  0.19066699 -0.30024236  0.00463588 -0.16802686 -0.16192731\n",
            "  0.02934982 -0.01803479 -0.10929547 -0.14479136  0.01027685 -0.01609619\n",
            "  0.12121692 -0.00999542 -0.00275691  0.12471175 -0.18684801  0.07294451\n",
            " -0.37769488  0.03536474 -0.17273751  0.04702791 -0.0427837  -0.0507363\n",
            " -0.00036926  0.03542957 -0.01693294  0.04382949 -0.06983746 -0.05606581\n",
            " -0.00960432  0.21732646  0.00663425 -0.12814167 -0.23026931 -0.02552627\n",
            "  0.10770415  0.10340698  0.09115598  0.28932235 -0.04000964 -0.30413002\n",
            "  0.05134343  0.02218289  0.2021121  -0.05006399  0.15644522  0.02054311\n",
            " -0.06432104 -0.05397166 -0.21791288 -0.21819355 -0.12349891  0.06518606\n",
            "  0.5341044   0.21315481 -0.00030615  0.20375879  0.02564058  0.21277496\n",
            "  0.09740363  0.2760902   0.09968164  0.122316    0.42816365  0.05928041\n",
            "  0.06922116 -0.00802217  0.28443214  0.3032074   0.07207128 -0.14170733\n",
            " -0.03941375  0.18431199  0.21994892 -0.06314216  0.49768615  0.04646955\n",
            "  0.04428806 -0.05701989 -0.00507991  0.2612822   0.9598253   0.19094954\n",
            " -0.08352069  0.11417772  0.20272024 -0.10090255  0.773514    0.26200104\n",
            " -0.06391945 -0.06261943 -0.00758461 -0.00799846  0.01314618  0.4149971\n",
            "  0.02761869  0.03459037  0.04009605  0.09457483 -0.12652281  0.14688917\n",
            "  0.2108453   0.5476735   0.24679857  0.62733847  0.36359617  0.14020869\n",
            " -0.0542671   0.33816415  0.13255262 -0.09359657  0.35604346 -0.05181196\n",
            "  0.7506861  -0.05007518  0.28283557  0.26996586]\n",
            "lstm_2/kernel:0 (64, 128)\n",
            "[[ 0.14882666 -0.15246692  0.2641151  ... -0.16140832 -0.58146507\n",
            "  -0.5860567 ]\n",
            " [ 0.55492723  0.1702621   0.42830765 ... -0.18238269  0.07259887\n",
            "   0.6178811 ]\n",
            " [ 0.23346397  0.4779209   0.12188248 ...  0.18809706  0.36549845\n",
            "   0.3482553 ]\n",
            " ...\n",
            " [ 0.18854026 -0.64816964 -0.37035728 ... -0.488197   -0.18658867\n",
            "  -0.4271752 ]\n",
            " [ 0.38258857  0.5573167   0.01487021 ... -0.15452473 -0.14598973\n",
            "   0.38290524]\n",
            " [-0.4318145  -0.11822388  0.02183052 ... -0.29413587 -0.43793714\n",
            "  -0.15075871]]\n",
            "lstm_2/recurrent_kernel:0 (32, 128)\n",
            "[[-1.4236747   0.5939996  -0.36045504 ... -0.0871355  -0.54427356\n",
            "   0.08891585]\n",
            " [ 0.8096541   0.46817186  0.2082673  ...  0.8892826   0.66731924\n",
            "   0.20584041]\n",
            " [-0.22608323  0.03929636  0.3318233  ... -0.71443    -0.7180802\n",
            "  -0.39503404]\n",
            " ...\n",
            " [ 0.2439116   0.4321105   0.41339242 ...  0.4655417  -0.38445088\n",
            "   0.6555878 ]\n",
            " [ 0.31133062 -0.7221192  -0.0298316  ...  0.10042283  0.89620304\n",
            "   0.02115083]\n",
            " [ 0.5497993   0.3467229   0.53541875 ... -0.5774768  -0.40358135\n",
            "   0.11331528]]\n",
            "lstm_2/bias:0 (128,)\n",
            "[ 0.4517487   0.21816586  0.14360781  0.11288222 -0.0247747   0.22711052\n",
            "  0.3596887   0.08488923  0.08393008  0.5088758   0.43398166  0.3765085\n",
            "  0.06940075  0.09750959  0.12340687  0.0432567   0.27597195  0.17115894\n",
            "  0.09694371  0.14083289  0.28103152  0.3902248   0.17670146  0.3303237\n",
            "  0.3033926   0.23715167  0.27909115  0.13127656  0.06669139  0.29363763\n",
            "  0.28336057  0.52582955  0.79190224  0.99184626  0.91089565  0.9930216\n",
            "  0.8637626   0.6683433   0.9153516   1.1476469   1.007713    0.7457168\n",
            "  1.0684953   1.1642991   1.0738349   0.7880637   0.60910994  1.0968556\n",
            "  1.00636     1.0153366   0.9278793   1.0843865   0.9540643   0.7357699\n",
            "  1.1263864   1.1441079   0.94765455  0.70056033  0.9750121   0.96271056\n",
            "  1.0598772   0.7369644   0.87674266  0.81016755  0.129502   -0.02552469\n",
            "  0.24059454 -0.10174239  0.05540775 -0.14222735 -0.07770392 -0.03695955\n",
            "  0.13569841 -0.31509587 -0.06745779 -0.02064211  0.15460074 -0.02840791\n",
            "  0.07415043 -0.21287996  0.0734686   0.06158287  0.10670114 -0.10713775\n",
            "  0.01725274 -0.13428493  0.20776662 -0.19472873  0.10332079 -0.12562299\n",
            " -0.05102674  0.08534332 -0.03555392 -0.20719646 -0.08053266  0.20174034\n",
            "  0.2853161   0.35115486  0.3050305   0.3933415   0.39001086  0.31349474\n",
            "  0.47436228  0.08026434  0.21259393  0.04920783  0.51547295  0.01243473\n",
            "  0.2632446   0.20966373  0.36946622  0.12617032  0.22903362  0.07927767\n",
            "  0.25220776  0.2295066   0.15433209  0.44996193  0.1796973   0.13600335\n",
            "  0.21559933  0.36342847  0.22481482  0.1441993   0.09426478  0.3143222\n",
            "  0.28746107  0.42490077]\n",
            "time_distributed_1/kernel:0 (32, 12)\n",
            "[[ 0.9564774  -0.9736714   2.8573399   4.4269876  -1.6148725  -3.3439605\n",
            "  -1.2074155   0.1253774  -1.0547293  -2.4137664  -0.04729455  2.0851288 ]\n",
            " [ 1.2998284   1.2669821  -0.8595586   4.3840017   3.0374064   1.1418009\n",
            "  -2.3750677  -1.4054443  -0.99156964 -1.2890154  -1.2632102  -1.2851866 ]\n",
            " [ 1.1512526  -1.0906297   1.8035629   1.6438078  -2.71948    -2.442042\n",
            "   0.5977142   0.4807867  -0.15691848  0.6212204   0.56370157  0.09181686]\n",
            " [-0.74878144  1.2291124  -0.7664575   2.5329933   0.04462455 -0.39887962\n",
            "  -1.0570412  -0.5806531  -0.561914    0.17853945 -1.133911    0.62565386]\n",
            " [-0.7544658  -0.44747642 -2.8467462  -0.33090308 -0.9056106  -0.39320615\n",
            "  -1.2294309   1.2868209   2.5676093   1.7561344   0.7594019   1.0541369 ]\n",
            " [-1.0586988   1.4489969   8.170848    1.1258895  -2.5517457  -1.0815359\n",
            "  -0.9505181  -1.1821865  -0.8909617  -1.0777763  -1.0034424   2.960731  ]\n",
            " [-0.6760196   1.0612688   0.20260738 -3.4762816  -0.3929427   4.4367476\n",
            "   3.8076787   0.8045816  -2.0449877  -1.1212124  -0.42660773 -0.9213672 ]\n",
            " [ 0.44916886 -0.9179207  -1.0276141  -3.5061774  -0.45687532  1.7258071\n",
            "   1.0967376   0.52478164 -0.04219367  0.8059581   1.248253    0.40642437]\n",
            " [ 0.6538489  -0.75531703  0.97925836  2.2544353  -1.2387997  -1.083934\n",
            "  -0.64005125 -1.0136412  -0.9542807   0.773316    0.60592276  0.05891652]\n",
            " [-0.9821871   0.7356144  -1.4544252   2.1512482   3.9976492   4.0347166\n",
            "   4.658107   -0.93071663 -5.521253   -3.7078934  -1.3701754  -0.70991904]\n",
            " [-1.8808078   0.19638455 -1.8804711  -0.70304656 -2.3172417  -2.0584114\n",
            "   1.7410206   5.8237624   6.1381154   2.6143768  -3.00259    -4.1872144 ]\n",
            " [-1.9221916  -0.8085485  -2.9273045  -1.9727427  -0.5864091  -1.456425\n",
            "  -2.095102   -0.51896244  2.4004126   3.3166323   2.8258748   0.91302973]\n",
            " [-3.2257228  -1.5058448   1.5534588   1.3216419   0.441292    1.0180622\n",
            "   2.8103027   3.0677934  -1.5575901  -6.9108734  -4.7878785   2.2012405 ]\n",
            " [-0.32842362  0.93799615  0.36923897  2.9550176   1.9537358  -1.9350394\n",
            "  -2.970712   -1.5349697  -0.950942   -0.7657542   1.4841202   0.9601934 ]\n",
            " [ 0.00027644  0.60775733  0.11543803 -1.7938138  -2.0158787  -2.2652173\n",
            "  -5.1819415  -5.847571    1.6330202   6.430249    5.8989563   5.2872934 ]\n",
            " [-0.6767583   0.8053717  -0.56111336 -1.2636287  -1.2246063  -1.8606715\n",
            "   2.4928772   2.4920313   1.5052364   0.32418334 -0.78209805 -1.6263982 ]\n",
            " [ 0.2698871  -1.5736622  -0.4042859  -3.1883051  -0.04129076  0.6226632\n",
            "   0.73293763  0.03306006 -0.42382097  0.2066458   1.0525458   0.84854347]\n",
            " [-1.8213494  -0.5014556   0.6295365   3.1560202   4.6682      3.1558466\n",
            "   0.4414179  -2.0347698  -1.925165   -2.0202904  -2.8343284  -2.2818792 ]\n",
            " [-1.1703973   0.42823863  3.1662214  -1.0827049  -5.0735955  -6.7633243\n",
            "   1.6502805   5.7104926   2.9148529   0.8444816   1.6042911   2.9378145 ]\n",
            " [ 0.6036395   0.23018616 -2.1919215  -3.1612172  -3.4396164  -0.5304084\n",
            "   1.9636828   2.6141386   2.6891453   2.8914182   1.5186044  -1.1562175 ]\n",
            " [ 2.502709    0.70624465  0.01163361 -3.8354824  -4.2641797  -0.09825955\n",
            "   1.212543    1.3877591   1.0559688   1.2164915   1.4922318   1.6625973 ]\n",
            " [-0.63741434  0.38139606 -2.0945516  -3.7912996  -3.9476278   2.228526\n",
            "   3.5525522   2.5514548   3.0351255   3.6827736   0.83647    -5.658772  ]\n",
            " [ 0.45759746 -1.2404784  -0.01075623 -0.16534568  0.44899833 -0.5397826\n",
            "  -0.06012556 -0.23344767 -0.96130687  1.1042718   1.0579165   0.45387897]\n",
            " [10.369608    2.1317666  -0.9518007  -0.0337424  -0.92526156 -1.7042196\n",
            "  -0.54358906  0.90287954  1.8374869  -0.7688061  -1.4164783  -0.4078629 ]\n",
            " [-0.07544233 -1.7293947   0.82479143  1.4448485   0.6759729   0.44483516\n",
            "  -0.09921514 -1.261516   -1.4531285   0.3770862   0.4756048   0.20016259]\n",
            " [ 0.48509833 -0.35277447 -0.9914893  -2.9270046  -0.07470752  2.6130452\n",
            "   2.4198349   2.6792734   5.603538   -2.5095646  -4.9831963  -3.6586974 ]\n",
            " [-1.3358206  -0.00905048 -0.85389906  1.7367285   2.6688724   3.1225393\n",
            "   2.4418752   0.36310542 -0.98774433 -1.5183041  -1.6653888  -1.7138551 ]\n",
            " [ 0.795351   -0.4693966   1.9640133   0.32587612  0.32070714  1.3177391\n",
            "   1.0406358  -0.66026485 -0.64944994 -1.711604   -1.380682   -1.2008381 ]\n",
            " [-2.1240683   0.44463527 -2.441637   -2.957429   -1.2487866   0.19404535\n",
            "   0.44952002  0.70112836  1.4364867   1.583899    1.8540328   1.3223432 ]\n",
            " [-1.1322471   0.74688506  1.5203272  -1.1383978   0.70835257  2.8731904\n",
            "   2.485824   -5.0499077  -3.2870548   2.158283    2.133747   -0.12689158]\n",
            " [-0.9446577   1.0053121   2.5781043  -1.066849    1.690897    1.3896866\n",
            "  -0.42756486 -0.581079   -3.2598085  -4.261694    2.485642    4.631475  ]\n",
            " [-3.9206069  -1.7559831   1.7511969   3.4251106  -1.9095874  -1.9274784\n",
            "  -0.8484767  -0.16070108 -1.6641359  -1.2346652   2.2508237   2.005346  ]]\n",
            "time_distributed_1/bias:0 (12,)\n",
            "[ 0.4453352  -0.62348646 -0.12472708  0.8568718  -0.05610938  0.07387629\n",
            " -0.00908362 -0.21968581 -0.13429621 -0.00288858  0.11554736 -0.457177  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN175JTWmzM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}