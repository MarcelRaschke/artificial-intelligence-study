{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras-BERT_simple-text-classification_20200421.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjKSUx4MirqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://androidkt.com/simple-text-classification-using-bert-in-tensorflow-keras-2-0/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK3krsA8KRIp",
        "colab_type": "code",
        "outputId": "151d2403-d797-46ca-b4dc-7a54b0fbdd17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "!pip install bert-for-tf2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.14.4)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.0)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.9.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.38.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LNI7arEPIPx",
        "colab_type": "code",
        "outputId": "ca502be3-ef05-465a-8555-6a5387d9b491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.85)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56enlVerOhEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09Ox_h10OhGx",
        "colab_type": "code",
        "outputId": "b7d588eb-b953-448e-ed2d-4af97d3ecc03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import bert\n",
        "from tensorflow.keras.models import  Model\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "print(\"TensorFlow Version:\",tf.__version__)\n",
        "print(\"Hub version: \",hub.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow Version: 2.2.0-rc3\n",
            "Hub version:  0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMOOiIVmP-3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "bert_layer=hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=True)\n",
        "\n",
        "MAX_SEQ_LEN=128\n",
        "input_word_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,\n",
        "                                       name=\"input_word_ids\")\n",
        "input_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,\n",
        "                                   name=\"input_mask\")\n",
        "segment_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,\n",
        "                                    name=\"segment_ids\")\n",
        "\n",
        "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXYcDM53V05P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "out = tf.keras.layers.Dense(6, activation=\"sigmoid\", name=\"dense_output\")(x)\n",
        "\n",
        "model = tf.keras.models.Model(\n",
        "      inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6CHF8WfV5gg",
        "colab_type": "code",
        "outputId": "6d485654-b9f8-4483-bf94-a32252e9cb0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d (Globa (None, 768)          0           keras_layer[0][1]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 768)          0           global_average_pooling1d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense_output (Dense)            (None, 6)            4614        dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 109,486,855\n",
            "Trainable params: 109,486,854\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQbqZQhXUTkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FullTokenizer=bert.bert_tokenization.FullTokenizer\n",
        "\n",
        "vocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "\n",
        "do_lower_case=bert_layer.resolved_object.do_lower_case.numpy()\n",
        "\n",
        "tokenizer=FullTokenizer(vocab_file,do_lower_case)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9IaN5_Fkt7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_masks(tokens, max_seq_length):\n",
        "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "def get_segments(tokens, max_seq_length):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
        "    segments = []\n",
        "    current_segment_id = 0\n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            current_segment_id = 1\n",
        "    return segments + [0] * (max_seq_length - len(tokens))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BsauL7dkt_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ids(tokens, tokenizer, max_seq_length):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens,)\n",
        "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
        "    return input_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrE6fz8oOhOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"jeongunkuk\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"e965f9b6b69ec1b182beef7519b4f4cc\" # key from the json file\n",
        "# kaggle competitions download -c jigsaw-toxic-comment-classification-challenge"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1JR2oSMOhRZ",
        "colab_type": "code",
        "outputId": "b9d0c654-55c5-4fd1-9997-b2b513316657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "test.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "sample_submission.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "train.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test_labels.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4MFikjLOhW1",
        "colab_type": "code",
        "outputId": "dba12e25-37f8-4702-deab-930fee4ba336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "!unzip train.csv.zip\n",
        "!unzip test.csv.zip\t\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  train.csv.zip\n",
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "Archive:  test.csv.zip\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hao1rgFmOhT6",
        "colab_type": "code",
        "outputId": "7445ec01-1ab9-4444-81b5-c1a950d36950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\t\t   test.csv\t test_labels.csv.zip  train.csv.zip\n",
            "sample_submission.csv.zip  test.csv.zip  train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn_0yNpsOhZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df=pd.read_csv('train.csv')\n",
        "\n",
        "df = df.sample(frac=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6hQ0xkqOhcN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "f143522d-2dae-417d-b22d-c43111bf958c"
      },
      "source": [
        "train_sentences = df[\"comment_text\"].fillna(\"CVxTz\").values\n",
        "print(train_sentences[0:5])\n",
        "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "train_y = df[list_classes].values\n",
        "print(train_y[0:5])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Retirement \\n\\nWhy is Owen's retirement from football not mentioned? He hasn't played a game since 2005.\"\n",
            " 'Another RfC on naming \\n\\nPlease see the further RfC here.'\n",
            " '\"\\n\\n Please Help! \\n\\nPlease help!  I saw that you said \"\"It seems to me is that permanent magnet synchronous generator is just another name for magneto (generator). Maybe we should just change that page to redirect to this page.\"\" on 17 October 2013. If you still hold this opinion, could you say so here: Talk:Magneto_(generator)#RFC_on_the_Status_of_This_Article ?  Actually, even if you opinion has changed, I still solicit your comment.    \"'\n",
            " 'SEED-WAAS\\nIf the category link at the bottom of the page is a red link, that means that the category in question does not exist. Which, in turn, means that the article is not properly categorized, and thus has to be retagged as  again. Could you please take a few minutes to find an appropriate category that actually exists before attempting to replace the tag with a category again? Thanks.'\n",
            " '\"\\nThe assertion that Holodomor is a term used exclusively by \"\"extremists\"\" is just absurd given the academic use of the term in the world. The arguments presented on this discussion page about it only coming to use in 1988 are just as weak, considering the word \"\"Holocaust\"\" wasn\\'t used in Russia until the 1990s - would someone make the claim that the .ru wiki not have a Holocaust page? Terms become more popular, it\\'s a given, its the evolution of language and the globalization and linguistic transcendence of terminology. TFD, in my opinion, is just trying to ruffle feathers and vex users here, there is no way his claims can be taken seriously, he has no once backed up his ridiculous claims of extremism and that it is a fringe word in academics. You see Ukrainian historians and academics in the diaspora use it more than others not due to the fact Ukrainian historians do most of the research in this field, just as Jewish historians have dominated Holocaust historiography, Americans American historiography, etc. To write off respected historical literature as \"\"nationalism\"\" or even worse \"\"extremism\"\" serves as nothing more than provocation, not legitimate deliberation. \\'\\'\\'\\'\\'\\'  \\n\"']\n",
            "[[0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2Z-ZcEhsECa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_single_input(sentence,MAX_LEN):\n",
        "  \n",
        "  stokens = tokenizer.tokenize(sentence)\n",
        "  \n",
        "  stokens = stokens[:MAX_LEN]\n",
        "  \n",
        "  stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
        " \n",
        "  ids = get_ids(stokens, tokenizer, MAX_SEQ_LEN)\n",
        "  masks = get_masks(stokens, MAX_SEQ_LEN)\n",
        "  segments = get_segments(stokens, MAX_SEQ_LEN)\n",
        "\n",
        "  return ids,masks,segments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnX-lYMrOheq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_input_array(sentences):\n",
        "\n",
        "  input_ids, input_masks, input_segments = [], [], []\n",
        "\n",
        "  for sentence in tqdm(sentences,position=0, leave=True):\n",
        "  \n",
        "    ids,masks,segments=create_single_input(sentence,MAX_SEQ_LEN-2)\n",
        "\n",
        "    input_ids.append(ids)\n",
        "    input_masks.append(masks)\n",
        "    input_segments.append(segments)\n",
        "\n",
        "  return [np.asarray(input_ids, dtype=np.int32), \n",
        "            np.asarray(input_masks, dtype=np.int32), \n",
        "            np.asarray(input_segments, dtype=np.int32)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OenQWBJEvx9F",
        "colab_type": "code",
        "outputId": "59e354c6-0acc-436d-e9c2-5dbc664b7670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "inputs=create_input_array(train_sentences)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 159571/159571 [03:04<00:00, 862.98it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZT3munMvw44",
        "colab_type": "code",
        "outputId": "c748c5c4-ed24-4099-f65f-e576c542eebb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.fit(inputs,train_y,epochs=1,batch_size=64,validation_split=0.2,shuffle=True) #batch_size=32"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  81/1995 [>.............................] - ETA: 5:15:14 - loss: 0.1949 - accuracy: 0.6827"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAah8tMgosYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coUa-kxPoskX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df=pd.read_csv(\"test.csv\")\n",
        "\n",
        "test_sentences = test_df[\"comment_text\"].fillna(\"CVxTz\").values\n",
        "\n",
        "test_inputs=create_input_array(test_sentences[110:150])\n",
        "\n",
        "print(model.predict(test_inputs))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cy1iusNqai5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kOuKe7JIxHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}