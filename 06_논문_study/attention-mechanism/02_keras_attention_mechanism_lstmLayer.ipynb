{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras-attention-mechanism-lstmLayer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7Ocnrs7Q0WS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "726e2d89-2880-43da-fdf0-7622205f8bc5"
      },
      "source": [
        "#https://github.com/philipperemy/keras-attention-mechanism/blob/master/attention_lstm.py\n",
        "\n",
        "!git clone https://github.com/philipperemy/keras-attention-mechanism.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras-attention-mechanism'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "Receiving objects:   0% (1/139)   \rReceiving objects:   1% (2/139)   \rReceiving objects:   2% (3/139)   \rReceiving objects:   3% (5/139)   \rReceiving objects:   4% (6/139)   \rReceiving objects:   5% (7/139)   \rReceiving objects:   6% (9/139)   \rReceiving objects:   7% (10/139)   \rReceiving objects:   8% (12/139)   \rReceiving objects:   9% (13/139)   \rReceiving objects:  10% (14/139)   \rReceiving objects:  11% (16/139)   \rReceiving objects:  12% (17/139)   \rReceiving objects:  13% (19/139)   \rReceiving objects:  14% (20/139)   \rReceiving objects:  15% (21/139)   \rReceiving objects:  16% (23/139)   \rReceiving objects:  17% (24/139)   \rReceiving objects:  18% (26/139)   \rReceiving objects:  19% (27/139)   \rReceiving objects:  20% (28/139)   \rReceiving objects:  21% (30/139)   \rReceiving objects:  22% (31/139)   \rReceiving objects:  23% (32/139)   \rReceiving objects:  24% (34/139)   \rReceiving objects:  25% (35/139)   \rReceiving objects:  26% (37/139)   \rReceiving objects:  27% (38/139)   \rReceiving objects:  28% (39/139)   \rReceiving objects:  29% (41/139)   \rReceiving objects:  30% (42/139)   \rReceiving objects:  31% (44/139)   \rReceiving objects:  32% (45/139)   \rReceiving objects:  33% (46/139)   \rReceiving objects:  34% (48/139)   \rReceiving objects:  35% (49/139)   \rReceiving objects:  36% (51/139)   \rremote: Total 139 (delta 0), reused 0 (delta 0), pack-reused 139\u001b[K\n",
            "Receiving objects:  37% (52/139)   \rReceiving objects:  38% (53/139)   \rReceiving objects:  39% (55/139)   \rReceiving objects:  40% (56/139)   \rReceiving objects:  41% (57/139)   \rReceiving objects:  42% (59/139)   \rReceiving objects:  43% (60/139)   \rReceiving objects:  44% (62/139)   \rReceiving objects:  45% (63/139)   \rReceiving objects:  46% (64/139)   \rReceiving objects:  47% (66/139)   \rReceiving objects:  48% (67/139)   \rReceiving objects:  49% (69/139)   \rReceiving objects:  50% (70/139)   \rReceiving objects:  51% (71/139)   \rReceiving objects:  52% (73/139)   \rReceiving objects:  53% (74/139)   \rReceiving objects:  54% (76/139)   \rReceiving objects:  55% (77/139)   \rReceiving objects:  56% (78/139)   \rReceiving objects:  57% (80/139)   \rReceiving objects:  58% (81/139)   \rReceiving objects:  59% (83/139)   \rReceiving objects:  60% (84/139)   \rReceiving objects:  61% (85/139)   \rReceiving objects:  62% (87/139)   \rReceiving objects:  63% (88/139)   \rReceiving objects:  64% (89/139)   \rReceiving objects:  65% (91/139)   \rReceiving objects:  66% (92/139)   \rReceiving objects:  67% (94/139)   \rReceiving objects:  68% (95/139)   \rReceiving objects:  69% (96/139)   \rReceiving objects:  70% (98/139)   \rReceiving objects:  71% (99/139)   \rReceiving objects:  72% (101/139)   \rReceiving objects:  73% (102/139)   \rReceiving objects:  74% (103/139)   \rReceiving objects:  75% (105/139)   \rReceiving objects:  76% (106/139)   \rReceiving objects:  77% (108/139)   \rReceiving objects:  78% (109/139)   \rReceiving objects:  79% (110/139)   \rReceiving objects:  80% (112/139)   \rReceiving objects:  81% (113/139)   \rReceiving objects:  82% (114/139)   \rReceiving objects:  83% (116/139)   \rReceiving objects:  84% (117/139)   \rReceiving objects:  85% (119/139)   \rReceiving objects:  86% (120/139)   \rReceiving objects:  87% (121/139)   \rReceiving objects:  88% (123/139)   \rReceiving objects:  89% (124/139)   \rReceiving objects:  90% (126/139)   \rReceiving objects:  91% (127/139)   \rReceiving objects:  92% (128/139)   \rReceiving objects:  93% (130/139)   \rReceiving objects:  94% (131/139)   \rReceiving objects:  95% (133/139)   \rReceiving objects:  96% (134/139)   \rReceiving objects:  97% (135/139)   \rReceiving objects:  98% (137/139)   \rReceiving objects:  99% (138/139)   \rReceiving objects: 100% (139/139)   \rReceiving objects: 100% (139/139), 1.15 MiB | 15.75 MiB/s, done.\n",
            "Resolving deltas:   0% (0/72)   \rResolving deltas:   1% (1/72)   \rResolving deltas:  11% (8/72)   \rResolving deltas:  23% (17/72)   \rResolving deltas:  43% (31/72)   \rResolving deltas:  48% (35/72)   \rResolving deltas:  51% (37/72)   \rResolving deltas:  73% (53/72)   \rResolving deltas:  79% (57/72)   \rResolving deltas:  91% (66/72)   \rResolving deltas:  98% (71/72)   \rResolving deltas: 100% (72/72)   \rResolving deltas: 100% (72/72), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEM9KsB7VKw0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "c8b85f7f-6316-4168-fdc6-43cfc1674232"
      },
      "source": [
        "ls -alrt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 20\n",
            "drwxr-xr-x 1 root root 4096 Oct 25 16:58 \u001b[0m\u001b[01;34msample_data\u001b[0m/\n",
            "drwxr-xr-x 1 root root 4096 Oct 30 15:14 \u001b[01;34m.config\u001b[0m/\n",
            "drwxr-xr-x 1 root root 4096 Oct 31 16:05 \u001b[01;34m..\u001b[0m/\n",
            "drwxr-xr-x 1 root root 4096 Oct 31 16:09 \u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x 4 root root 4096 Oct 31 16:09 \u001b[01;34mkeras-attention-mechanism\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQXdG0lWVKzN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "03b76d5d-f933-4b77-a76a-a6aa9530eecf"
      },
      "source": [
        "cd keras-attention-mechanism"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/keras-attention-mechanism\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtMzqG_XVK5V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "26685f8a-07fe-47e2-fc9a-8eaa71bfc27d"
      },
      "source": [
        "from keras.layers import merge\n",
        "from keras.layers.core import *\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import *\n",
        "\n",
        "from attention_utils import get_activations, get_data_recurrent\n",
        "\n",
        "\n",
        "INPUT_DIM = 2\n",
        "TIME_STEPS = 20\n",
        "# if True, the attention vector is shared across the input_dimensions where the attention is applied.\n",
        "SINGLE_ATTENTION_VECTOR = False\n",
        "APPLY_ATTENTION_BEFORE_LSTM = False"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xEy2HUIkuK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras.layers import multiply\n",
        "\n",
        "def attention_3d_block(inputs):\n",
        "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
        "    input_dim = int(inputs.shape[2])\n",
        "    a = Permute((2, 1))(inputs)\n",
        "    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
        "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
        "    if SINGLE_ATTENTION_VECTOR:\n",
        "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
        "        a = RepeatVector(input_dim)(a)\n",
        "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
        "    #output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
        "    output_attention_mul = multiply([inputs, a_probs], name='attention_mul')\n",
        "    \n",
        "    return output_attention_mul\n",
        "\n",
        "\n",
        "def model_attention_applied_after_lstm():\n",
        "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
        "    lstm_units = 32\n",
        "    lstm_out = LSTM(lstm_units, return_sequences=True)(inputs)\n",
        "    attention_mul = attention_3d_block(lstm_out)\n",
        "    attention_mul = Flatten()(attention_mul)\n",
        "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
        "    model = Model(input=[inputs], output=output)\n",
        "    return model\n",
        "\n",
        "\n",
        "def model_attention_applied_before_lstm():\n",
        "    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))\n",
        "    attention_mul = attention_3d_block(inputs)\n",
        "    lstm_units = 32\n",
        "    attention_mul = LSTM(lstm_units, return_sequences=False)(attention_mul)\n",
        "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
        "    model = Model(input=[inputs], output=output)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMmNvbXNk9nx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "943e4260-4b63-403f-d51a-f7a29721437d"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    N = 300000\n",
        "    # N = 300 -> too few = no training\n",
        "    inputs_1, outputs = get_data_recurrent(N, TIME_STEPS, INPUT_DIM)\n",
        "\n",
        "    if APPLY_ATTENTION_BEFORE_LSTM:\n",
        "        m = model_attention_applied_before_lstm()\n",
        "    else:\n",
        "        m = model_attention_applied_after_lstm()\n",
        "\n",
        "    m.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    print(m.summary())\n",
        "\n",
        "    m.fit([inputs_1], outputs, epochs=1, batch_size=64, validation_split=0.1)\n",
        "\n",
        "    attention_vectors = []\n",
        "    for i in range(300):\n",
        "        testing_inputs_1, testing_outputs = get_data_recurrent(1, TIME_STEPS, INPUT_DIM)\n",
        "        attention_vector = np.mean(get_activations(m,\n",
        "                                                   testing_inputs_1,\n",
        "                                                   print_shape_only=True,\n",
        "                                                   layer_name='attention_vec')[0], axis=2).squeeze()\n",
        "        print('attention =', attention_vector)\n",
        "        assert (np.sum(attention_vector) - 1.0) < 1e-5\n",
        "        attention_vectors.append(attention_vector)\n",
        "\n",
        "    attention_vector_final = np.mean(np.array(attention_vectors), axis=0)\n",
        "    "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 20, 2)        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 20, 32)       4480        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "permute_2 (Permute)             (None, 32, 20)       0           lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 32, 20)       0           permute_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 32, 20)       420         reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "attention_vec (Permute)         (None, 20, 32)       0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "attention_mul (Multiply)        (None, 20, 32)       0           lstm_2[0][0]                     \n",
            "                                                                 attention_vec[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 640)          0           attention_mul[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            641         flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 5,541\n",
            "Trainable params: 5,541\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 270000 samples, validate on 30000 samples\n",
            "Epoch 1/1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "270000/270000 [==============================] - 193s 714us/step - loss: 0.0513 - acc: 0.9745 - val_loss: 1.0167e-04 - val_acc: 1.0000\n",
            "----- activations -----\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "(1, 20, 32)\n",
            "attention = [0.00398361 0.00864736 0.00442721 0.0078052  0.00438269 0.00500983\n",
            " 0.00590835 0.00824708 0.00409412 0.00376676 0.6792405  0.00575141\n",
            " 0.01287578 0.22189723 0.00322527 0.00468026 0.00451009 0.00442717\n",
            " 0.00352764 0.00359241]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00469997 0.00909651 0.00514783 0.00826286 0.0055124  0.00569167\n",
            " 0.00716221 0.00820974 0.00405601 0.0047606  0.67125213 0.00846102\n",
            " 0.01336258 0.21579577 0.00510655 0.00538581 0.00492958 0.00530686\n",
            " 0.00403379 0.00376608]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00637017 0.01191115 0.0068716  0.01104856 0.00752687 0.00803288\n",
            " 0.00917473 0.01014497 0.00534993 0.00734572 0.6584321  0.00833118\n",
            " 0.01328216 0.19868779 0.00549915 0.00690571 0.0067285  0.00730078\n",
            " 0.00564937 0.00540665]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00583336 0.00850569 0.00579642 0.00822737 0.00595732 0.00618473\n",
            " 0.00647769 0.00832747 0.0056405  0.00532451 0.65669465 0.00559805\n",
            " 0.00984747 0.22661608 0.00443078 0.00615327 0.00677155 0.0063781\n",
            " 0.0060504  0.00518459]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00759724 0.01080777 0.00838929 0.01402778 0.00952304 0.00985006\n",
            " 0.00923124 0.01256014 0.00889898 0.00993059 0.6015438  0.00725226\n",
            " 0.01232895 0.20587443 0.00598428 0.00914004 0.01011412 0.01045324\n",
            " 0.02600092 0.01049186]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00580411 0.01039771 0.0069891  0.01025098 0.00703635 0.00710702\n",
            " 0.00870552 0.00961532 0.00469016 0.00613127 0.6444632  0.01179115\n",
            " 0.01965531 0.21073334 0.00713483 0.00691371 0.00590822 0.00684951\n",
            " 0.00526452 0.00455873]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00525298 0.00987014 0.00555242 0.00847702 0.00638964 0.00697665\n",
            " 0.00827275 0.00820507 0.00476002 0.00566069 0.66562676 0.00835403\n",
            " 0.01160198 0.2120294  0.00567742 0.00623392 0.00563542 0.00636957\n",
            " 0.00448799 0.00456613]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00565459 0.01022385 0.00632917 0.00985712 0.00671712 0.00732626\n",
            " 0.008548   0.00939632 0.00534555 0.00646475 0.64564663 0.01623201\n",
            " 0.01617545 0.20885094 0.00589397 0.00747936 0.00676244 0.00696467\n",
            " 0.00527653 0.00485528]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00602034 0.01130086 0.00631628 0.01013222 0.00697269 0.00737111\n",
            " 0.00868467 0.01070634 0.00490667 0.00654316 0.66207826 0.01000943\n",
            " 0.01423618 0.20144723 0.0062945  0.0061079  0.00534318 0.00625368\n",
            " 0.00453745 0.00473784]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00679038 0.01428263 0.00718191 0.01019827 0.00738401 0.00830809\n",
            " 0.01099763 0.01011324 0.00702321 0.00774369 0.6376754  0.01080996\n",
            " 0.01615725 0.19727367 0.00727038 0.00834965 0.0077667  0.00794144\n",
            " 0.01002632 0.0067062 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00740951 0.01209023 0.00708469 0.00962877 0.00748006 0.0080394\n",
            " 0.00977405 0.0095148  0.00614021 0.00758272 0.63558924 0.0141616\n",
            " 0.01768016 0.20385742 0.00765528 0.00922615 0.00749877 0.00770872\n",
            " 0.00574968 0.00612852]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00445233 0.00898033 0.00515824 0.00834038 0.00546318 0.00683765\n",
            " 0.00833397 0.00786787 0.00436379 0.00451903 0.67138684 0.00723352\n",
            " 0.01071743 0.21872136 0.00407358 0.00544706 0.0052496  0.00558505\n",
            " 0.0036081  0.0036607 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00794364 0.01309875 0.007334   0.00972794 0.00747307 0.00775597\n",
            " 0.00851144 0.01030999 0.00722307 0.00703013 0.6280838  0.01247499\n",
            " 0.02150336 0.19821046 0.00763032 0.00804094 0.00774131 0.00794778\n",
            " 0.01230131 0.00965768]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00567118 0.01036483 0.00580094 0.00867258 0.00671463 0.00679765\n",
            " 0.00758015 0.00873628 0.0052347  0.00517211 0.670852   0.00619496\n",
            " 0.0109486  0.20704885 0.00477267 0.00646567 0.00692298 0.00684515\n",
            " 0.00445989 0.00474418]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00835719 0.01252721 0.01064991 0.01168952 0.00809089 0.00843194\n",
            " 0.00896452 0.0129166  0.00938605 0.01151517 0.6157104  0.01016185\n",
            " 0.01637943 0.1918214  0.0065112  0.01118227 0.01019705 0.01015024\n",
            " 0.01233503 0.01302211]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00579056 0.00981696 0.00574522 0.00937171 0.0062508  0.00660846\n",
            " 0.00774017 0.0087325  0.00478241 0.0061084  0.66660285 0.00765819\n",
            " 0.01151047 0.20919147 0.00487856 0.00704072 0.0062899  0.00699343\n",
            " 0.00437388 0.0045133 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00559905 0.0111535  0.0067772  0.01120208 0.00772665 0.00697456\n",
            " 0.01035364 0.00976529 0.00589863 0.00701206 0.6523844  0.00969638\n",
            " 0.01117387 0.19681165 0.00812262 0.00709818 0.00713798 0.00763157\n",
            " 0.01112502 0.00635567]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00667195 0.01182586 0.00736554 0.00965143 0.00742667 0.00860198\n",
            " 0.01029326 0.0097897  0.00813795 0.0074304  0.6348653  0.009873\n",
            " 0.01345707 0.2060804  0.00722823 0.00945886 0.00988198 0.00751058\n",
            " 0.00798092 0.00646892]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00554823 0.01076724 0.00590453 0.01017704 0.00622281 0.00736744\n",
            " 0.00841568 0.0107131  0.00555798 0.0055269  0.661005   0.00994438\n",
            " 0.01187343 0.20742577 0.0056785  0.00626193 0.00628323 0.00627654\n",
            " 0.00458454 0.00446567]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00516894 0.00888601 0.00515827 0.0072531  0.00535153 0.00567029\n",
            " 0.00602502 0.00795096 0.00515653 0.0056441  0.66616225 0.00761063\n",
            " 0.01407723 0.21776396 0.00388672 0.00654966 0.00580198 0.00547331\n",
            " 0.00463972 0.00576975]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00535247 0.01039974 0.00569232 0.00923734 0.00599509 0.00646442\n",
            " 0.00778747 0.00864134 0.00523306 0.00562319 0.64586467 0.01412282\n",
            " 0.01939888 0.21686405 0.00539279 0.00654836 0.00601709 0.00643985\n",
            " 0.00440514 0.00451989]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00635598 0.01220139 0.00703916 0.01056267 0.00704177 0.00763283\n",
            " 0.00917119 0.01085719 0.00618812 0.00779805 0.6441361  0.01124375\n",
            " 0.0179256  0.20336229 0.00701995 0.00794248 0.00625689 0.00694279\n",
            " 0.00492585 0.00539593]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00601017 0.01139056 0.0058855  0.00916544 0.00584944 0.00677545\n",
            " 0.00785535 0.00962471 0.00555944 0.00675633 0.6627106  0.00864606\n",
            " 0.0134837  0.20513727 0.00567531 0.00696315 0.00592236 0.00616136\n",
            " 0.00536737 0.00506045]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0046693  0.00827316 0.00526675 0.00814496 0.00570776 0.00611927\n",
            " 0.00701562 0.007874   0.00419502 0.00461232 0.669605   0.00872757\n",
            " 0.01272651 0.21511775 0.00484304 0.00536967 0.0051402  0.00563832\n",
            " 0.00680651 0.00414725]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00457988 0.00789801 0.0049334  0.00758332 0.00545156 0.00565044\n",
            " 0.00684885 0.00710783 0.00418749 0.00461714 0.68125147 0.0044732\n",
            " 0.00843534 0.21839648 0.00357562 0.00547024 0.00555483 0.00552329\n",
            " 0.00454382 0.00391773]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0067709  0.01056316 0.00635914 0.00943771 0.00644342 0.00727323\n",
            " 0.00820161 0.0093713  0.00797313 0.00693889 0.6358416  0.00863521\n",
            " 0.01351354 0.22074991 0.00589604 0.00796917 0.00765175 0.0073804\n",
            " 0.00603066 0.00699923]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00588045 0.01003974 0.00643892 0.00923792 0.00733331 0.00710465\n",
            " 0.00807546 0.00928022 0.00641981 0.00625338 0.64768887 0.00720848\n",
            " 0.01095864 0.2168023  0.00639446 0.00686892 0.00698922 0.00714805\n",
            " 0.00809101 0.00578623]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00424855 0.00866278 0.00460203 0.00807543 0.00502611 0.00550919\n",
            " 0.00628619 0.00792643 0.00421976 0.00404726 0.6785325  0.00514147\n",
            " 0.01083062 0.22112711 0.00341204 0.00489738 0.00494496 0.00521219\n",
            " 0.00371761 0.00358036]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00652087 0.01122429 0.00692377 0.0102919  0.00727443 0.00803803\n",
            " 0.0093573  0.01018838 0.00572473 0.00709601 0.6436243  0.01056207\n",
            " 0.01538987 0.20749305 0.00754333 0.00797442 0.00699587 0.00755134\n",
            " 0.00516568 0.00506032]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00586365 0.01261224 0.00696824 0.00976596 0.0068937  0.00765719\n",
            " 0.00954254 0.00971456 0.0079773  0.00644163 0.6498072  0.00769611\n",
            " 0.01497103 0.20535901 0.00578807 0.00736457 0.0070009  0.00700516\n",
            " 0.00579051 0.0057804 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00542662 0.00982512 0.00642887 0.01057343 0.00885074 0.00753051\n",
            " 0.00884077 0.00967642 0.00495359 0.00758725 0.65880734 0.00930854\n",
            " 0.01203688 0.20063788 0.00670463 0.00735021 0.00649832 0.0073099\n",
            " 0.00630703 0.00534597]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00556405 0.00901554 0.00607236 0.00878076 0.00626199 0.00671652\n",
            " 0.00768723 0.00880532 0.00488476 0.00556118 0.6696142  0.00671728\n",
            " 0.01140431 0.20963722 0.00507328 0.00672885 0.00609527 0.00634527\n",
            " 0.00454744 0.0044872 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00529932 0.01112221 0.00553679 0.00881174 0.0061474  0.00678058\n",
            " 0.00857409 0.00808367 0.00483976 0.00588201 0.66877216 0.00864853\n",
            " 0.01168431 0.20588538 0.00533253 0.00694758 0.00630463 0.00658109\n",
            " 0.00420115 0.00456507]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0063762  0.01054985 0.00647215 0.0086798  0.00693434 0.00790546\n",
            " 0.00823244 0.00856363 0.0058867  0.00686946 0.6565509  0.00772573\n",
            " 0.01228537 0.20535684 0.0054035  0.00774877 0.00732046 0.00778402\n",
            " 0.00677119 0.00658314]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0054125  0.01033916 0.00611872 0.01138487 0.00691011 0.00767645\n",
            " 0.00859686 0.0096137  0.00537797 0.00678996 0.6481925  0.01484174\n",
            " 0.01448271 0.20465513 0.00618839 0.00744495 0.00793445 0.00758608\n",
            " 0.00517184 0.00528183]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00475781 0.00935938 0.0051341  0.00835016 0.00587552 0.00605317\n",
            " 0.00763873 0.00790065 0.00441431 0.00451078 0.6644421  0.00961065\n",
            " 0.01515107 0.21569848 0.00556619 0.00581641 0.00542396 0.00608465\n",
            " 0.00430661 0.00390522]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00438806 0.00854117 0.00500406 0.00762913 0.00571206 0.00585513\n",
            " 0.00787102 0.00768483 0.00435429 0.00460568 0.67779326 0.0058929\n",
            " 0.00936852 0.21742104 0.0044376  0.00554765 0.00518653 0.00534437\n",
            " 0.00387229 0.00349035]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00422022 0.00905585 0.00478751 0.00783405 0.00513197 0.00545705\n",
            " 0.00662033 0.00806683 0.00396905 0.00480514 0.6768171  0.00909469\n",
            " 0.01585333 0.21295309 0.0042316  0.00558504 0.00442656 0.00468666\n",
            " 0.00298053 0.00342338]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00522085 0.0081705  0.00514761 0.0072634  0.00527674 0.00566769\n",
            " 0.00635625 0.00753189 0.00482923 0.00518705 0.6755655  0.00621819\n",
            " 0.01072448 0.2171821  0.0040258  0.00564978 0.00542054 0.00534549\n",
            " 0.00431501 0.00490201]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00541053 0.00858705 0.00523634 0.00740011 0.00572064 0.00598909\n",
            " 0.00646296 0.00739471 0.00533477 0.00532779 0.67435384 0.0076099\n",
            " 0.01188144 0.2066082  0.00492986 0.00695448 0.0070628  0.00619483\n",
            " 0.00658473 0.00495592]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00533534 0.00830765 0.00532462 0.00785418 0.00582097 0.0061034\n",
            " 0.00671595 0.00807943 0.0053814  0.00534197 0.6681008  0.00583958\n",
            " 0.01099812 0.21971513 0.00412238 0.00576862 0.00590319 0.00569999\n",
            " 0.00510014 0.00448719]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00599112 0.00884169 0.00605797 0.00865652 0.00637182 0.00613139\n",
            " 0.00726544 0.00817434 0.00645897 0.00627319 0.64964867 0.00668105\n",
            " 0.01045408 0.21789257 0.00550133 0.00718903 0.00935342 0.00728819\n",
            " 0.00907985 0.00668932]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0063912  0.00951213 0.006369   0.00880617 0.0073547  0.00698084\n",
            " 0.00829527 0.00933911 0.00603237 0.00679172 0.67101395 0.0090188\n",
            " 0.01299312 0.19045168 0.00676364 0.00710262 0.00738764 0.00665883\n",
            " 0.0073093  0.00542791]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00451265 0.00850287 0.00519449 0.0083531  0.00563958 0.0057938\n",
            " 0.00761613 0.00842036 0.004785   0.00464849 0.67624724 0.00774817\n",
            " 0.01221235 0.2109179  0.00440868 0.00567069 0.00526433 0.00536096\n",
            " 0.00507096 0.00363222]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00532965 0.00954699 0.00577077 0.00834414 0.00602424 0.00648721\n",
            " 0.00698607 0.00821247 0.00464743 0.00567553 0.6484205  0.00813336\n",
            " 0.01558622 0.2275247  0.00534229 0.00633614 0.00560817 0.00636646\n",
            " 0.00486284 0.0047948 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00609385 0.01018909 0.00685498 0.0090817  0.00701288 0.00678699\n",
            " 0.00823231 0.00866077 0.00473871 0.00709506 0.65487325 0.00968947\n",
            " 0.01389826 0.20888513 0.00710602 0.00735051 0.00624582 0.00669775\n",
            " 0.00540142 0.00510597]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0050293  0.00838335 0.00537045 0.00782642 0.0057246  0.0060266\n",
            " 0.00669285 0.00837855 0.00571609 0.00487054 0.6531831  0.0088963\n",
            " 0.01344415 0.22559142 0.00555997 0.00610408 0.00597709 0.00594221\n",
            " 0.00654804 0.00473488]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00433388 0.00802494 0.00512926 0.00825926 0.0057168  0.00575626\n",
            " 0.00631322 0.00828633 0.00470191 0.00441722 0.67444384 0.00494477\n",
            " 0.00970299 0.21863043 0.00394967 0.00558882 0.00566674 0.00594476\n",
            " 0.00585376 0.00433508]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00446684 0.00863437 0.00502875 0.00857672 0.00548399 0.00594429\n",
            " 0.00703362 0.00803735 0.004145   0.00471524 0.6712502  0.00725426\n",
            " 0.01424184 0.21584472 0.0042342  0.00547665 0.00518964 0.0053848\n",
            " 0.00522683 0.00383066]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00563227 0.01121467 0.00687598 0.01054162 0.00724356 0.00719685\n",
            " 0.0086413  0.00971139 0.00549554 0.00672585 0.6539924  0.00857834\n",
            " 0.01444302 0.19925167 0.00667314 0.00832389 0.00688814 0.00758281\n",
            " 0.00863707 0.00635047]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00482049 0.01048577 0.00587627 0.00868217 0.00539696 0.00591578\n",
            " 0.00750966 0.0086022  0.00527929 0.00520462 0.6514321  0.01569554\n",
            " 0.02286532 0.20755604 0.00512882 0.00599152 0.00637458 0.00586843\n",
            " 0.00680554 0.00450892]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00567517 0.01038594 0.00621763 0.00952276 0.00719191 0.00765036\n",
            " 0.00854247 0.00947797 0.00543756 0.00645591 0.6492361  0.01289117\n",
            " 0.02084975 0.200391   0.0058762  0.00706704 0.00706228 0.00785979\n",
            " 0.00704815 0.00516089]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00500628 0.00831772 0.00520381 0.00784636 0.0056222  0.00610375\n",
            " 0.00709697 0.00799311 0.00527628 0.00545491 0.6646297  0.00673387\n",
            " 0.01045276 0.22208978 0.00473922 0.00618118 0.00605426 0.0060162\n",
            " 0.00490234 0.0042793 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00476983 0.00765968 0.00503895 0.00740992 0.00535327 0.00553762\n",
            " 0.00678533 0.00716681 0.00477475 0.00513959 0.6731173  0.00824578\n",
            " 0.01034116 0.21731824 0.00543434 0.00641176 0.00584894 0.00582391\n",
            " 0.00393077 0.00389213]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00490556 0.00900342 0.00583626 0.01010229 0.00605816 0.00581224\n",
            " 0.00885181 0.00845878 0.00474823 0.00588519 0.6713103  0.00785462\n",
            " 0.01195128 0.20554034 0.00511971 0.00680569 0.0060733  0.00614404\n",
            " 0.00538896 0.00414977]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00522613 0.00942812 0.00635803 0.00868487 0.00709022 0.00690377\n",
            " 0.00742125 0.00905213 0.00521924 0.0066115  0.64467317 0.01141917\n",
            " 0.01542298 0.21744967 0.00704243 0.00862846 0.00637014 0.00659917\n",
            " 0.00510575 0.00529382]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00506416 0.00951707 0.00575707 0.00874725 0.00607196 0.00651104\n",
            " 0.00846661 0.00803911 0.00468331 0.00578735 0.6542547  0.01326513\n",
            " 0.01684066 0.21300079 0.00514349 0.00690332 0.00599535 0.00647969\n",
            " 0.00487707 0.00459487]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00519665 0.00900953 0.00545921 0.00900254 0.00590363 0.00653355\n",
            " 0.00709732 0.00905235 0.00507569 0.00533059 0.65427166 0.00978894\n",
            " 0.01243043 0.22251514 0.00488054 0.00618808 0.00613164 0.00607207\n",
            " 0.00494839 0.0051121 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00478109 0.00801068 0.00510931 0.00775023 0.00526448 0.00591821\n",
            " 0.00675782 0.00730958 0.00426993 0.00492909 0.6664865  0.00634201\n",
            " 0.01135282 0.226465   0.00458234 0.00588826 0.00522926 0.00565836\n",
            " 0.00407542 0.00381965]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00559927 0.01214707 0.00699634 0.01044508 0.00789762 0.00793127\n",
            " 0.00880707 0.01008536 0.00706012 0.00684443 0.6411966  0.00893075\n",
            " 0.01166113 0.20552878 0.00675987 0.00898273 0.00872674 0.00824802\n",
            " 0.00891411 0.00723769]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00546972 0.01131467 0.0065024  0.01075929 0.00815035 0.0079918\n",
            " 0.00933493 0.01044312 0.00526313 0.00631922 0.6481732  0.00777129\n",
            " 0.01297244 0.20505512 0.00649377 0.00702348 0.00655168 0.00756702\n",
            " 0.0105535  0.00628982]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00545578 0.01001472 0.0055058  0.00875689 0.00641658 0.00673033\n",
            " 0.00732426 0.0091598  0.00584641 0.00564652 0.663774   0.00655901\n",
            " 0.01124906 0.2156821  0.00461705 0.00602608 0.00632888 0.00625328\n",
            " 0.00412881 0.00452465]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00626643 0.01003718 0.00614    0.00932895 0.00624844 0.00668604\n",
            " 0.00773572 0.0107485  0.00541519 0.00604383 0.6383345  0.01071069\n",
            " 0.01580122 0.22563604 0.00548507 0.00644527 0.00629285 0.00618479\n",
            " 0.00464836 0.00581088]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00618032 0.01103945 0.00651173 0.00977546 0.00689517 0.00769399\n",
            " 0.00830998 0.01007758 0.00565204 0.00749897 0.6276529  0.01066335\n",
            " 0.016296   0.22196598 0.00692178 0.00821193 0.00745942 0.00735064\n",
            " 0.00715986 0.00668344]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00631543 0.01130614 0.00708974 0.01256097 0.00726146 0.00715311\n",
            " 0.01127884 0.00978534 0.00610184 0.00664355 0.6496427  0.00925199\n",
            " 0.01384006 0.19843426 0.00686059 0.00733963 0.0072442  0.00781386\n",
            " 0.00771636 0.00635993]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00547471 0.00995826 0.00566749 0.00878025 0.00637235 0.00711768\n",
            " 0.00837016 0.00895726 0.00551847 0.00538693 0.6640357  0.00785801\n",
            " 0.01076396 0.21105361 0.00548676 0.00622234 0.00641605 0.00674969\n",
            " 0.0052579  0.00455242]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00635646 0.01126397 0.00646623 0.00981773 0.00646562 0.00742005\n",
            " 0.0083755  0.00888629 0.00503128 0.00615522 0.65483344 0.01018683\n",
            " 0.01475487 0.2062432  0.00573136 0.00739727 0.00644634 0.00773032\n",
            " 0.00494223 0.00549578]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00659787 0.01019907 0.00729087 0.00982427 0.00769616 0.00726559\n",
            " 0.00838628 0.01058416 0.00623208 0.00811403 0.6398518  0.01361843\n",
            " 0.01555665 0.20530036 0.00761042 0.00818948 0.00695113 0.00731627\n",
            " 0.00721493 0.00620011]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00562837 0.00903508 0.0056757  0.00857635 0.00554939 0.00636211\n",
            " 0.00704627 0.00873156 0.0050249  0.00534897 0.6761061  0.00455499\n",
            " 0.00942333 0.21265501 0.00370774 0.00555879 0.00581756 0.00576183\n",
            " 0.00488453 0.00455141]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00591182 0.00868811 0.00556447 0.00845447 0.00665109 0.00659243\n",
            " 0.00722675 0.00810407 0.00515972 0.00725124 0.668241   0.00936863\n",
            " 0.01092008 0.20528725 0.00548141 0.00694703 0.00672862 0.00687215\n",
            " 0.00546479 0.00508479]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00642788 0.01278657 0.00692299 0.01222553 0.00816198 0.00777647\n",
            " 0.00932398 0.01012055 0.00625044 0.00769326 0.65100443 0.00914152\n",
            " 0.0134987  0.1946477  0.00663063 0.00796434 0.00760575 0.00837134\n",
            " 0.00669484 0.00675106]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00488539 0.00947942 0.00529718 0.008342   0.00598389 0.00677053\n",
            " 0.00793973 0.00848855 0.00504751 0.00487382 0.6676859  0.0066769\n",
            " 0.01006003 0.21696448 0.00436385 0.00590929 0.00624142 0.00610988\n",
            " 0.00450552 0.00437467]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00627766 0.0116682  0.00681942 0.00969608 0.00899587 0.00778171\n",
            " 0.00989747 0.00989271 0.00693098 0.00651207 0.63518184 0.00917195\n",
            " 0.01316046 0.20987345 0.00832998 0.00767854 0.00950755 0.00808976\n",
            " 0.00791746 0.00661686]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00589465 0.01308236 0.00646545 0.00999388 0.00705328 0.00739363\n",
            " 0.00900326 0.009598   0.00518947 0.00659553 0.65995115 0.00806073\n",
            " 0.01511993 0.20144618 0.00591215 0.00693417 0.00614134 0.0066063\n",
            " 0.00462572 0.0049328 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00614516 0.0104971  0.00564949 0.00859089 0.00561513 0.00712082\n",
            " 0.00716129 0.00879336 0.00549902 0.00534817 0.6739954  0.00817035\n",
            " 0.01698869 0.19830191 0.00451957 0.00630859 0.00562861 0.00659152\n",
            " 0.00411075 0.00496428]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00648905 0.01134082 0.00652495 0.0094621  0.0069023  0.00759221\n",
            " 0.00916188 0.00903985 0.00617923 0.00730597 0.65421975 0.01235567\n",
            " 0.01446351 0.19382301 0.00724958 0.0087515  0.00797843 0.00773873\n",
            " 0.00721634 0.00620512]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00863226 0.01346028 0.00705053 0.0098775  0.00800003 0.01018881\n",
            " 0.00894111 0.01119329 0.01107536 0.00665365 0.63460064 0.00866383\n",
            " 0.01506943 0.19084589 0.00614114 0.00822118 0.01013242 0.01064933\n",
            " 0.01049237 0.01011094]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0054377  0.0106315  0.00612138 0.01163721 0.00653859 0.00715753\n",
            " 0.00998823 0.00996837 0.00553417 0.00581624 0.6662997  0.00690754\n",
            " 0.01136571 0.20201951 0.00491173 0.00649377 0.00629379 0.00652291\n",
            " 0.00553697 0.00481746]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0050623  0.00935528 0.00533877 0.0077623  0.00565709 0.00592212\n",
            " 0.00698998 0.0077926  0.00396291 0.00474631 0.67348534 0.00876258\n",
            " 0.01756708 0.20951927 0.00451756 0.00562093 0.0049818  0.00548508\n",
            " 0.00381722 0.00365347]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00530906 0.00925904 0.00523128 0.00782948 0.00584719 0.00647169\n",
            " 0.00691005 0.00800018 0.00482279 0.00514878 0.6751882  0.00773774\n",
            " 0.0135249  0.20794776 0.00478148 0.00628596 0.00572965 0.00577801\n",
            " 0.00403303 0.00416367]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00586466 0.01048496 0.00639115 0.00989415 0.00694471 0.0072357\n",
            " 0.00855419 0.0099807  0.00451833 0.00655529 0.6654585  0.00809427\n",
            " 0.01471894 0.20345111 0.0057499  0.00613137 0.00533631 0.00627409\n",
            " 0.00417066 0.00419102]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0058161  0.01063304 0.00666022 0.00878132 0.00764173 0.00651059\n",
            " 0.00809541 0.00907486 0.00571232 0.00639977 0.63270724 0.01567978\n",
            " 0.01702693 0.21389326 0.00944532 0.00850981 0.00731319 0.00781873\n",
            " 0.00558415 0.00669623]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00551415 0.011872   0.00580417 0.00982332 0.0068289  0.00692982\n",
            " 0.00963743 0.00854459 0.00491732 0.00656497 0.6642533  0.00694343\n",
            " 0.01171557 0.20660858 0.00505922 0.00645036 0.00611448 0.00626208\n",
            " 0.00567893 0.00447743]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00678287 0.01036061 0.00659876 0.00941746 0.00670436 0.00717837\n",
            " 0.00814639 0.00939768 0.00610407 0.00736922 0.64785427 0.01083989\n",
            " 0.0161469  0.20941254 0.00644947 0.00806985 0.00674406 0.00673735\n",
            " 0.00478573 0.00490016]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00585759 0.01262083 0.00648795 0.01027857 0.0066388  0.00849822\n",
            " 0.00896119 0.01068444 0.00595471 0.0062957  0.6503206  0.0111836\n",
            " 0.01388146 0.20406619 0.00541633 0.00739487 0.00665299 0.0069956\n",
            " 0.00607227 0.00573809]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00452602 0.00760546 0.00488623 0.00710697 0.00497712 0.00565465\n",
            " 0.00643734 0.00718357 0.00499415 0.00464576 0.66146874 0.00606646\n",
            " 0.01142865 0.23296106 0.0041743  0.00582756 0.00525584 0.00535747\n",
            " 0.00472656 0.00471601]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00509278 0.00823519 0.00489501 0.00732221 0.00513308 0.00565634\n",
            " 0.00643895 0.00713936 0.00445458 0.00477026 0.6719541  0.00574309\n",
            " 0.01135235 0.22300294 0.00379683 0.00571704 0.00534744 0.00541802\n",
            " 0.00433187 0.00419857]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0063187  0.01066666 0.00708114 0.00947968 0.00736693 0.00724131\n",
            " 0.00880981 0.00891087 0.00722667 0.00722833 0.6256752  0.00877308\n",
            " 0.01332715 0.22375742 0.00664916 0.00909587 0.00956579 0.0087852\n",
            " 0.00678422 0.00725681]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00726201 0.01307471 0.0074307  0.01144376 0.0070535  0.008682\n",
            " 0.0093466  0.01044781 0.00693386 0.00751795 0.64448655 0.00848391\n",
            " 0.01489587 0.19782755 0.0056566  0.00800412 0.00768348 0.00796102\n",
            " 0.00843508 0.00737294]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00647584 0.01043613 0.00678109 0.00954616 0.00674346 0.00719729\n",
            " 0.00836589 0.00904429 0.00549682 0.00700588 0.6573199  0.00829802\n",
            " 0.01436999 0.20421118 0.00593832 0.00824628 0.00682703 0.00765004\n",
            " 0.00490347 0.00514289]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00449988 0.0087374  0.00544434 0.00893428 0.00595697 0.00567708\n",
            " 0.00725395 0.00898783 0.0048424  0.00479671 0.68085766 0.0052393\n",
            " 0.0116008  0.20851319 0.00400269 0.00532789 0.00504438 0.00555613\n",
            " 0.00485564 0.00387154]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00614758 0.01268269 0.00635432 0.00976936 0.00681239 0.00803039\n",
            " 0.00843079 0.0102931  0.00565696 0.00593617 0.66656864 0.00744404\n",
            " 0.01414857 0.19495392 0.00549929 0.00667251 0.00646242 0.00667602\n",
            " 0.00593288 0.00552799]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00650292 0.01212358 0.0070352  0.01147894 0.00789165 0.00765649\n",
            " 0.00882619 0.01050859 0.00601957 0.00735711 0.64216983 0.0092971\n",
            " 0.01521094 0.20559148 0.00632517 0.00804916 0.00720918 0.00799444\n",
            " 0.00642539 0.00632706]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00805266 0.01236877 0.00813466 0.01083546 0.00922753 0.00856924\n",
            " 0.00941069 0.01062827 0.00674755 0.01075272 0.63024235 0.00897553\n",
            " 0.01476986 0.19745213 0.00598451 0.00918863 0.01201707 0.00945693\n",
            " 0.00906083 0.00812461]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00534934 0.00900258 0.00557515 0.00873118 0.00616191 0.00643443\n",
            " 0.00757415 0.00887815 0.0052638  0.00534313 0.66363925 0.00635762\n",
            " 0.01104233 0.21701062 0.00467838 0.00645223 0.00640037 0.00623908\n",
            " 0.00525835 0.00460796]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00490965 0.00947826 0.005199   0.00815687 0.00532367 0.00555338\n",
            " 0.00689613 0.00853991 0.00468031 0.00522594 0.67563576 0.00759472\n",
            " 0.01348058 0.2115562  0.00464476 0.00575913 0.00516104 0.00501327\n",
            " 0.00321846 0.00397301]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00486345 0.00890729 0.00528633 0.00842168 0.00583396 0.00599499\n",
            " 0.00789017 0.00765816 0.00454014 0.00518391 0.6770935  0.00737984\n",
            " 0.01192495 0.20850594 0.0046376  0.0063632  0.0057857  0.00579309\n",
            " 0.00389087 0.00404521]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00519515 0.01049826 0.00539044 0.00864442 0.00573255 0.00594432\n",
            " 0.00746654 0.00816926 0.00441699 0.00541989 0.6698068  0.00948447\n",
            " 0.0154552  0.208976   0.00492369 0.00612883 0.00539042 0.00558248\n",
            " 0.00354151 0.00383283]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.01048542 0.01141595 0.01202848 0.01715221 0.01081481 0.00995107\n",
            " 0.01583188 0.01338148 0.00859451 0.01623134 0.5629879  0.01504029\n",
            " 0.01383563 0.20740095 0.01365459 0.01016582 0.01109601 0.01179323\n",
            " 0.01813758 0.01000081]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00534548 0.01038371 0.00612335 0.00976904 0.00680774 0.00681002\n",
            " 0.00822076 0.00830182 0.00413411 0.00574916 0.6747615  0.00757009\n",
            " 0.01357575 0.19796738 0.00449663 0.00649663 0.00629816 0.00676468\n",
            " 0.0059562  0.00446784]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0061756  0.01208686 0.00634661 0.00882936 0.00628792 0.00779251\n",
            " 0.0088807  0.00870128 0.00541028 0.00604012 0.6589315  0.00858271\n",
            " 0.01690009 0.20161352 0.00604851 0.00716516 0.00639345 0.00719811\n",
            " 0.00537353 0.00524219]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00502661 0.0088897  0.00538369 0.00799102 0.00613818 0.00596509\n",
            " 0.00703583 0.0083077  0.00479885 0.00505548 0.66056013 0.01291136\n",
            " 0.01718934 0.21073887 0.00677644 0.00597417 0.00509818 0.00584499\n",
            " 0.00584007 0.00447424]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0069634  0.0100038  0.00659169 0.00837765 0.00656539 0.00731332\n",
            " 0.00736375 0.00894781 0.00573789 0.00646693 0.66107106 0.00813777\n",
            " 0.01423167 0.20425478 0.00608759 0.00681595 0.00633337 0.00675597\n",
            " 0.0062083  0.00577189]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00675792 0.01164507 0.00805957 0.01155048 0.00726428 0.00871692\n",
            " 0.00933811 0.01220301 0.00709747 0.0082048  0.6419773  0.00997886\n",
            " 0.01336914 0.20029742 0.00599317 0.00799107 0.00760066 0.00759483\n",
            " 0.00722966 0.00713026]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00485489 0.00899527 0.00527593 0.00840266 0.0057456  0.00614859\n",
            " 0.00773191 0.0073654  0.00402075 0.00479756 0.66989386 0.00791044\n",
            " 0.01160819 0.21836284 0.00425137 0.00561345 0.00535989 0.0058327\n",
            " 0.0040617  0.00376701]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00622654 0.01273809 0.00637985 0.01000147 0.00770943 0.00817311\n",
            " 0.0093962  0.01104264 0.00581396 0.00598075 0.6651409  0.00726304\n",
            " 0.01294162 0.19475108 0.00616838 0.00640507 0.006452   0.00688533\n",
            " 0.00514373 0.00538677]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00578098 0.01281963 0.00657448 0.01086644 0.00641718 0.00685794\n",
            " 0.00834726 0.0105935  0.00557784 0.00650511 0.65252805 0.00984837\n",
            " 0.01930078 0.19221033 0.00622673 0.00716019 0.00647854 0.00672088\n",
            " 0.01336826 0.00581748]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00580398 0.01026986 0.00591998 0.01000237 0.00658515 0.00690545\n",
            " 0.00799307 0.0093021  0.0047328  0.00631658 0.6735714  0.00548569\n",
            " 0.01160608 0.20431376 0.00412496 0.00604269 0.00612418 0.00622634\n",
            " 0.00436381 0.00430973]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00545199 0.01155265 0.00548832 0.00930865 0.00618976 0.00803731\n",
            " 0.00783892 0.00996344 0.00581254 0.00577604 0.6617668  0.0093257\n",
            " 0.01723553 0.20176244 0.00507742 0.00655228 0.00634163 0.00659349\n",
            " 0.00525863 0.0046664 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00467553 0.00913275 0.00533646 0.00846095 0.00529196 0.00601047\n",
            " 0.00697257 0.0087381  0.00474612 0.00456124 0.67105484 0.00596025\n",
            " 0.01308394 0.2174379  0.004002   0.00581075 0.00547088 0.00531453\n",
            " 0.00406526 0.00387351]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00627537 0.00967376 0.00626243 0.0080311  0.00659781 0.00652166\n",
            " 0.00710098 0.00777029 0.00477084 0.0071771  0.6512184  0.0071854\n",
            " 0.01202082 0.22025305 0.00471805 0.00745882 0.00781011 0.0067788\n",
            " 0.00615385 0.00622135]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00498885 0.00917263 0.00777185 0.00987147 0.00763435 0.00639794\n",
            " 0.00920444 0.0096207  0.00557089 0.0061091  0.6571511  0.00718043\n",
            " 0.01108451 0.21130633 0.0074821  0.00668197 0.00645807 0.00588312\n",
            " 0.00531546 0.00511463]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00492926 0.0084215  0.00498707 0.00725095 0.00519615 0.00579749\n",
            " 0.00650307 0.0071926  0.00454474 0.00502206 0.6850812  0.0058103\n",
            " 0.00998343 0.2088774  0.00447701 0.00600293 0.00554949 0.00564511\n",
            " 0.00449083 0.00423742]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00640936 0.01059797 0.0067044  0.00919752 0.00695593 0.00695128\n",
            " 0.00780999 0.00867679 0.00536607 0.0067996  0.66953707 0.00766719\n",
            " 0.01458698 0.1926108  0.00489213 0.00766678 0.0069969  0.00712811\n",
            " 0.00776795 0.00567717]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00586982 0.01308714 0.00622469 0.01086541 0.00736947 0.00764739\n",
            " 0.00900374 0.01085522 0.00580078 0.00645591 0.66342205 0.00646778\n",
            " 0.01214261 0.19880477 0.00477022 0.00648711 0.00661671 0.00694301\n",
            " 0.00544725 0.0057189 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00486931 0.00908133 0.00521354 0.0083419  0.00601403 0.00632898\n",
            " 0.00768096 0.00774958 0.00458637 0.00457798 0.6703811  0.00768145\n",
            " 0.01235597 0.21494445 0.00467756 0.00567917 0.00547869 0.0060341\n",
            " 0.00451208 0.00381138]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00756365 0.01208841 0.0068049  0.01103522 0.00627997 0.0076096\n",
            " 0.00855543 0.01041901 0.00611073 0.00801217 0.6409221  0.01205284\n",
            " 0.01666314 0.20366365 0.00604313 0.00850935 0.00882224 0.00733644\n",
            " 0.00489247 0.00661555]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00523651 0.00923808 0.00647542 0.00949576 0.00671741 0.00616114\n",
            " 0.00807852 0.00910621 0.00471692 0.006125   0.6678288  0.00637656\n",
            " 0.01430402 0.20721418 0.00410256 0.00596901 0.00674846 0.00620193\n",
            " 0.00573423 0.00416931]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00521776 0.00974583 0.00509085 0.00873292 0.00571621 0.00705106\n",
            " 0.00719768 0.00953444 0.00609932 0.00494004 0.67115444 0.00495204\n",
            " 0.00960819 0.21268314 0.00392818 0.00606259 0.00657144 0.00614886\n",
            " 0.0051719  0.00439311]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00596361 0.01247091 0.00639399 0.00960919 0.00683442 0.00685274\n",
            " 0.00933153 0.00875233 0.00518172 0.0068386  0.65977633 0.00853714\n",
            " 0.0134387  0.20458862 0.00643842 0.00707595 0.00637536 0.00661469\n",
            " 0.00445119 0.00447456]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00588154 0.01055511 0.00835814 0.01015381 0.00775939 0.00729857\n",
            " 0.00879072 0.01089219 0.00607594 0.00623987 0.6312952  0.01300572\n",
            " 0.02363248 0.2037113  0.01177841 0.00845549 0.00769178 0.00743614\n",
            " 0.00565185 0.00533636]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00489698 0.0090342  0.00525023 0.00801154 0.00611962 0.00622676\n",
            " 0.00700466 0.00755298 0.00430649 0.00529551 0.6718598  0.00694168\n",
            " 0.01056137 0.21376176 0.0047981  0.00617815 0.005688   0.00637784\n",
            " 0.00523546 0.0048989 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00495395 0.00959897 0.00532796 0.00837526 0.0063718  0.00633221\n",
            " 0.00730857 0.00835762 0.00417028 0.00536318 0.6747265  0.0077289\n",
            " 0.0156349  0.20339358 0.00432447 0.0064532  0.00554432 0.00592178\n",
            " 0.00608962 0.00402293]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00582679 0.00936796 0.0059431  0.00883233 0.0061631  0.00650232\n",
            " 0.00762646 0.00879615 0.0056277  0.00652045 0.6477559  0.01035476\n",
            " 0.0161592  0.21262693 0.00609369 0.00708745 0.00654394 0.0064782\n",
            " 0.01054418 0.00514936]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00546408 0.00999861 0.00578597 0.00858503 0.00651933 0.00681651\n",
            " 0.00782705 0.00823246 0.004832   0.00547824 0.66420376 0.0069048\n",
            " 0.01292928 0.21342592 0.0045698  0.00710026 0.00628021 0.00674548\n",
            " 0.00384575 0.00445543]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00549721 0.00951879 0.00583654 0.00929464 0.00621618 0.0065913\n",
            " 0.00758308 0.00840286 0.00451052 0.00578274 0.6694051  0.00734201\n",
            " 0.01132715 0.20988001 0.00496621 0.00625757 0.00572564 0.00674543\n",
            " 0.00458982 0.00452721]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00503338 0.00806064 0.00518604 0.00768119 0.00564288 0.00599133\n",
            " 0.00638232 0.00796031 0.00497383 0.00480978 0.6615218  0.00848966\n",
            " 0.009936   0.2235002  0.00501654 0.005536   0.00569086 0.00579242\n",
            " 0.00804469 0.00475011]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00573474 0.00911096 0.00591421 0.00820185 0.00627766 0.00684227\n",
            " 0.00739977 0.00875697 0.00578649 0.00612436 0.6597299  0.00572366\n",
            " 0.01066067 0.21816032 0.00429117 0.00633767 0.00655781 0.00622306\n",
            " 0.00721524 0.00495123]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00634089 0.00921874 0.00649152 0.00891316 0.00629696 0.00693337\n",
            " 0.00764098 0.00932841 0.00655267 0.00633775 0.6470661  0.0081331\n",
            " 0.011753   0.21687508 0.00629613 0.00743681 0.00706658 0.00675493\n",
            " 0.00784017 0.00672365]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0062431  0.01041308 0.0062228  0.00883375 0.00634359 0.00677866\n",
            " 0.00795604 0.00872225 0.00547846 0.00626588 0.6635542  0.00942612\n",
            " 0.01450559 0.20170853 0.00549191 0.00758057 0.00702023 0.00698462\n",
            " 0.00540875 0.0050619 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00606509 0.00983193 0.00655613 0.00926817 0.00630815 0.00696907\n",
            " 0.00754773 0.00940635 0.00617461 0.00632409 0.65074265 0.00844059\n",
            " 0.01487841 0.213195   0.00565363 0.00730543 0.00721666 0.0067191\n",
            " 0.00578936 0.00560787]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0052667  0.00962193 0.00563965 0.00857213 0.00547541 0.00603582\n",
            " 0.00674534 0.00891277 0.00488562 0.00534393 0.651109   0.01443613\n",
            " 0.01545461 0.21990189 0.00601319 0.005877   0.00575171 0.00571401\n",
            " 0.00454598 0.00469726]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00716082 0.01059557 0.00788435 0.00961551 0.00755413 0.00831564\n",
            " 0.00983022 0.00952082 0.00649856 0.00816822 0.620129   0.0117562\n",
            " 0.01576159 0.2194319  0.0088583  0.00968748 0.00818768 0.00808539\n",
            " 0.00690129 0.00605736]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00511001 0.0110697  0.0053791  0.00988284 0.00679087 0.00639851\n",
            " 0.00798429 0.00942779 0.00462817 0.00529953 0.6689822  0.00655446\n",
            " 0.01222765 0.2059239  0.00466548 0.00605307 0.00689215 0.00666743\n",
            " 0.00570476 0.00435806]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00548609 0.01006003 0.00587271 0.00883432 0.00625839 0.00653013\n",
            " 0.00809833 0.00801125 0.00504892 0.00588127 0.65227485 0.01214922\n",
            " 0.01475827 0.21412039 0.00571098 0.00676696 0.00623332 0.00683904\n",
            " 0.00578217 0.00528328]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00648234 0.01015651 0.00736269 0.00970313 0.0070275  0.00736216\n",
            " 0.0082753  0.0095809  0.0056519  0.0075149  0.65124017 0.00814526\n",
            " 0.01423462 0.20393339 0.00556354 0.00843399 0.00878565 0.00777421\n",
            " 0.00672435 0.00604751]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00675743 0.0104923  0.00707562 0.00926441 0.00685764 0.00699968\n",
            " 0.0086233  0.00883418 0.00596057 0.00713056 0.6379171  0.01229378\n",
            " 0.01626669 0.21180171 0.0076325  0.00930842 0.00770703 0.00753318\n",
            " 0.00587641 0.00566749]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00454814 0.00853501 0.00478888 0.00760062 0.00480787 0.00518136\n",
            " 0.00626435 0.00722149 0.00424308 0.00445574 0.68209934 0.00403372\n",
            " 0.00805551 0.22244646 0.00298017 0.00501269 0.00502287 0.0050289\n",
            " 0.0037887  0.00388514]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00647048 0.01136926 0.00659442 0.009399   0.00629914 0.00732904\n",
            " 0.00837839 0.00959041 0.00647321 0.00637422 0.64557964 0.01447892\n",
            " 0.01555158 0.20919982 0.00602425 0.00681338 0.00670339 0.00627957\n",
            " 0.00578988 0.00530201]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00779141 0.01142382 0.00645927 0.01028091 0.00706183 0.00841959\n",
            " 0.00878648 0.01065272 0.00764765 0.00623118 0.6510921  0.00942491\n",
            " 0.01424306 0.19338413 0.00690232 0.00743065 0.00973077 0.00794659\n",
            " 0.00851213 0.00657848]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00607261 0.01154411 0.00613076 0.00955568 0.00723717 0.00810488\n",
            " 0.0089602  0.0095392  0.00564073 0.00694146 0.6447047  0.01058397\n",
            " 0.01335448 0.21179229 0.00717015 0.00822931 0.0075194  0.00737896\n",
            " 0.0046578  0.00488213]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00978244 0.0157551  0.00994823 0.0134989  0.01258886 0.0095993\n",
            " 0.01265279 0.01428613 0.01253692 0.00904308 0.5907572  0.01417259\n",
            " 0.01858193 0.19443205 0.01284738 0.00836392 0.01209091 0.00943886\n",
            " 0.00926421 0.01035919]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00466833 0.00977109 0.00518007 0.00927855 0.00570448 0.00622933\n",
            " 0.00820509 0.0080059  0.00409165 0.00479214 0.6772765  0.00872475\n",
            " 0.01492998 0.20375177 0.00446728 0.00585078 0.00505794 0.00590643\n",
            " 0.0043545  0.00375341]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00593374 0.00900605 0.00606254 0.00843726 0.00600065 0.00632052\n",
            " 0.00744456 0.00822623 0.00508066 0.00650121 0.6609718  0.00850884\n",
            " 0.0131359  0.21473391 0.00544063 0.00709816 0.00642855 0.00608939\n",
            " 0.00424569 0.00433367]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00584154 0.01026873 0.0059014  0.00915121 0.00661313 0.00665424\n",
            " 0.00832795 0.00855838 0.00484201 0.00569328 0.66137993 0.01038085\n",
            " 0.01358567 0.20859727 0.00616513 0.00655263 0.00616717 0.00644465\n",
            " 0.00432577 0.00454907]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00669049 0.00975958 0.00645418 0.00917375 0.0073908  0.00724864\n",
            " 0.00825823 0.00972808 0.00585915 0.00613067 0.6521419  0.00950005\n",
            " 0.01358686 0.20933108 0.00634713 0.00670095 0.00676825 0.0069017\n",
            " 0.00607681 0.00595162]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00845498 0.01285966 0.00861377 0.01134011 0.00740396 0.00828339\n",
            " 0.01151588 0.01111655 0.00709603 0.00848918 0.63470167 0.00961928\n",
            " 0.01493309 0.19318224 0.00801337 0.00778991 0.00827418 0.00799209\n",
            " 0.01153525 0.00878545]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00541315 0.00992736 0.00586503 0.00864014 0.00540857 0.00619518\n",
            " 0.00730964 0.00876534 0.00487674 0.00534564 0.6662287  0.00928923\n",
            " 0.01708002 0.20794179 0.00565187 0.00658157 0.00537759 0.00594701\n",
            " 0.00363846 0.00451695]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00409679 0.0088046  0.00483842 0.00804642 0.00502083 0.00504529\n",
            " 0.00640259 0.00796132 0.00376084 0.00503319 0.6815491  0.00793489\n",
            " 0.01552803 0.2103162  0.00376409 0.00588007 0.00462713 0.00486347\n",
            " 0.00309343 0.00343333]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0050861  0.0105358  0.0055051  0.00892928 0.00651058 0.00707812\n",
            " 0.00921976 0.00803198 0.00453261 0.00537791 0.6684502  0.00727986\n",
            " 0.01321162 0.2102969  0.00479688 0.00593218 0.00544791 0.00578269\n",
            " 0.0039686  0.00402597]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00520461 0.00919572 0.00531863 0.00842066 0.00628732 0.00652878\n",
            " 0.00766585 0.00778454 0.00467849 0.0048942  0.6682562  0.00768915\n",
            " 0.01118813 0.21393447 0.00536035 0.00616895 0.00579515 0.00625336\n",
            " 0.00515763 0.00421778]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00626033 0.01167988 0.00781184 0.01074375 0.00840896 0.00784192\n",
            " 0.00917926 0.0104842  0.00652877 0.0090288  0.6430156  0.01044356\n",
            " 0.01287856 0.19023007 0.00826626 0.00978681 0.00992512 0.00832452\n",
            " 0.0114783  0.0076835 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00651035 0.01129903 0.00746543 0.00955803 0.00738052 0.00720571\n",
            " 0.00851296 0.00989314 0.00527562 0.00754343 0.6341128  0.01361596\n",
            " 0.02166194 0.20782635 0.00830049 0.00894099 0.00717711 0.00689145\n",
            " 0.00547176 0.00535695]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00564982 0.01246401 0.00664619 0.01200267 0.00760305 0.00676372\n",
            " 0.00870043 0.00975843 0.0061329  0.00631807 0.6484473  0.00755882\n",
            " 0.01424788 0.20198742 0.00579959 0.0074373  0.00751341 0.00808484\n",
            " 0.01048376 0.00640042]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00578187 0.01011674 0.00660326 0.00929125 0.00739972 0.0077707\n",
            " 0.00836559 0.00861447 0.00559614 0.00867504 0.6375694  0.01159999\n",
            " 0.01297038 0.2111719  0.00760713 0.01099499 0.00766729 0.00869396\n",
            " 0.00716975 0.00634041]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00533473 0.01001652 0.00543316 0.00837686 0.00600728 0.00646255\n",
            " 0.0067749  0.00819057 0.0049407  0.00636961 0.66328865 0.0089192\n",
            " 0.01438349 0.20950785 0.00492213 0.00741347 0.00642868 0.00657363\n",
            " 0.00531654 0.00533944]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0054716  0.00891111 0.00661217 0.01005659 0.00729663 0.00698927\n",
            " 0.00820698 0.01162417 0.00495885 0.00619757 0.6582103  0.00766403\n",
            " 0.01177832 0.21075185 0.00606572 0.00683454 0.00621124 0.00607679\n",
            " 0.00524935 0.00483298]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0051803  0.00856838 0.0055432  0.00828033 0.00564361 0.00601425\n",
            " 0.0067565  0.00807524 0.00462148 0.00514189 0.6504098  0.01210989\n",
            " 0.01531707 0.22487989 0.00561255 0.00603062 0.00584213 0.00589728\n",
            " 0.00518658 0.00488896]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00568287 0.01069853 0.00605467 0.00977199 0.00678573 0.00787219\n",
            " 0.00945645 0.00880994 0.00678324 0.00586159 0.6596953  0.00673046\n",
            " 0.01291136 0.20695424 0.004914   0.00662783 0.0066368  0.00721717\n",
            " 0.0053518  0.00518384]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00640496 0.01026664 0.00668526 0.00925077 0.00688172 0.00687262\n",
            " 0.00790475 0.00952071 0.00594855 0.00742811 0.6416806  0.01120305\n",
            " 0.01587425 0.2139342  0.00653752 0.00776188 0.0070479  0.00697073\n",
            " 0.00597712 0.00584864]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00528567 0.00861104 0.00548459 0.00782571 0.00532093 0.00579865\n",
            " 0.00647676 0.00826168 0.00507456 0.00561828 0.6723038  0.00632132\n",
            " 0.01170269 0.21535984 0.00411063 0.00594679 0.00566138 0.00553849\n",
            " 0.00439919 0.00489809]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00617837 0.01067926 0.00687486 0.00936438 0.00747739 0.00731901\n",
            " 0.00888624 0.00954186 0.00604734 0.00728323 0.662763   0.00864882\n",
            " 0.01267667 0.19621703 0.00654315 0.00744892 0.00684584 0.00708774\n",
            " 0.00629402 0.00582286]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00540062 0.00970505 0.00560382 0.00861453 0.0070193  0.00691044\n",
            " 0.00775635 0.0085675  0.00589411 0.006234   0.6592488  0.00892715\n",
            " 0.01090514 0.21036457 0.0063716  0.00693483 0.00694893 0.00698552\n",
            " 0.00583826 0.00576947]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00532538 0.01004453 0.00574288 0.00896602 0.00598852 0.00651392\n",
            " 0.00711694 0.00867152 0.00530368 0.00559867 0.6594467  0.00712772\n",
            " 0.0125943  0.213729   0.00456498 0.00691735 0.00697957 0.00674561\n",
            " 0.00701864 0.00560405]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00542317 0.01149079 0.00651688 0.01215242 0.00704767 0.00699294\n",
            " 0.00819505 0.01000116 0.00455904 0.00645641 0.6691551  0.0065678\n",
            " 0.01191296 0.19882697 0.00490699 0.00672043 0.00596084 0.00701659\n",
            " 0.00492949 0.00516727]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00534949 0.00889425 0.006302   0.00888637 0.00618429 0.0066376\n",
            " 0.00727374 0.00935695 0.00487303 0.00684871 0.6634697  0.01007858\n",
            " 0.01279886 0.2112465  0.00536349 0.00662066 0.00559672 0.0057268\n",
            " 0.00428075 0.00421156]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00617654 0.01012145 0.00623019 0.00879622 0.00685006 0.00694236\n",
            " 0.00841743 0.00887265 0.00590776 0.00613922 0.6479858  0.00999539\n",
            " 0.0124118  0.21519136 0.00774888 0.007472   0.00697462 0.00687324\n",
            " 0.00569427 0.00519881]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00554179 0.01085087 0.00536305 0.00827614 0.00693432 0.00648908\n",
            " 0.00746905 0.0087597  0.00555567 0.00493057 0.6708048  0.00623053\n",
            " 0.01221654 0.20642278 0.00437957 0.00598921 0.00770863 0.00644435\n",
            " 0.00503781 0.00459555]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00680993 0.01025654 0.00684649 0.00941717 0.00757369 0.00782357\n",
            " 0.00822696 0.00907863 0.00515559 0.00679304 0.6600783  0.00985145\n",
            " 0.01537036 0.19566408 0.0061434  0.00711858 0.00673996 0.00802519\n",
            " 0.00720865 0.00581843]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0049241  0.01078892 0.00519873 0.00937039 0.00617942 0.00669196\n",
            " 0.00829475 0.00893599 0.00455545 0.00478128 0.67384183 0.0059208\n",
            " 0.01091038 0.2102283  0.00428627 0.00586611 0.00546389 0.00596405\n",
            " 0.00379157 0.00400581]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0050299  0.00894472 0.00524478 0.00783501 0.00608777 0.0062969\n",
            " 0.00689004 0.00770686 0.00456887 0.00536466 0.66077644 0.01170287\n",
            " 0.01789803 0.21270886 0.00521789 0.00627863 0.00536458 0.00611743\n",
            " 0.00532993 0.00463583]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00594668 0.00995311 0.00606862 0.00905722 0.00663508 0.00750425\n",
            " 0.00781085 0.00884573 0.00561095 0.00594786 0.6662226  0.00826473\n",
            " 0.0136553  0.19561474 0.00531393 0.00692289 0.00676832 0.00775388\n",
            " 0.0100453  0.00605802]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00792411 0.0152557  0.01343384 0.01710156 0.01021528 0.01036711\n",
            " 0.01599414 0.01306168 0.00891876 0.00901584 0.5748193  0.01476345\n",
            " 0.01828331 0.19888815 0.01596788 0.01275826 0.01062067 0.01106315\n",
            " 0.00844395 0.01310384]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00473544 0.00868482 0.00523189 0.0084905  0.00573779 0.00539163\n",
            " 0.00775141 0.00738902 0.00483854 0.00509947 0.6858041  0.00592453\n",
            " 0.0097596  0.20377679 0.00451333 0.00610063 0.00595621 0.00623193\n",
            " 0.004515   0.00406735]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00527482 0.01065057 0.00540757 0.00918272 0.00634469 0.00681166\n",
            " 0.0081085  0.00904886 0.00459374 0.0054767  0.6627111  0.00955443\n",
            " 0.01240646 0.21388623 0.00499446 0.00637953 0.00553669 0.00602974\n",
            " 0.00361585 0.00398572]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00765827 0.01174001 0.00643573 0.00884555 0.00693136 0.0079922\n",
            " 0.00770584 0.00937887 0.0075505  0.00720389 0.6440496  0.00826457\n",
            " 0.01326946 0.20789227 0.00531393 0.00824528 0.00895796 0.00762974\n",
            " 0.00678972 0.00814524]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00561663 0.01179131 0.00607863 0.01009869 0.00658658 0.00781151\n",
            " 0.00867098 0.00925595 0.00600319 0.00654691 0.66071403 0.00902252\n",
            " 0.0130717  0.19424331 0.00562089 0.0079222  0.00667004 0.0080856\n",
            " 0.00962435 0.00656495]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00600257 0.0116613  0.00650275 0.01083705 0.00847756 0.00803003\n",
            " 0.00994663 0.00986126 0.0064465  0.0075148  0.6440723  0.01028225\n",
            " 0.01274862 0.20389836 0.00732566 0.00849756 0.00842363 0.0081397\n",
            " 0.00568652 0.00564499]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00553913 0.00900664 0.00564754 0.00845194 0.00597589 0.00614835\n",
            " 0.00770124 0.00796562 0.00538875 0.00532023 0.6665625  0.00903319\n",
            " 0.01327732 0.21052372 0.00544111 0.0065154  0.00605191 0.00606429\n",
            " 0.00503703 0.00434822]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00518395 0.00863795 0.00532488 0.00849146 0.00554921 0.00632026\n",
            " 0.00702085 0.00828469 0.00450199 0.00508936 0.6765982  0.00518379\n",
            " 0.01098781 0.21457627 0.00368335 0.00596223 0.00547838 0.00606734\n",
            " 0.0031112  0.00394677]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00667292 0.01011849 0.00898097 0.01022494 0.00726347 0.00824901\n",
            " 0.00970066 0.01157674 0.0057036  0.00742551 0.60340536 0.0127448\n",
            " 0.02455243 0.23122106 0.01092727 0.00812643 0.00610068 0.00695925\n",
            " 0.00504189 0.00500454]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00479999 0.00994952 0.00553086 0.00870483 0.00579984 0.00577854\n",
            " 0.00739795 0.00891792 0.00412431 0.00527674 0.67052877 0.00841021\n",
            " 0.01528448 0.21094029 0.00510448 0.00583085 0.0047937  0.00544852\n",
            " 0.00366155 0.00371663]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00566677 0.00871204 0.00571866 0.00788182 0.00549397 0.00598953\n",
            " 0.00689999 0.00846599 0.00571443 0.00530766 0.66636014 0.00608737\n",
            " 0.01221816 0.21709165 0.00436764 0.00588468 0.00601162 0.00556023\n",
            " 0.00537872 0.00518893]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00534698 0.00965694 0.00620008 0.0088591  0.00613241 0.00684706\n",
            " 0.00779294 0.00910871 0.00550202 0.00567619 0.6489825  0.01582719\n",
            " 0.0170916  0.21153332 0.00620086 0.00650119 0.00615708 0.00627605\n",
            " 0.00540264 0.00490511]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00567824 0.01021254 0.00662751 0.0088921  0.00698704 0.00797094\n",
            " 0.00876799 0.00978238 0.0052832  0.00633621 0.6543027  0.0075541\n",
            " 0.01707605 0.20419171 0.00575493 0.00692423 0.00621497 0.00620725\n",
            " 0.00928887 0.00594702]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00581691 0.00936772 0.00600764 0.00964219 0.00666002 0.00703077\n",
            " 0.0075489  0.00915765 0.00584859 0.0067119  0.6616902  0.00642005\n",
            " 0.01008986 0.20867239 0.00505244 0.00755404 0.00740579 0.00747202\n",
            " 0.00633328 0.00551768]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00630551 0.01118396 0.00645997 0.00964924 0.00672817 0.00846935\n",
            " 0.00837124 0.00933606 0.00681423 0.00627759 0.67075884 0.00651547\n",
            " 0.01261916 0.18925722 0.00516889 0.0071493  0.00731462 0.00810735\n",
            " 0.00743813 0.00607573]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0063774  0.01197675 0.0067452  0.01111255 0.00682354 0.0078892\n",
            " 0.00909275 0.01013537 0.00513505 0.00620375 0.6714211  0.00749595\n",
            " 0.01413885 0.18927991 0.004862   0.00659272 0.00609227 0.00717655\n",
            " 0.00575252 0.00569658]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00482448 0.00808126 0.00516744 0.0078885  0.00604874 0.00635179\n",
            " 0.00724264 0.00767993 0.00487063 0.00528287 0.67713475 0.00695641\n",
            " 0.00907829 0.21137469 0.00569927 0.00590638 0.00589478 0.00594857\n",
            " 0.00472873 0.00383984]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0048113  0.00891567 0.00513304 0.00835502 0.00501853 0.00561615\n",
            " 0.006314   0.00841691 0.0042555  0.00503846 0.6543107  0.0083835\n",
            " 0.01500853 0.23111853 0.00467156 0.0058391  0.00535909 0.00521532\n",
            " 0.00384818 0.0043709 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00524603 0.01260596 0.00574981 0.00811798 0.00612124 0.00727746\n",
            " 0.00924145 0.0081058  0.00454607 0.00525367 0.67302525 0.00680743\n",
            " 0.01581831 0.2016346  0.00479804 0.00645091 0.00551427 0.00600618\n",
            " 0.00372443 0.00395509]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00642841 0.00979521 0.00653658 0.00924086 0.00666532 0.00724674\n",
            " 0.00817522 0.0091967  0.0066302  0.00698478 0.6398556  0.00926092\n",
            " 0.01394619 0.21668711 0.00650602 0.0080302  0.00791713 0.00744036\n",
            " 0.0072566  0.00619979]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00513359 0.01147931 0.0055328  0.01091184 0.00609948 0.00680787\n",
            " 0.00868846 0.00970586 0.00560165 0.00519715 0.67993605 0.00535226\n",
            " 0.01021475 0.19620714 0.00445068 0.00609723 0.00612157 0.00679322\n",
            " 0.00480772 0.00486136]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00603828 0.01095734 0.00626077 0.01020317 0.00697517 0.00736281\n",
            " 0.00847682 0.00953905 0.00616024 0.00612638 0.6509744  0.00908715\n",
            " 0.01213155 0.21141921 0.00599824 0.00699886 0.00692137 0.00693147\n",
            " 0.00637578 0.00506196]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00792162 0.0111966  0.00758826 0.00962161 0.00727815 0.00808588\n",
            " 0.00898859 0.00948445 0.00630938 0.00685119 0.62937176 0.00929738\n",
            " 0.01621809 0.21562672 0.00727634 0.00899043 0.00831331 0.0086254\n",
            " 0.00672207 0.00623279]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00599068 0.01184207 0.00618044 0.00920479 0.00717693 0.00785259\n",
            " 0.00930759 0.0095849  0.00616412 0.0062154  0.65829897 0.00816156\n",
            " 0.0157262  0.1987508  0.00543659 0.00713145 0.00724224 0.00674957\n",
            " 0.00742665 0.00555645]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00634741 0.01233492 0.0068131  0.0098662  0.00748982 0.00807075\n",
            " 0.00893546 0.00921429 0.00613029 0.00688596 0.6507443  0.00889692\n",
            " 0.01286921 0.20331149 0.00619949 0.00776076 0.00737168 0.00810757\n",
            " 0.00613163 0.00651877]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00556603 0.00920066 0.00589005 0.00869033 0.005829   0.00630603\n",
            " 0.00816098 0.00875921 0.00526701 0.00552635 0.6542251  0.0120273\n",
            " 0.0153715  0.21387881 0.00790896 0.00667434 0.00571639 0.00601418\n",
            " 0.00441695 0.00457081]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00718374 0.01090547 0.00647348 0.01044052 0.00661774 0.00770286\n",
            " 0.0082728  0.01147758 0.00740214 0.00645356 0.65289116 0.00880623\n",
            " 0.0126585  0.20150119 0.00651131 0.00756117 0.00736187 0.00727994\n",
            " 0.00559351 0.00690522]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00705152 0.0128154  0.00712183 0.01047876 0.00734225 0.00839887\n",
            " 0.00994907 0.0094664  0.00543433 0.00749169 0.6517949  0.01011187\n",
            " 0.01433394 0.19609962 0.00750952 0.00834525 0.00711495 0.00816168\n",
            " 0.00523275 0.00574537]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0056931  0.01026772 0.00571633 0.00764362 0.00622289 0.00637085\n",
            " 0.00804861 0.00728444 0.00488647 0.00705569 0.6699045  0.00983376\n",
            " 0.01375335 0.20289205 0.00475075 0.00765397 0.00724234 0.00589221\n",
            " 0.00430596 0.00458145]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.005111   0.00880152 0.00511902 0.00800951 0.00607765 0.0063223\n",
            " 0.00694372 0.00784449 0.00494498 0.00557728 0.664745   0.01115767\n",
            " 0.01191526 0.21348664 0.00563453 0.00689495 0.00601027 0.00631817\n",
            " 0.00450776 0.00457834]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00669522 0.0103469  0.0069891  0.01037215 0.00759693 0.00733864\n",
            " 0.00914544 0.01023224 0.00545922 0.00729701 0.63513505 0.0124553\n",
            " 0.01467827 0.21196897 0.00911856 0.00716664 0.00666581 0.00702973\n",
            " 0.00875393 0.00555489]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00577291 0.01244157 0.00620861 0.01104951 0.00662133 0.00734663\n",
            " 0.00921804 0.01097132 0.00501224 0.00645741 0.6599814  0.01006556\n",
            " 0.01440406 0.20091574 0.00615102 0.00657617 0.00593114 0.00641597\n",
            " 0.00397213 0.00448724]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00518594 0.00950689 0.00551263 0.00879779 0.00575442 0.00649915\n",
            " 0.00689359 0.00950015 0.00570166 0.00569695 0.67011774 0.00642115\n",
            " 0.01162619 0.21010014 0.00454075 0.00635097 0.00590568 0.00620813\n",
            " 0.00507529 0.00460474]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00534713 0.00843952 0.00610761 0.0095231  0.00600454 0.00631049\n",
            " 0.0071102  0.00883414 0.00598887 0.00590342 0.6569918  0.00546376\n",
            " 0.00868811 0.22150414 0.00520242 0.00618222 0.0065037  0.00712779\n",
            " 0.00678259 0.00598447]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00500729 0.0103508  0.00586984 0.01054214 0.00658172 0.00677098\n",
            " 0.00859435 0.00855783 0.00423608 0.00589661 0.6696649  0.00704768\n",
            " 0.01187549 0.20673531 0.00505916 0.00635829 0.00560196 0.00672544\n",
            " 0.00441754 0.00410656]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00619227 0.01059966 0.00671428 0.01165361 0.00713065 0.00710033\n",
            " 0.0077791  0.0111453  0.006063   0.00703658 0.64750004 0.0088763\n",
            " 0.0124209  0.20820682 0.00599961 0.0081039  0.00755118 0.0075531\n",
            " 0.00625843 0.00611494]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00461864 0.01096695 0.00504455 0.00926779 0.0062254  0.00594721\n",
            " 0.00854671 0.007869   0.00376289 0.00490244 0.67984545 0.00469967\n",
            " 0.00986622 0.21180658 0.00357041 0.00521014 0.00515878 0.0056085\n",
            " 0.0033839  0.00369879]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00538966 0.00842614 0.00588316 0.00824326 0.00622815 0.00675235\n",
            " 0.00754297 0.00825153 0.00577761 0.00557483 0.66258043 0.00624327\n",
            " 0.01015199 0.2145209  0.00575125 0.00680711 0.00690863 0.00672171\n",
            " 0.00730109 0.00494399]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00589059 0.00930771 0.006519   0.0093394  0.00733454 0.00751296\n",
            " 0.00810261 0.00930987 0.00579633 0.00745952 0.63872766 0.01226143\n",
            " 0.01367335 0.2156484  0.00768815 0.00932614 0.00717413 0.00768909\n",
            " 0.00596739 0.00527171]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00567759 0.00895869 0.00645938 0.0108872  0.00725915 0.00685769\n",
            " 0.00725114 0.01082151 0.00660427 0.01020916 0.65176654 0.00952664\n",
            " 0.01298547 0.20165719 0.00538627 0.00812236 0.00860634 0.00749086\n",
            " 0.00761986 0.00585264]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00596614 0.01125348 0.00593467 0.00924289 0.00686281 0.00678805\n",
            " 0.00853971 0.00906233 0.00554942 0.00642653 0.6562953  0.01151657\n",
            " 0.01459787 0.20638755 0.00638116 0.00772363 0.00636106 0.00663006\n",
            " 0.0039399  0.00454088]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00704019 0.01330465 0.00773117 0.01103278 0.0069515  0.00870833\n",
            " 0.00898771 0.01154999 0.0064925  0.00712553 0.65251756 0.00777901\n",
            " 0.01360879 0.195003   0.00532972 0.00757728 0.00747522 0.00764513\n",
            " 0.00721132 0.00692859]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00518392 0.00888845 0.00582488 0.0090213  0.00611985 0.00602232\n",
            " 0.0069897  0.00871884 0.00559552 0.00714552 0.6617408  0.00745713\n",
            " 0.01055693 0.21310054 0.00558124 0.00706974 0.00692638 0.00649396\n",
            " 0.00591002 0.00565292]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00567245 0.01013225 0.00566688 0.00852828 0.00611122 0.00668669\n",
            " 0.00857346 0.00786844 0.00484925 0.00553127 0.6573917  0.00799117\n",
            " 0.0117615  0.21993004 0.00520386 0.00648049 0.00609938 0.00623883\n",
            " 0.00451606 0.00476672]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00680044 0.0118952  0.00689684 0.00945406 0.00698908 0.00757908\n",
            " 0.00927485 0.00881291 0.0055783  0.00661791 0.63431036 0.01183953\n",
            " 0.01900438 0.21378115 0.00722759 0.00832792 0.00696474 0.00752506\n",
            " 0.00542307 0.00569751]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00516201 0.0080897  0.00529039 0.00785129 0.00570164 0.00587163\n",
            " 0.00672668 0.00800601 0.00518828 0.00498341 0.65672755 0.0081334\n",
            " 0.01030591 0.22572586 0.00572357 0.0057354  0.00607065 0.00581389\n",
            " 0.00819757 0.00469519]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00455292 0.00806781 0.00505941 0.00731331 0.00550567 0.00542621\n",
            " 0.0064166  0.00748368 0.00445587 0.00483125 0.6839139  0.00606662\n",
            " 0.00996316 0.21169958 0.00499528 0.00546333 0.00497953 0.00533952\n",
            " 0.00454962 0.00391679]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00556643 0.0099359  0.00573906 0.00886035 0.0063933  0.00656953\n",
            " 0.00755993 0.0086333  0.00402017 0.00598015 0.66167927 0.010604\n",
            " 0.01788577 0.20792517 0.0054607  0.00627853 0.00558387 0.00620988\n",
            " 0.00486742 0.00424729]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00626175 0.00961368 0.00605334 0.0087457  0.00654197 0.00698486\n",
            " 0.00791818 0.00822687 0.00518299 0.00757967 0.63930523 0.01374128\n",
            " 0.01351625 0.21777737 0.00685212 0.00959416 0.00700694 0.00807634\n",
            " 0.00513193 0.00588933]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00585724 0.01068421 0.00640682 0.01074173 0.00632838 0.0068023\n",
            " 0.00782484 0.01084465 0.005385   0.00610038 0.6595819  0.01034609\n",
            " 0.01259951 0.20431356 0.00576713 0.0067439  0.00663748 0.00646719\n",
            " 0.00547887 0.00508882]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00499896 0.0084365  0.00529906 0.00826653 0.00607228 0.00599411\n",
            " 0.00747968 0.00792159 0.004738   0.00539926 0.6696994  0.0068234\n",
            " 0.01040576 0.21533498 0.00479856 0.0064053  0.00608922 0.00664593\n",
            " 0.00502266 0.0041688 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00611256 0.01197725 0.0070182  0.01146153 0.00778573 0.00744759\n",
            " 0.00931021 0.00990775 0.00563116 0.00772118 0.6371801  0.01222283\n",
            " 0.01483128 0.1997872  0.00834417 0.00869742 0.00740229 0.00902277\n",
            " 0.01065144 0.00748731]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00589026 0.01111997 0.0062154  0.00897782 0.00733284 0.00724235\n",
            " 0.00836787 0.00888574 0.00586681 0.00618666 0.6663674  0.00742443\n",
            " 0.0126393  0.19546902 0.00568912 0.00730781 0.00837014 0.0079413\n",
            " 0.00680739 0.00589838]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00642918 0.00887134 0.00612222 0.00756218 0.00618475 0.00620507\n",
            " 0.00677752 0.00802187 0.00534858 0.00593764 0.67043114 0.00809224\n",
            " 0.0136472  0.20645322 0.00535621 0.00645905 0.00633152 0.00576202\n",
            " 0.00526191 0.00474515]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00530446 0.0095434  0.00531444 0.00921044 0.00677492 0.00681376\n",
            " 0.00894243 0.00900188 0.00548881 0.00514706 0.6673306  0.00734496\n",
            " 0.01265393 0.20849411 0.0048527  0.00631213 0.00650267 0.00649516\n",
            " 0.00423051 0.00424161]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00491115 0.00905158 0.00511999 0.00851471 0.00582558 0.00642635\n",
            " 0.00680299 0.00846605 0.00416955 0.0057543  0.6812195  0.00546481\n",
            " 0.00986835 0.20961972 0.00397627 0.00584751 0.00514443 0.0057668\n",
            " 0.00392015 0.00413018]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00695596 0.01090806 0.00750202 0.00931299 0.00665405 0.00744856\n",
            " 0.00892177 0.00917876 0.00607609 0.00708223 0.6442628  0.00905889\n",
            " 0.01481764 0.20881689 0.00735814 0.00751445 0.00684803 0.00813855\n",
            " 0.00648657 0.00665759]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0061092  0.01020327 0.0061172  0.01019529 0.00680282 0.00770455\n",
            " 0.00809929 0.00956458 0.00658402 0.00605989 0.6487331  0.01111299\n",
            " 0.01449816 0.20633693 0.00605283 0.00717213 0.00819839 0.0084067\n",
            " 0.00626077 0.0057879 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00613134 0.01022753 0.00672458 0.00886815 0.00716909 0.00728672\n",
            " 0.00857706 0.0088069  0.00613822 0.00743919 0.6305066  0.01457676\n",
            " 0.0191045  0.2130682  0.00776073 0.00871956 0.00763184 0.0072476\n",
            " 0.00849463 0.00552086]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00537203 0.01051101 0.00541346 0.00884965 0.00654697 0.00723144\n",
            " 0.00869841 0.00849007 0.00483602 0.00548067 0.66791356 0.00848238\n",
            " 0.01397075 0.20650814 0.00506178 0.00658535 0.00618196 0.00609111\n",
            " 0.00377504 0.00400013]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0054097  0.00987249 0.00600035 0.00931924 0.00606613 0.00655317\n",
            " 0.00797303 0.00924538 0.00528835 0.0059092  0.6455604  0.01546138\n",
            " 0.01618549 0.21624023 0.00654772 0.00619635 0.00600012 0.00626017\n",
            " 0.00499667 0.00491445]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00516182 0.00921626 0.00553569 0.00894086 0.00623245 0.00653621\n",
            " 0.00768992 0.00848669 0.00505491 0.00504672 0.66443324 0.00743431\n",
            " 0.01131682 0.2149722  0.00530953 0.00654407 0.00623932 0.006358\n",
            " 0.00530318 0.00418782]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00571772 0.00947549 0.00597704 0.00870009 0.00649972 0.00689483\n",
            " 0.00799001 0.00832874 0.00562381 0.00638532 0.6431478  0.00914899\n",
            " 0.01202778 0.22332096 0.00608062 0.00725743 0.00705216 0.00697602\n",
            " 0.0076495  0.00574594]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00585559 0.01035992 0.00644259 0.00910568 0.00726179 0.00724531\n",
            " 0.00861819 0.0083129  0.00484689 0.00584941 0.6663703  0.00700175\n",
            " 0.01251842 0.20395947 0.00588769 0.00718284 0.00670056 0.0071894\n",
            " 0.0046384  0.00465297]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00511696 0.00984665 0.00608106 0.01153411 0.0064086  0.0060569\n",
            " 0.00957124 0.00924047 0.00527228 0.00579945 0.6707961  0.00683001\n",
            " 0.01145507 0.19865312 0.00605767 0.00667361 0.00735887 0.00710262\n",
            " 0.00571139 0.00443386]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00504819 0.00904129 0.00520562 0.0078228  0.00561862 0.00576869\n",
            " 0.00635009 0.00775526 0.00492145 0.00492153 0.67382145 0.00779264\n",
            " 0.01248423 0.21170008 0.00434631 0.006599   0.00631567 0.00588069\n",
            " 0.00421032 0.00439607]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00536365 0.00896645 0.00600899 0.0089391  0.00653931 0.00706227\n",
            " 0.00795242 0.00897547 0.00532519 0.00600115 0.65121686 0.01088436\n",
            " 0.01312872 0.2181918  0.00592248 0.00673765 0.00658916 0.00638257\n",
            " 0.00500987 0.00480259]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00525141 0.00887863 0.00558834 0.00903367 0.00685351 0.00686674\n",
            " 0.00724014 0.00863129 0.00513711 0.00612859 0.6548542  0.01200962\n",
            " 0.01379529 0.2150103  0.00559097 0.00646762 0.00646922 0.00628191\n",
            " 0.00520503 0.00470644]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00626449 0.00918968 0.00621491 0.0098787  0.00717938 0.00792592\n",
            " 0.00793893 0.00972904 0.00754202 0.00673733 0.6510574  0.00835981\n",
            " 0.01082939 0.20872545 0.0064863  0.00766088 0.00800086 0.00783994\n",
            " 0.00686964 0.00556995]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00574383 0.01010798 0.00624848 0.00941342 0.00665026 0.00733823\n",
            " 0.0085501  0.00947028 0.00563532 0.00579661 0.6586056  0.00688424\n",
            " 0.01260092 0.21167299 0.00559161 0.0070761  0.00640483 0.00675165\n",
            " 0.00491041 0.00454714]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00465119 0.00989107 0.00519579 0.00884745 0.00512111 0.00568693\n",
            " 0.00743602 0.00846704 0.00430061 0.00521455 0.66177106 0.01257096\n",
            " 0.02411157 0.20691454 0.00450174 0.00580175 0.0048421  0.00544372\n",
            " 0.00538253 0.00384832]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00484346 0.00894437 0.00521414 0.00803108 0.00545776 0.00591146\n",
            " 0.00694723 0.00792964 0.00433591 0.00477602 0.67403543 0.00557824\n",
            " 0.01079231 0.22021341 0.00396281 0.00552501 0.0051425  0.00545626\n",
            " 0.00333144 0.00357148]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00514712 0.00860467 0.00529329 0.00767625 0.00561282 0.00571824\n",
            " 0.00642097 0.0080633  0.00439632 0.0051167  0.67768425 0.00867579\n",
            " 0.01342925 0.20832962 0.0052124  0.00568415 0.00515519 0.00528267\n",
            " 0.00426666 0.00423035]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00523938 0.00836315 0.00581649 0.00820703 0.00581043 0.00611601\n",
            " 0.00720802 0.00761823 0.00496243 0.00582181 0.66035044 0.00585908\n",
            " 0.00959317 0.22372475 0.00466294 0.00600388 0.00596198 0.00632339\n",
            " 0.00703028 0.00532709]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00732527 0.01168297 0.00873899 0.01210601 0.00842687 0.00839158\n",
            " 0.00860164 0.01160003 0.00904629 0.0097755  0.62238395 0.00970322\n",
            " 0.01400281 0.19762215 0.00722521 0.01011682 0.00897279 0.01105194\n",
            " 0.01099991 0.01222607]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00533809 0.00932346 0.00582153 0.00830161 0.00597338 0.00670853\n",
            " 0.00821372 0.0083156  0.00518513 0.00551074 0.6610049  0.00727325\n",
            " 0.01095588 0.21619622 0.00643666 0.00677903 0.00625053 0.0062002\n",
            " 0.00544221 0.00476938]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00620061 0.01154352 0.00636381 0.01063054 0.00677381 0.00739195\n",
            " 0.00832004 0.01036525 0.00634956 0.0066705  0.6474304  0.01263172\n",
            " 0.01537047 0.20227328 0.00643312 0.00754432 0.00758958 0.00797303\n",
            " 0.00570325 0.00644124]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.005507   0.00971754 0.00599286 0.00905753 0.00623606 0.00664084\n",
            " 0.00773816 0.00895281 0.00517148 0.00606672 0.6706753  0.00682381\n",
            " 0.01165122 0.20704103 0.00549571 0.00650805 0.0058738  0.0061392\n",
            " 0.00412319 0.00458768]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00682949 0.01265959 0.00692206 0.0111281  0.00718148 0.00813462\n",
            " 0.00932698 0.01139408 0.00659955 0.00683171 0.6506175  0.00863893\n",
            " 0.01323572 0.2024928  0.00640424 0.00702365 0.00701499 0.00695319\n",
            " 0.0052082  0.00540312]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00657658 0.01035189 0.00652063 0.00985668 0.00658776 0.00724199\n",
            " 0.00871462 0.00994829 0.00768716 0.00671649 0.6566802  0.00916754\n",
            " 0.01394834 0.19864647 0.00645973 0.00718997 0.00756261 0.00696277\n",
            " 0.00728475 0.0058955 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00622899 0.01198777 0.00688675 0.01041888 0.00739794 0.00791036\n",
            " 0.00915966 0.01126236 0.0065009  0.00651444 0.65724665 0.00815137\n",
            " 0.01318017 0.19711861 0.0072984  0.00792558 0.00745239 0.00739828\n",
            " 0.00480652 0.00515399]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00601884 0.01112495 0.00560441 0.00865124 0.0059665  0.00704744\n",
            " 0.00740087 0.00810473 0.00565154 0.00487026 0.68029094 0.00508719\n",
            " 0.0114856  0.19778368 0.00369374 0.00594606 0.00633603 0.0069215\n",
            " 0.00614204 0.00587242]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00631191 0.01259094 0.006341   0.00988913 0.00770948 0.00804419\n",
            " 0.00894169 0.01022039 0.00582098 0.00641962 0.65397656 0.0089542\n",
            " 0.01370071 0.20336975 0.00621579 0.00716745 0.00701566 0.00724239\n",
            " 0.00481767 0.00525051]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00752111 0.01043969 0.0071791  0.00920527 0.00668692 0.0069134\n",
            " 0.00783802 0.0100165  0.00554839 0.00859079 0.6562698  0.01005236\n",
            " 0.0147679  0.1979258  0.00616565 0.00823444 0.00787089 0.00710684\n",
            " 0.00535802 0.00630907]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00695797 0.00984299 0.00684452 0.01123174 0.00686992 0.00741123\n",
            " 0.00823533 0.01245829 0.00698304 0.00662898 0.63916445 0.00840992\n",
            " 0.01316594 0.2149285  0.00648908 0.00721032 0.00777826 0.00687175\n",
            " 0.00606586 0.00645193]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00581658 0.00981947 0.00622987 0.00983675 0.00666764 0.00752028\n",
            " 0.00959973 0.0099965  0.00720726 0.00600051 0.6492918  0.00790576\n",
            " 0.0129158  0.21127945 0.00555703 0.00693174 0.00713337 0.00778003\n",
            " 0.0068235  0.00568691]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00667338 0.01124817 0.00707368 0.01125267 0.00788138 0.00754392\n",
            " 0.01079885 0.01031181 0.00791793 0.00655012 0.6402556  0.0076524\n",
            " 0.01275761 0.20792845 0.00719253 0.00701109 0.00836591 0.008966\n",
            " 0.00608339 0.00653514]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0060882  0.01095936 0.00655895 0.00976786 0.00700813 0.00693551\n",
            " 0.00899947 0.00874334 0.00472481 0.00651409 0.65649664 0.00972703\n",
            " 0.01326128 0.2059162  0.00706582 0.00687926 0.00654224 0.00725146\n",
            " 0.00551239 0.00504797]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00539975 0.01007937 0.00594724 0.00866    0.00589497 0.00641605\n",
            " 0.00735503 0.00869883 0.00490842 0.00535233 0.652545   0.0173612\n",
            " 0.01590999 0.21219356 0.0062662  0.00588496 0.00608147 0.00581952\n",
            " 0.00462565 0.00460055]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00458699 0.01011259 0.0049035  0.00891176 0.00592197 0.00682975\n",
            " 0.0086344  0.00900742 0.00411967 0.00441488 0.6792113  0.0060693\n",
            " 0.01115124 0.21022189 0.00399342 0.00523604 0.00469635 0.00527584\n",
            " 0.00325665 0.00344502]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00586277 0.01019397 0.00613399 0.0082544  0.00609046 0.00682559\n",
            " 0.00708893 0.0089125  0.00504026 0.00530074 0.6716474  0.00674731\n",
            " 0.01364549 0.2057928  0.00432655 0.00586221 0.00592651 0.00609831\n",
            " 0.00512748 0.00512226]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00597754 0.00944595 0.00596552 0.00885987 0.00617616 0.00691077\n",
            " 0.00804191 0.00948622 0.00676492 0.00548403 0.66049194 0.00833878\n",
            " 0.01179363 0.20905434 0.00746632 0.00618531 0.00629701 0.00604764\n",
            " 0.00626609 0.004946  ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00543893 0.01145108 0.00598914 0.0095983  0.0064855  0.00667926\n",
            " 0.00855941 0.00880182 0.00456906 0.0060728  0.6658678  0.01111158\n",
            " 0.01871201 0.19726413 0.00612529 0.00676136 0.00552917 0.00635304\n",
            " 0.00402395 0.00460637]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00539083 0.00983774 0.0056394  0.00890411 0.006793   0.00669806\n",
            " 0.00787546 0.00873144 0.00520457 0.00584361 0.65817684 0.00746608\n",
            " 0.01073338 0.21760526 0.00515957 0.00646464 0.00661647 0.00648264\n",
            " 0.00530632 0.00507063]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00666613 0.01045273 0.00660537 0.0091063  0.00680812 0.00765983\n",
            " 0.00864763 0.00888362 0.00595612 0.00687457 0.63226926 0.01307995\n",
            " 0.01724934 0.2158678  0.00845685 0.00919085 0.00706749 0.00747995\n",
            " 0.00618567 0.00549246]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00489309 0.00815727 0.00517793 0.00734807 0.00514347 0.00518859\n",
            " 0.00605644 0.0072157  0.00415842 0.00530617 0.6902652  0.00616756\n",
            " 0.0103235  0.20658955 0.00424601 0.00555341 0.0051081  0.00552015\n",
            " 0.00360676 0.00397462]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00715559 0.01324106 0.00731768 0.0101466  0.0083896  0.0076611\n",
            " 0.00955393 0.00954324 0.00618266 0.00845659 0.64039946 0.01253579\n",
            " 0.01754068 0.18987207 0.00741362 0.00925591 0.00906117 0.0090264\n",
            " 0.0094249  0.0078219 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00498743 0.00895651 0.00569516 0.00848329 0.00626809 0.00664215\n",
            " 0.00799795 0.00828111 0.00482546 0.00562832 0.6675506  0.00727569\n",
            " 0.0118406  0.21400039 0.00512423 0.00660779 0.00572906 0.00605203\n",
            " 0.00394434 0.00410975]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00534084 0.01193397 0.00594061 0.01020616 0.00686765 0.00773749\n",
            " 0.00817956 0.00953725 0.00532587 0.00643773 0.6618283  0.00915587\n",
            " 0.01547225 0.1936999  0.00507416 0.00760561 0.00697316 0.00717692\n",
            " 0.00960746 0.00589927]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00459533 0.0083293  0.00505538 0.0079047  0.00549843 0.00576572\n",
            " 0.00663817 0.00798134 0.00431206 0.00530543 0.6813662  0.00750464\n",
            " 0.01133874 0.21041304 0.00492971 0.00578628 0.00496923 0.00517894\n",
            " 0.00345133 0.00367601]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00554104 0.00953139 0.00571069 0.00845811 0.00612634 0.00661047\n",
            " 0.00713793 0.00794969 0.00418055 0.00506981 0.68076277 0.00584361\n",
            " 0.0134972  0.20109953 0.00369947 0.00594572 0.00615747 0.00652795\n",
            " 0.00576879 0.0043815 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00609195 0.01121107 0.00599181 0.00946464 0.00733207 0.00722123\n",
            " 0.00832755 0.01003961 0.00650364 0.00702486 0.657519   0.00957236\n",
            " 0.01240022 0.20173348 0.00610148 0.00711442 0.00766272 0.0067527\n",
            " 0.00669564 0.00523952]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00621656 0.01254066 0.00647217 0.01063266 0.00699857 0.00755345\n",
            " 0.00863552 0.00947097 0.00550926 0.00660899 0.6502701  0.0095847\n",
            " 0.01479866 0.20541503 0.00550796 0.0073287  0.00671733 0.00778525\n",
            " 0.00603131 0.00592213]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00713931 0.01371344 0.00710806 0.01026616 0.00854086 0.00926578\n",
            " 0.00928388 0.01256409 0.00833764 0.00625618 0.6320567  0.00703923\n",
            " 0.01387775 0.20516771 0.00581528 0.00724576 0.00848791 0.00777493\n",
            " 0.01159921 0.0084601 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00500531 0.00867941 0.00514474 0.00782652 0.00545124 0.00565987\n",
            " 0.00667155 0.00738061 0.00450893 0.00504319 0.66959083 0.0055031\n",
            " 0.0097569  0.222064   0.00411609 0.00609781 0.00577357 0.00584306\n",
            " 0.00530582 0.00457744]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00461966 0.00885408 0.00508698 0.00782717 0.00551919 0.00671733\n",
            " 0.00798025 0.00750773 0.00413561 0.00481741 0.6753384  0.00520746\n",
            " 0.00990621 0.21966681 0.00376712 0.00560912 0.0052162  0.00548314\n",
            " 0.00324557 0.00349453]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0072299  0.00965608 0.0066898  0.00882661 0.00790302 0.00769014\n",
            " 0.00853307 0.00844515 0.00591721 0.00916916 0.63289446 0.01292343\n",
            " 0.01364856 0.21226165 0.00798781 0.00853424 0.0078213  0.00899894\n",
            " 0.0082657  0.00660377]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00558959 0.0113318  0.00605721 0.01036813 0.00679999 0.0080325\n",
            " 0.00918919 0.00971841 0.00543672 0.00577323 0.65812975 0.00746748\n",
            " 0.01160838 0.20690715 0.00631511 0.00733782 0.00653637 0.00749992\n",
            " 0.00513291 0.00476835]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0041802  0.0082276  0.00471638 0.00754921 0.00541868 0.00557875\n",
            " 0.0067974  0.00797723 0.00380142 0.0045585  0.6734445  0.00679717\n",
            " 0.01369786 0.22237115 0.00366949 0.00531166 0.00446643 0.00461176\n",
            " 0.00333933 0.00348528]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00460818 0.00999704 0.00495179 0.00837893 0.00568943 0.00618921\n",
            " 0.00671647 0.00841043 0.00373985 0.00516664 0.6801892  0.00872683\n",
            " 0.01765749 0.19963105 0.00376658 0.00586785 0.00494599 0.00589601\n",
            " 0.00561008 0.00386099]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00512188 0.00959354 0.00585148 0.00852731 0.00592926 0.00584736\n",
            " 0.00714846 0.00845092 0.00501917 0.0050859  0.66045386 0.00732625\n",
            " 0.01256995 0.21977463 0.00509464 0.00614193 0.00567725 0.00611738\n",
            " 0.00502427 0.00524461]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0051918  0.01160057 0.0060275  0.00931804 0.00592636 0.00682431\n",
            " 0.01073404 0.0085763  0.00546802 0.00538854 0.6648382  0.00680486\n",
            " 0.01224234 0.2076213  0.00520902 0.0064607  0.00582585 0.00645284\n",
            " 0.00457383 0.00491556]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00578234 0.0091166  0.00609776 0.00889347 0.00642147 0.00680802\n",
            " 0.00788997 0.00907866 0.00595828 0.00586483 0.6566247  0.0087747\n",
            " 0.01300623 0.21216004 0.00647373 0.00700429 0.00675116 0.00701191\n",
            " 0.00508361 0.00519825]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00589564 0.01163682 0.00634492 0.00919892 0.00656232 0.00692651\n",
            " 0.00733442 0.00923945 0.00509    0.00611774 0.670647   0.00748178\n",
            " 0.01740039 0.19430253 0.00463743 0.00696053 0.00620186 0.00684013\n",
            " 0.00539764 0.00578393]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00548053 0.0110883  0.00560324 0.00921941 0.00656571 0.00675809\n",
            " 0.00978714 0.00838978 0.00466983 0.00607788 0.66673213 0.0089929\n",
            " 0.01227366 0.2051504  0.00606419 0.00674277 0.00588606 0.00633799\n",
            " 0.00408528 0.00409476]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00551842 0.0096934  0.00573051 0.00874446 0.006585   0.00685282\n",
            " 0.00813242 0.00807    0.00474505 0.00641637 0.66401577 0.00916635\n",
            " 0.01338787 0.20787859 0.00537594 0.00724205 0.00618419 0.00679078\n",
            " 0.00501467 0.00445538]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00441572 0.00776379 0.00507991 0.00755935 0.00495437 0.0054713\n",
            " 0.00625784 0.00766419 0.00464349 0.00433986 0.67640465 0.00524124\n",
            " 0.01022259 0.22066309 0.0040496  0.00534118 0.0049564  0.00522263\n",
            " 0.00545815 0.0042906 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0061546  0.00985577 0.00675026 0.00960429 0.00664371 0.00682852\n",
            " 0.00763419 0.0108593  0.00627379 0.00673716 0.66030085 0.00896716\n",
            " 0.0126247  0.20191397 0.00623527 0.0070612  0.00661756 0.00684816\n",
            " 0.00641302 0.00567651]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.0047492  0.0090229  0.00502594 0.00826032 0.00565603 0.00613833\n",
            " 0.00739397 0.00790691 0.00455744 0.00476937 0.6783916  0.00538011\n",
            " 0.01032575 0.21278588 0.00425386 0.00592493 0.00561825 0.00572429\n",
            " 0.00436841 0.00374651]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00509904 0.0093208  0.00537298 0.008707   0.0058782  0.00623721\n",
            " 0.00733316 0.00839134 0.00482662 0.00494968 0.6710377  0.00517536\n",
            " 0.0101458  0.21694471 0.0039612  0.00608683 0.00617456 0.00602985\n",
            " 0.00422366 0.00410427]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00507653 0.01119586 0.00576613 0.00934973 0.00636774 0.00673641\n",
            " 0.00853519 0.00880755 0.00497177 0.00528508 0.669446   0.00590981\n",
            " 0.01072317 0.21049392 0.00467294 0.00608152 0.00589534 0.00611837\n",
            " 0.00450851 0.0040584 ]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00677496 0.01132479 0.00743048 0.01160499 0.00746244 0.00808797\n",
            " 0.00914065 0.01124135 0.00670791 0.00719094 0.6360725  0.01046954\n",
            " 0.01339773 0.20969135 0.00694414 0.00828908 0.00761512 0.00740759\n",
            " 0.00589664 0.00724979]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00599229 0.0109727  0.00626601 0.00964907 0.00683429 0.00754522\n",
            " 0.00883486 0.00932167 0.00623687 0.00749105 0.631245   0.01611561\n",
            " 0.01717983 0.21475434 0.00713989 0.00985102 0.0071263  0.00701294\n",
            " 0.00502942 0.00540158]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00466879 0.00889764 0.00514064 0.0084089  0.00566765 0.00631111\n",
            " 0.00742223 0.00848952 0.0040962  0.0046294  0.6755439  0.00849365\n",
            " 0.0145745  0.21004277 0.00435664 0.00514788 0.00482694 0.00534142\n",
            " 0.00437106 0.00356913]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00608048 0.00904184 0.00664391 0.00926712 0.00643145 0.00709034\n",
            " 0.00716356 0.01066225 0.00652609 0.00706914 0.662441   0.00683048\n",
            " 0.01282927 0.20729361 0.00487015 0.00679404 0.00614651 0.00637285\n",
            " 0.00528144 0.00516441]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00610497 0.01054208 0.00652038 0.00984247 0.00702776 0.00738767\n",
            " 0.00938672 0.00945408 0.00562625 0.00703309 0.6513353  0.01118136\n",
            " 0.01278946 0.20246223 0.00845321 0.00904537 0.00745478 0.00732917\n",
            " 0.00537802 0.00564566]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00605141 0.01025691 0.00609377 0.00988501 0.00706445 0.00736976\n",
            " 0.00864827 0.00891127 0.00601904 0.00599062 0.6532445  0.00755935\n",
            " 0.01167723 0.21306343 0.00578713 0.00733087 0.0071306  0.00738933\n",
            " 0.00581458 0.00471246]\n",
            "----- activations -----\n",
            "(1, 20, 32)\n",
            "attention = [0.00491296 0.00925988 0.00568792 0.0087566  0.00597101 0.00656063\n",
            " 0.00871516 0.00884644 0.00495735 0.00479047 0.6706294  0.00633887\n",
            " 0.01047996 0.21308137 0.00447784 0.00596109 0.0055061  0.0058513\n",
            " 0.0048823  0.00433342]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfZT0GZulCJ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "307c1453-3946-4b23-96e3-a77bd42d679d"
      },
      "source": [
        "    # plot part.\n",
        "    import matplotlib.pyplot as plt\n",
        "    import pandas as pd\n",
        "\n",
        "    pd.DataFrame(attention_vector_final, columns=['attention (%)']).plot(kind='bar',\n",
        "                                                                         title='Attention Mechanism as '\n",
        "                                                                               'a function of input'\n",
        "                                                                               ' dimensions.')\n",
        "    plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAELCAYAAADJF31HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de5gU1dXv8e+SuxEFYTQiKLxewRvC\niHiJYo5R8IKcxCQq+qpROZ7I0aMmiho1QWNQHvWNib6KRokCXqJHQhQDMYpGIzqDjhpEZSQoA15G\nBG9ABF3nj71nLJvu6ZqZhmkrv8/z9DPdVbt2raratapqV3WPuTsiIvL1t0lbByAiIqWhhC4ikhFK\n6CIiGaGELiKSEUroIiIZoYQuIpIRSugJZjbazGa3dRzNZWZ9zczNrP0GqPtruU7SMLNdzKzGzD42\ns7M34ny3M7NPzKzdxppnnO/WZvZkXN5r84y/2cwu3ZgxpWVmPzezKfF9m6y/ppTLumvzhG5mc8xs\nhZl1yhk+2cyuzBm22MwOLdF810uC7j7V3Q8rRf058xoW5/VgzvC94vA5pZ5nqWyodVImLgAed/eu\n7n7DhppJbrt197fcfTN3/3xDzbOAMcD7wObufn7uSHc/092v2NBBmNkpZvZUS6dvw/VX0MZad8W0\naUI3s77AtwAHRrZlLBtBPbCfmfVIDDsZeL2N4hHYHpjf1kFsRNsDr7i+TZhd7t5mL+Ay4GngOuCh\nxPAxwFrgM+AT4E/AXcAXwOo47IJYdijwd2Al8CIwLFHPHOCKOI+PgdlAzzjuLcKB5JP42g84BXgq\nMf3+QBXwYfy7f5q68yznMKAOuBk4Kw5rByyN62BOouyuwF+AD4DXgB8kxnUBrgXejDE9FYf1jcty\nclyu94FLEtMNAZ6J6+ht4LdAx8R4B84EFsYyNwIWxzWuE8CA64H3gI+Al4Hd47jJwE3AI3F9Pg18\nE/gvYAXwKrB3E23h18CSWO884Fs58VfHce8C1xWoozvwEOHguSK+712g7GPA58CaGO/OcZueniiT\n2x4Krqc4/gxgQWwPrwCDyNNuE9urfZyuFzAjbvNa4IxEnT8H7gPujPXOByqbWI9522zcPsl96tA8\n004Grsxps+fH7f02cGpO2ZsJbfVj4Alg+zjuK8uX2F9OB/rHdf55jGNlgeXoF+v8OM7jt8CUfPXH\nuq8k5IGGfNEDmBrbTBXQN+U+Njlu14fjvJ8FdkjZ/q/MaQu1cR4zgF4p97cd43J/SNiP721WTt2Y\nCTzPRqsFfgwMjo1t63yNKzFscbIhAtsCy4EjCFcb34mfKxIb+g3Cztolfp7QRKM7hS+T15aEpHAS\n0B44Pn7uUazuPMs5jLBz7A88G4cdAcwiNPI5cdg3CEnt1DjPveNGHRDH3xjnsy3hgLA/0CmxLLfG\nWPYC/gX0j9MNJhz42seyC4D/m9PAHgK6AdsREuLwPOvkcEKy7UZo3P2BbRLb6/04r86EhPlP4D9j\nrFcSujcKtYUTCTthe0ISeQfoHMc9A5wU328GDC1QRw/ge8CmQFfgD8D0JuY5h68m8NzPjcueYj19\nn3CA3ieumx35MsEt5qvttmF7NSSkJwkHw87AwFjvt+O4nxMS4BFxPf4KmFtgeYq12cnk7FM50zeO\nJ7TZdcB4oEOc/yqge6Lsx8BBhDb460Q7+cry5a7b3PVaIJZnCCd6neI8PqbphF4L7ABsQTiYvg4c\nGtfDncAdKfexyYQcMiSOnwrck7L9N6y7b8c6B8X4fwM8mbId3Q1cQshnnYEDm5NT26zLxcwOJFwC\n3ufu8wjJ8YRmVnMiMNPdZ7r7F+7+F8KZ3BGJMne4++vuvppwpjMwZd1HAgvd/S53X+fudxPOMo9u\nad3u/ndgSzPbhZDo7swpchSw2N3viPN8AXgA+L6ZbQL8CDjH3Ze6++fu/nd3/1di+l+4+2p3f5Fw\ntbJXnO88d58b61wM3AIcnDPvCe6+0t3fAh4vsCxrCYlyV8IZxQJ3fzsx/sE4rzXAg8Aad7/TQ1/n\nvYSdp9C6meLuy2OM1xJ2hF0S893RzHq6+yfuPrdAHcvd/QF3X+XuHwO/zLOcrVVoPZ0OXOPuVR7U\nuvubxSozsz7AAcCF7r7G3WuA2wjto8FTsY1/Tjjj36tAdWnabHOsBca7+1p3n0k4+90lMf5hd38y\ntsFLCF2KfVo4r0Zmth3hwHipu//L3Z8knHU35Q53f8PdPyRcJb7h7o+6+zrCgb2h7RXcxxJ1Peju\nz8Vpp/LlNi7W/huMBm539+fjurmIsG76JsoUakdrCXmxV2wPzbrX0JZ96CcDs939/fh5WhzWHNsT\nkt3KhhdwILBNosw7iferCGd4afQidG0kvUk4O25N3XcBY4FDCEkvaXtg35zlGU3ouuhJOGK/0UTd\neeMxs53N7CEze8fMPgKuivUVnTbJ3R8jXPreCLxnZpPMbPNEkXcT71fn+Vxw/ZjZT8xsgZl9GJd7\ni0SMpxGuhF41syozO6pAHZua2S1m9mZczieBbiV+GqLQeupD09umkF7AB/EA1KBYO+tc4ImmNG22\nOZbHpJacd3IbLml44+6fELoXerVwXkm9gBXu/mliWLGDY9q219Q+1iDvNk7R/pPxN8Yb181y0uWO\nCwhn/8+Z2Xwz+1HhRV5fmyR0M+sC/AA4OCaZd4Bzgb3MrOHsI9+Nm9xhS4C73L1b4vUNd5+QIoxi\nN4aWETZ+0naEy+rWuIvQzTTT3VfljFsCPJGzPJu5+/8mXMKtIVxWNtd/E87UdnL3zYGLCY2m2dz9\nBncfDAwgJNmftqSeJDP7FqEh/4BwSd+N0IdocZ4L3f14YCvgauB+M/tGnqrOJ5xB7huX86CGWaQM\n5VNCd02DbxYqmMcSCm+bptraMsJVW9fEsJa2sw3VZgtpPBs3s80IXT7LCOsRCq/LYvve20D3nG28\nXSviTGpqHysqZfv/ynaIy9GDFNvB3d9x9zPcvRfwv4CbzGzHNLFB252hjyLcFBlAuNQYSOiP+htf\nXmq+C/xHznS5w6YAR5vZ4WbWzsw6x0cEe6eIoZ5wsyp3Hg1mAjub2Qlm1t7MfhjjfShF3QW5+z8J\n3QCX5Bn9UJznSWbWIb72MbP+7v4FcDtwnZn1isu7X+7jngV0JdzE+cTMdgVSNd5cMZZ9zawDYadd\nQ1iHrdWV0F9bD7Q3s8uAxjMfMzvRzCriOlgZB+ebb1fC2dhKM9sSuLyZcdQA341n+jsSrgzSug34\niZkNtmBHM2vYqfO1ZQDcfQnhZt6vYvvdM853SjNjhw3UZptwhJkdaGYdCQ8IzHX3Je5eT0heJ8Z2\n+iO+erB7F+gdp1tP7KqqBn5hZh1j92xLu41yFdzHik3YjPZ/N3CqmQ2M++dVhHtni1PM4/uJ/LWC\ncPBLvY+1VUI/mdDn9VY8Ir3j7u8QLmdGx8vJ3wED4mXR9Djdr4CfxWE/iTvDMYQzznrC0fenpFiu\neHb8S+DpWN/QnPHLCf1t5xMuly4Ajkp0EbWYuz/l7svyDP8YOAw4jnCUf4dwRtqQtH9CuLNeRbi8\nvZp02/AnhPsTHxNunN7bwtA3j9OvIFxSLgcmtrCupFnAnwk3st4k7ChLEuOHA/PN7BPCzbfjPNy3\nyPVfhJvC7wNzY53NcT3hKZB3gd8T+k9Tcfc/ENrTNMJ6nk44Y4Wcdptn8uMJN/qWEbrhLnf3R5sZ\n+wZtswVMIxw0PyDcDD8xMe4Mwr64HNiNcNBq8BjhaZ13zKxQbCcA+8a6L2f9+00tkmIfa0qq9h+3\n3aWEvvm3CQez41KGuA/wbGzrMwj3zBYBxC6Y0U1N3PCojIhIamY2Gahz95+1dSzypTb/pqiIiJSG\nErqISEaoy0VEJCN0hi4ikhFK6CIiGVHy389Oq2fPnt63b9+2mr2IyNfSvHnz3nf3inzj2iyh9+3b\nl+rq6raavYjI15KZFfwZBHW5iIhkhBK6iEhGKKGLiGREm/Whi0j5WLt2LXV1daxZs6atQ5Goc+fO\n9O7dmw4dOqSeRgldRKirq6Nr16707dsXsxb9srKUkLuzfPly6urq6NevX+rp1OUiIqxZs4YePXoo\nmZcJM6NHjx7NvmJSQhcRACXzMtOS7aGELiKSEepDF0mh77iHmxy/eMKRGymSjaPY8jZXa9bPVVdd\nxcUXXwzAypUrmTZtGj/+8Y9bXN/kyZM57LDD6NUr/PvT008/nfPOO48BAwa0uM4G06dP56WXXuKy\nyy7jN7/5Dbfccgvbbbcd06dPp2PHjjz11FM88MADXH/99QDU19dz0kkn8ec/N/d/seSnM3QRKWtX\nXXVV4/uVK1dy0003taq+yZMns2zZl/8w7LbbbitJMge45pprGg82U6dO5aWXXmL//fdn1qxZuDtX\nXHEFl156aWP5iooKttlmG55++umSzF8JXUTKwqhRoxg8eDC77bYbkyZNAmDcuHGsXr2agQMHMnr0\naMaNG8cbb7zBwIED+elPw/9nnjhxIvvssw977rknl18e/o3s4sWL6d+/P2eccQa77bYbhx12GKtX\nr+b++++nurqa0aNHM3DgQFavXs2wYcMaf4bk7rvvZo899mD33XfnwgsvbIxts80245JLLmGvvfZi\n6NChvPvuu+vF//rrr9OpUyd69uwJhCdV1q5dy6pVq+jQoQNTpkxhxIgRbLnlll+ZbtSoUUydmvq/\nHTZJCV1EysLtt9/OvHnzqK6u5oYbbmD58uVMmDCBLl26UFNTw9SpU5kwYQI77LADNTU1TJw4kdmz\nZ7Nw4UKee+45ampqmDdvHk8++SQACxcu5KyzzmL+/Pl069aNBx54gGOPPZbKykqmTp1KTU0NXbp0\naZz/smXLuPDCC3nssceoqamhqqqK6dPDvzP+9NNPGTp0KC+++CIHHXQQt95663rxP/300wwaNKjx\n89ixYxk6dChvvfUWBxxwAHfccQdnnXXWetNVVlbyt7/9rSTrUAldRMrCDTfc0HgGvGTJEhYuXFh0\nmtmzZzN79mz23ntvBg0axKuvvto4Xb9+/Rg4cCAAgwcPZvHixU3WVVVVxbBhw6ioqKB9+/aMHj26\n8eDQsWNHjjrqqCbrevvtt6mo+PJHEE866SReeOEFpkyZwvXXX8/ZZ5/NI488wrHHHsu5557LF198\nAcBWW231lS6g1lBCF5E2N2fOHB599FGeeeYZXnzxRfbee+9Uz2C7OxdddBE1NTXU1NRQW1vLaaed\nBkCnTp0ay7Vr145169a1OL4OHTo0PkZYqK4uXbrkjXnZsmU899xzjBo1imuvvZZ7772Xbt268de/\n/hUI3wFIXim0hhK6iLS5Dz/8kO7du7Ppppvy6quvMnfu3MZxHTp0YO3atQB07dqVjz/+uHHc4Ycf\nzu23384nn3wCwNKlS3nvvfeanFduHQ2GDBnCE088wfvvv8/nn3/O3XffzcEHH5x6Gfr3709tbe16\nwy+99FLGjx8PwOrVqzEzNtlkE1atWgWEvvfdd9899XyaoscWRWQ9G/sxzOHDh3PzzTfTv39/dtll\nF4YOHdo4bsyYMey5554MGjSIqVOncsABB7D77rszYsQIJk6cyIIFC9hvv/2AcPNyypQptGvXruC8\nTjnlFM4880y6dOnCM8880zh8m222YcKECRxyyCG4O0ceeSTHHHNM6mU46KCDOP/883H3xrP5F154\nAaCxb/2EE05gjz32oE+fPlxwwQUAPP744xx5ZGnWd5v9k+jKykrXP7iQr4usP4e+YMEC+vfv39Zh\nfO2dc845HH300Rx66KGppznooIP44x//SPfu3dcbl2+7mNk8d6/MV5e6XERESuTiiy9u7EpJo76+\nnvPOOy9vMm8JJXQRkRLZeuutGTlyZOryFRUVjBo1qmTzV0IXESA8MSLloyXbQwldROjcuTPLly9X\nUi8TDb+H3rlz52ZNp6dcRITevXtTV1dHfX19W4ciUcN/LGoOJXQRoUOHDs36zzhSnlJ1uZjZcDN7\nzcxqzWxcgTI/MLNXzGy+mU0rbZgiIlJM0TN0M2sH3Ah8B6gDqsxshru/kiizE3ARcIC7rzCzrTZU\nwCIikl+aM/QhQK27L3L3z4B7gNyvT50B3OjuKwDcvenv3oqISMmlSejbAksSn+visKSdgZ3N7Gkz\nm2tmw/NVZGZjzKzazKp180VEpLRK9dhie2AnYBhwPHCrmXXLLeTuk9y90t0rkz8zKSIirZcmoS8F\n+iQ+947DkuqAGe6+1t3/CbxOSPAiIrKRpEnoVcBOZtbPzDoCxwEzcspMJ5ydY2Y9CV0wi0oYp4iI\nFFE0obv7OmAsMAtYANzn7vPNbLyZNfxowSxguZm9AjwO/NTdl2+ooEVEZH2pvljk7jOBmTnDLku8\nd+C8+BIRkTag33IREckIJXQRkYxQQhcRyQgldBGRjFBCFxHJCCV0EZGMUEIXEckIJXQRkYxQQhcR\nyQgldBGRjFBCFxHJCCV0EZGMUEIXEckIJXQRkYxQQhcRyQgldBGRjFBCFxHJCCV0EZGMUEIXEckI\nJXQRkYxQQhcRyQgldBGRjEiV0M1suJm9Zma1ZjYuz/hTzKzezGri6/TShyoiIk1pX6yAmbUDbgS+\nA9QBVWY2w91fySl6r7uP3QAxiohICmnO0IcAte6+yN0/A+4BjtmwYYmISHOlSejbAksSn+visFzf\nM7OXzOx+M+tTkuhERCS1Ut0U/RPQ1933BP4C/D5fITMbY2bVZlZdX19folmLiAikS+hLgeQZd+84\nrJG7L3f3f8WPtwGD81Xk7pPcvdLdKysqKloSr4iIFJAmoVcBO5lZPzPrCBwHzEgWMLNtEh9HAgtK\nF6KIiKRR9CkXd19nZmOBWUA74HZ3n29m44Fqd58BnG1mI4F1wAfAKRswZhERyaNoQgdw95nAzJxh\nlyXeXwRcVNrQRESkOfRNURGRjFBCFxHJCCV0EZGMUEIXEckIJXQRkYxQQhcRyQgldBGRjFBCFxHJ\nCCV0EZGMUEIXEckIJXQRkYxQQhcRyQgldBGRjFBCFxHJCCV0EZGMUEIXEckIJXQRkYxQQhcRyQgl\ndBGRjFBCFxHJCCV0EZGMUEIXEckIJXQRkYxIldDNbLiZvWZmtWY2roly3zMzN7PK0oUoIiJpFE3o\nZtYOuBEYAQwAjjezAXnKdQXOAZ4tdZAiIlJcmjP0IUCtuy9y98+Ae4Bj8pS7ArgaWFPC+EREJKU0\nCX1bYEnic10c1sjMBgF93P3hpioyszFmVm1m1fX19c0OVkRECmv1TVEz2wS4Dji/WFl3n+Tule5e\nWVFR0dpZi4hIQpqEvhTok/jcOw5r0BXYHZhjZouBocAM3RgVEdm40iT0KmAnM+tnZh2B44AZDSPd\n/UN37+nufd29LzAXGOnu1RskYhERyatoQnf3dcBYYBawALjP3eeb2XgzG7mhAxQRkXTapynk7jOB\nmTnDLitQdljrwxIRkebSN0VFRDJCCV1EJCOU0EVEMkIJXUQkI5TQRUQyQgldRCQjlNBFRDJCCV1E\nJCOU0EVEMkIJXUQkI5TQRUQyQgldRCQjlNBFRDJCCV1EJCOU0EVEMkIJXUQkI5TQRUQyQgldRCQj\nlNBFRDJCCV1EJCOU0EVEMkIJXUQkI1IldDMbbmavmVmtmY3LM/5MM3vZzGrM7CkzG1D6UEVEpClF\nE7qZtQNuBEYAA4Dj8yTsae6+h7sPBK4Brit5pCIi0qQ0Z+hDgFp3X+TunwH3AMckC7j7R4mP3wC8\ndCGKiEga7VOU2RZYkvhcB+ybW8jMzgLOAzoC3y5JdCIiklrJboq6+43uvgNwIfCzfGXMbIyZVZtZ\ndX19falmLSIipEvoS4E+ic+947BC7gFG5Rvh7pPcvdLdKysqKtJHKSIiRaVJ6FXATmbWz8w6AscB\nM5IFzGynxMcjgYWlC1FERNIo2ofu7uvMbCwwC2gH3O7u881sPFDt7jOAsWZ2KLAWWAGcvCGDFhGR\n9aW5KYq7zwRm5gy7LPH+nBLHJSIizaRvioqIZIQSuohIRiihi4hkhBK6iEhGKKGLiGSEErqISEYo\noYuIZIQSuohIRiihi4hkhBK6iEhGKKGLiGSEErqISEYooYuIZIQSuohIRiihi4hkhBK6iEhGKKGL\niGSEErqISEYooYuIZIQSuohIRiihi4hkhBK6iEhGKKGLiGRE+zSFzGw48GugHXCbu0/IGX8ecDqw\nDqgHfuTub5Y4VhFppb7jHm5y/OIJR26kSGRDKHqGbmbtgBuBEcAA4HgzG5BT7AWg0t33BO4Hril1\noCIi0rQ0XS5DgFp3X+TunwH3AMckC7j74+6+Kn6cC/QubZgiIlJMmoS+LbAk8bkuDivkNOCRfCPM\nbIyZVZtZdX19ffooRUSkqJLeFDWzE4FKYGK+8e4+yd0r3b2yoqKilLMWEfm3l+am6FKgT+Jz7zjs\nK8zsUOAS4GB3/1dpwhMRkbTSnKFXATuZWT8z6wgcB8xIFjCzvYFbgJHu/l7pwxQRkWKKJnR3XweM\nBWYBC4D73H2+mY03s5Gx2ERgM+APZlZjZjMKVCciIhtIqufQ3X0mMDNn2GWJ94eWOC4REWkmfVNU\nRCQjlNBFRDJCCV1EJCOU0EVEMkIJXUQkI5TQRUQyQgldRCQjlNBFRDJCCV1EJCOU0EVEMkIJXUQk\nI5TQRUQyQgldRCQjlNBFRDJCCV1EJCOU0EVEMkIJXUQkI5TQRUQyQgldRCQjlNBFRDJCCV1EJCOU\n0EVEMiJVQjez4Wb2mpnVmtm4POMPMrPnzWydmR1b+jBFRKSYogndzNoBNwIjgAHA8WY2IKfYW8Ap\nwLRSBygiIum0T1FmCFDr7osAzOwe4BjglYYC7r44jvtiA8QoIiIppOly2RZYkvhcF4eJiEgZ2ag3\nRc1sjJlVm1l1fX39xpy1iEjmpUnoS4E+ic+947Bmc/dJ7l7p7pUVFRUtqUJERApIk9CrgJ3MrJ+Z\ndQSOA2Zs2LBERKS5iiZ0d18HjAVmAQuA+9x9vpmNN7ORAGa2j5nVAd8HbjGz+RsyaBERWV+ap1xw\n95nAzJxhlyXeVxG6YkREpI3om6IiIhmhhC4ikhFK6CIiGaGELiKSEUroIiIZoYQuIpIRSugiIhmh\nhC4ikhFK6CIiGaGELiKSEUroIiIZoYQuIpIRSugiIhmhhC4ikhFK6CIiGaGELiKSEUroIiIZoYQu\nIpIRSugiIhmhhC4ikhFK6CIiGaGELiKSEe3bOgARSafvuIeLllk84ciNEImUq1QJ3cyGA78G2gG3\nufuEnPGdgDuBwcBy4Ifuvri0oRZXrMFvjMaepZ2uteszS+uitbQuZGMomtDNrB1wI/AdoA6oMrMZ\n7v5KothpwAp339HMjgOuBn7YnEDKpcFn5aBQDstRCloX5accDvSl2KblUEep816aPvQhQK27L3L3\nz4B7gGNyyhwD/D6+vx/4H2ZmqaMQEZFWM3dvuoDZscBwdz89fj4J2NfdxybK/COWqYuf34hl3s+p\nawwwJn7cBXitiVn3BN5vYnwaWamjHGIolzrKIYZyqaMcYiiXOsohho1Vx/buXpFvxEa9Keruk4BJ\nacqaWbW7V7ZmflmpoxxiKJc6yiGGcqmjHGIolzrKIYZyqCNNl8tSoE/ic+84LG8ZM2sPbEG4OSoi\nIhtJmoReBexkZv3MrCNwHDAjp8wM4OT4/ljgMS/WlyMiIiVVtMvF3deZ2VhgFuGxxdvdfb6ZjQeq\n3X0G8DvgLjOrBT4gJP3WStU1829SRznEUC51lEMM5VJHOcRQLnWUQwxtXkfRm6IiIvL1oK/+i4hk\nhBK6iEhGKKGLiGRE2fw4l5ntSvjG6bZx0FJghrsvaIM4tgWedfdPEsOHu/ufU0w/BHB3rzKzAcBw\n4FV3n9mKmO509/9sxfQHEr7x+w93n51ymn2BBe7+kZl1AcYBg4BXgKvc/cMi058NPOjuS1oRd8NT\nVcvc/VEzOwHYH1gATHL3tSnr+Q/gu4RHaz8HXgemuftHLY1NpByVxU1RM7sQOJ7wswJ1cXBvws58\nT+6PgbWg/lPd/Y4U5c4GziIkjIHAOe7+xzjueXcfVGT6y4ERhAPlX4B9gccJv4Mzy91/mSKG3EdC\nDTgEeAzA3UemqOM5dx8S358Rl+lB4DDgT2nWp5nNB/aKTzlNAlYRf9YhDv9ukek/BD4F3gDuBv7g\n7vXF5ptTx1TCutwUWAlsBvy/GIO5+8lNTN5Qx9nAUcCTwBHAC7Gu/wn82N3nNCcmWZ+ZbeXu77Vx\nDD3cXd99cfc2fxHOmDrkGd4RWFiC+t9KWe5lYLP4vi9QTUjqAC+knL4dIQF9BGweh3cBXkoZw/PA\nFGAYcHD8+3Z8f3DKOl5IvK8CKuL7bwAvp6xjQTKmnHE1aWIgdOkdRnistR74M+H7Cl1TxvBS/Nse\neBdoFz9bM9bny4npNgXmxPfbpdmmsewWwATgVcJjucsJB/0JQLcStM9HUpTZHPgVcBdwQs64m1LO\n55vAfxN+bK8H8PO4fu4DtklZx5Y5rx7AYqA7sGXKOobnrNvfAS8B04CtU0w/AegZ31cCi4Ba4M1m\n7CPPAz8DdmjFdqsknLBNIVz9/QX4MO5ze6esYzNgPDA/TlsPzAVOaUlM5dKH/gXQK8/wbeK4oszs\npQKvl4GtU8axicduFg8//zsMGGFm1xGSSDHr3P1zd18FvOHxkt7dV6ddDkIjmQdcAnzo4Qxytbs/\n4e5PpF0OM+tuZj0IZ7L1MY5PgXUp6/iHmZ0a379oZpUAZrYzkKarw939C3ef7e6nEbbvTYQuqEXN\nWI6OQFdCMt4iDu8EdEhZB3zZtdiJsAPh7m81o477gBXAMHff0t17EK6aVsRxRZnZoAKvwYSrwWLu\nILTBB4DjzOyB+LPVAENTLsdkQpfZEkIiWk24avkbcHPKOt4ntM+GVzWhi/L5+D6NqxLvryWcsBxN\nSIS3pJj+SP/yd6ImEn6ue0fClfC1KWPoDnQDHjez58zsXDPLl4OachNwDfAw8HfgFnffgtA9eVPK\nOqYS9ofDgV8ANwAnAYeY2VVNTZhXS49OpXwRdvJa4BHCQ/WTCGdztSSO5kXqeJewY2yf8+pL6INN\nU8djwMCcYe0Jv/X+eYrpnxJD+nUAAALrSURBVAU2je83SQzfgpyz3BR19Qb+APyWlFcYiWkXx0by\nz/h3m8TZQNGz60TMkwldJs8Skvgi4AlCl0ux6Que/TasoxR1nBvn+SZwNvBX4FbCWeXlKes4h3D2\ndyvhDPvUOLwCeDJlHa+1ZFxOuc9j+3o8z2t1iulrcj5fAjxNOENO1bb46pXbW03V30Qd58d9c4/E\nsH82s30+X2i+aeIgXB21j+/n5oxLewWajOFbhAT8TtweY0qwPtNe/b2Y87kq/t2EcO8t9Xp19/JI\n6IkFGAp8L76GEi+VU07/O+DAAuOmpayjN/DNAuMOSDF9pwLDeyZ3gGaulyMJNyFLsY43Bfo1c5rN\ngb0I/7yk6OVwYrqdSxRzL6BXfN+N8NMSQ5pZx25xul1bGMNs4ILk8hOu+i4EHk1Zxz+AnQqMW5Ji\n+gUkThLisFMIl+pvpozhxcT7K3PGpUqEsWzDycZ1hKunRc1cn3XAefHgsIh4Ly+OK9qVBvyfuE2+\nTeg2+jWhS/IXwF0pY1jvIEjoLh0O3JGyjmcIXYrfJ5x0jIrDDyZ8iz5NHX9vyFvASMK9toZxqU4W\nvlJfcyfQS69/txfh8vxqvuxD/yAm2KuB7inrOBbYpcC4USmmvwY4NM/w4aS8z0Toq90sz/Adgftb\nsF5GEvp732nmdJfnvBru8XwTuDNlHcOAewn3al4GZhJ+mrt9yunvKUG72IvwkyiPALvGA8vKeJDd\nP2UdewLPEbrvniKeCBGuIM9ubkxl8ZSLyNdV2ieoNmQdbRlDfKR1B3f/x7/7uiiHOpTQRVrBzN5y\n9+3aso5yiKFc6iiHGNqyjrL5YpFIuTKzlwqNIuUTVK2toxxiKJc6yiGGcqojSQldpLitCY+VrcgZ\nboSbWhujjnKIoVzqKIcYyqmORkroIsU9RLiZWJM7wszmbKQ6yiGGcqmjHGIopzq+nEZ96CIi2VAu\n3xQVEZFWUkIXEckIJXQRkYxQQhcRyQgldBGRjPj/5DzJXOJpW5gAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}