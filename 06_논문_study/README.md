# Paper Stduy

### 1. Transformer & Attetion
- http://jalammar.github.io/illustrated-transformer/  
#### 1-1. Attention  
- https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/  
#### 1-2. Attention in Keras  
- https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39  
#### 1-3. Attention 레이어를 접목한 핵심어구(단어) 가중치 추출  
- 논문리뷰1: https://lovit.github.io/machine%20learning/2019/03/17/attention_in_nlp/    
- 논문리뷰2: https://jsideas.net/name_classifier/  
- 논문원문(A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING): https://openreview.net/pdf?id=BJC_jUqxe  
- 구현체: https://github.com/ExplorerFreda/Structured-Self-Attentive-Sentence-Embedding  

<br>

### 2. [A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING](https://github.com/jukyellow/artificial-intelligence-study/blob/master/06_%EB%85%BC%EB%AC%B8_study/2_A%20STRUCTURED%20SELF-ATTENTIVE%20SENTENCE%20EMBEDDING.md)  


<br>

### 3. [대화체 유연한 띄어쓰기 모델](https://github.com/jukyellow/artificial-intelligence-study/blob/master/06_%EB%85%BC%EB%AC%B8_study/2_%EB%8C%80%ED%99%94%EC%B2%B4%20%EC%9C%A0%EC%97%B0%ED%95%9C%20%EB%9D%84%EC%96%B4%EC%93%B0%EA%B8%B0%20%EB%AA%A8%EB%8D%B8.md)
```python
pip install chatspace
from chatspace import ChatSpace
spacer = ChatSpace()
spacer.space("안녕만나서반가워내이름은뽀로로라고해")
# '안녕 만나서 반가워 내 이름은 뽀로로라고 해'
```

<br>

### 4. NLTK(Natural Language Toolkit) 자연어 처리 패키지  
- https://datascienceschool.net/view-notebook/118731eec74b4ad3bdd2f89bab077e1b/  

