# Paper Stduy

### 1. Transformer & Attetion
- http://jalammar.github.io/illustrated-transformer/  
#### 1-1. Attention  
- https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/  
#### 1-2. Attention in Keras  
- https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39  

<br>

### 2. [대화체 유연한 띄어쓰기 모델](https://github.com/jukyellow/artificial-intelligence-study/blob/master/06_%EB%85%BC%EB%AC%B8_study/2_%EB%8C%80%ED%99%94%EC%B2%B4%20%EC%9C%A0%EC%97%B0%ED%95%9C%20%EB%9D%84%EC%96%B4%EC%93%B0%EA%B8%B0%20%EB%AA%A8%EB%8D%B8.md)
```python
pip install chatspace
from chatspace import ChatSpace
spacer = ChatSpace()
spacer.space("안녕만나서반가워내이름은뽀로로라고해")
# '안녕 만나서 반가워 내 이름은 뽀로로라고 해'
```
