{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras-attention-visualize-run-detail.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNRdqf86o3Lu",
        "colab_type": "code",
        "outputId": "eb543a61-4bd9-49ab-9f5a-eeb801dc2074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "#https://github.com/likejazz/jupyter-notebooks/tree/master/deep-learning/keras-attention\n",
        "#>> https://github.com/datalogue/keras-attention.git\n",
        "\n",
        "!git clone https://github.com/jukyellow/keras-attention.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras-attention'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 154 (delta 4), reused 0 (delta 0), pack-reused 143\u001b[K\n",
            "Receiving objects: 100% (154/154), 15.63 MiB | 16.02 MiB/s, done.\n",
            "Resolving deltas: 100% (60/60), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gqGpVZdJLyg",
        "colab_type": "code",
        "outputId": "1dc2bbde-4079-4fac-96ef-fe1a27651b90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "!pip install Faker"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Faker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/ed/2fd5337ed405c4258dde1254e60f4e8ef9f1787576c0a2cd0d750b1716a6/Faker-2.0.3-py2.py3-none-any.whl (892kB)\n",
            "\r\u001b[K     |▍                               | 10kB 19.8MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 4.1MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 5.9MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40kB 7.4MB/s eta 0:00:01\r\u001b[K     |█▉                              | 51kB 4.9MB/s eta 0:00:01\r\u001b[K     |██▏                             | 61kB 5.7MB/s eta 0:00:01\r\u001b[K     |██▋                             | 71kB 6.5MB/s eta 0:00:01\r\u001b[K     |███                             | 81kB 7.2MB/s eta 0:00:01\r\u001b[K     |███▎                            | 92kB 8.0MB/s eta 0:00:01\r\u001b[K     |███▊                            | 102kB 6.4MB/s eta 0:00:01\r\u001b[K     |████                            | 112kB 6.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 122kB 6.4MB/s eta 0:00:01\r\u001b[K     |████▊                           | 133kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 143kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 153kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 163kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 174kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 184kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 194kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 204kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 215kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 225kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 235kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 245kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 256kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 266kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 276kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 286kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 296kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 307kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 317kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 327kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 337kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 348kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 358kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 368kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 378kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 389kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 399kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 409kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 419kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 430kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 440kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 450kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 460kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 471kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 481kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 491kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 501kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 512kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 522kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 532kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 542kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 552kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 563kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 573kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 583kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 593kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 604kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 614kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 624kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 634kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 645kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 655kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 665kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 675kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 686kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 696kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 706kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 716kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 727kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 737kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 747kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 757kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 768kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 778kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 788kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 798kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 808kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 819kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 829kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 839kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 849kB 6.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 860kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 870kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 880kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 890kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 901kB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.6/dist-packages (from Faker) (2.6.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from Faker) (1.12.0)\n",
            "Requirement already satisfied: text-unidecode==1.3 in /usr/local/lib/python3.6/dist-packages (from Faker) (1.3)\n",
            "Installing collected packages: Faker\n",
            "Successfully installed Faker-2.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otcMjkDfIFf8",
        "colab_type": "code",
        "outputId": "e4c49b14-f7a8-4c68-837f-2bd8c455ecb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import random\n",
        "import json\n",
        "import os\n",
        "\n",
        "#DATA_FOLDER = os.path.realpath(os.path.join(os.path.realpath(__file__), '..'))\n",
        "DATA_FOLDER = os.path.realpath(os.path.join(os.path.abspath(''), '..'))\n",
        "print('DATA_FOLDER:', DATA_FOLDER)\n",
        "DATA_FOLDER = '/content/keras-attention/data'\n",
        "print('DATA_FOLDER:', DATA_FOLDER)\n",
        "\n",
        "\n",
        "from faker import Faker\n",
        "import babel\n",
        "from babel.dates import format_date\n",
        "\n",
        "fake = Faker()\n",
        "fake.seed(230517)\n",
        "random.seed(230517)\n",
        "\n",
        "FORMATS = ['short',\n",
        "           'medium',\n",
        "           'long',\n",
        "           'full',\n",
        "           'd MMM YYY',\n",
        "           'd MMMM YYY',\n",
        "           'dd MMM YYY',\n",
        "           'd MMM, YYY',\n",
        "           'd MMMM, YYY',\n",
        "           'dd, MMM YYY',\n",
        "           'd MM YY',\n",
        "           'd MMMM YYY',\n",
        "           'MMMM d YYY',\n",
        "           'MMMM d, YYY',\n",
        "           'dd.MM.YY',\n",
        "           ]\n",
        "\n",
        "# change this if you want it to work with only a single language\n",
        "# LOCALES = ['en_US']\n",
        "LOCALES = babel.localedata.locale_identifiers()\n",
        "\n",
        "\n",
        "def create_date():\n",
        "    \"\"\"\n",
        "        Creates some fake dates \n",
        "        :returns: tuple containing \n",
        "                  1. human formatted string\n",
        "                  2. machine formatted string\n",
        "                  3. date object.\n",
        "    \"\"\"\n",
        "    dt = fake.date_object()\n",
        "\n",
        "    # wrapping this in a try catch because\n",
        "    # the locale 'vo' and format 'full' will fail\n",
        "    try:\n",
        "        human = format_date(dt,\n",
        "                            format=random.choice(FORMATS),\n",
        "                            locale=random.choice(LOCALES))\n",
        "\n",
        "        case_change = random.randint(0,3) # 1/2 chance of case change\n",
        "        if case_change == 1:\n",
        "            human = human.upper()\n",
        "        elif case_change == 2:\n",
        "            human = human.lower()\n",
        "\n",
        "        machine = dt.isoformat()\n",
        "    except AttributeError as e:\n",
        "        # print(e)\n",
        "        return None, None, None\n",
        "\n",
        "    return human, machine, dt\n",
        "\n",
        "\n",
        "def create_dataset(dataset_name, n_examples, vocabulary=False):\n",
        "    \"\"\"\n",
        "        Creates a csv dataset with n_examples and optional vocabulary\n",
        "        :param dataset_name: name of the file to save as\n",
        "        :n_examples: the number of examples to generate\n",
        "        :vocabulary: if true, will also save the vocabulary\n",
        "    \"\"\"\n",
        "    human_vocab = set()\n",
        "    machine_vocab = set()\n",
        "\n",
        "    with open(dataset_name, 'w') as f:\n",
        "        for i in range(n_examples):\n",
        "            h, m, _ = create_date()\n",
        "            if h is not None:\n",
        "                f.write('\"'+h + '\",\"' + m + '\"\\n')\n",
        "                human_vocab.update(tuple(h))\n",
        "                machine_vocab.update(tuple(m))\n",
        "            if(i==0):\n",
        "                print('create_dataset h:',h,',m:',m,',_:',_)\n",
        "\n",
        "    if vocabulary:\n",
        "        int2human = dict(enumerate(human_vocab))\n",
        "        int2human.update({len(int2human): '<unk>',\n",
        "                          len(int2human)+1: '<eot>'})\n",
        "        int2machine = dict(enumerate(machine_vocab))\n",
        "        int2machine.update({len(int2machine):'<unk>',\n",
        "                            len(int2machine)+1:'<eot>'})\n",
        "\n",
        "        human2int = {v: k for k, v in int2human.items()}\n",
        "        machine2int = {v: k for k, v in int2machine.items()}\n",
        "\n",
        "        with open(os.path.join(DATA_FOLDER, 'human_vocab.json'), 'w') as f:\n",
        "            json.dump(human2int, f)\n",
        "        with open(os.path.join(DATA_FOLDER, 'machine_vocab.json'), 'w') as f:\n",
        "            json.dump(machine2int, f)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DATA_FOLDER: /\n",
            "DATA_FOLDER: /content/keras-attention/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYptL7ouK9Vc",
        "colab_type": "code",
        "outputId": "1c57d468-25d1-4934-efd9-c7e3e8d460c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    print('creating dataset')\n",
        "    create_dataset(os.path.join(DATA_FOLDER, 'training.csv'), 50000,\n",
        "                   vocabulary=True)\n",
        "    create_dataset(os.path.join(DATA_FOLDER, 'validation.csv'), 100)\n",
        "    print('dataset created.')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "creating dataset\n",
            "create_dataset h: 12, sept. 2010 ,m: 2010-09-12 ,_: 2010-09-12\n",
            "create_dataset h: 18 mar. 1990 ,m: 1990-03-18 ,_: 1990-03-18\n",
            "dataset created.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0s2_luBBycQ",
        "colab_type": "code",
        "outputId": "498de7a3-d77d-4cb8-a27b-e494eb6f0cbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "\"\"\"\n",
        "    Runs a simple Neural Machine Translation model\n",
        "    Type `python run.py -h` for help with arguments.\n",
        "\"\"\"\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "#from models.NMT import simpleNMT\n",
        "#from data.reader import Data, Vocabulary\n",
        "#from utils.metrics import all_acc\n",
        "#from utils.examples import run_examples\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEWoyh5a2aS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def all_acc(y_true, y_pred):\n",
        "    \"\"\"\n",
        "        All Accuracy\n",
        "        https://github.com/rasmusbergpalm/normalization/blob/master/train.py#L10\n",
        "    \"\"\"\n",
        "    return K.mean(\n",
        "        K.all(\n",
        "            K.equal(\n",
        "                K.max(y_true, axis=-1),\n",
        "                K.cast(K.argmax(y_pred, axis=-1), K.floatx())\n",
        "            ),\n",
        "            axis=1)\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afVnYsBd3CPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import csv\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "random.seed(1984)\n",
        "\n",
        "INPUT_PADDING = 50\n",
        "OUTPUT_PADDING = 100\n",
        "\n",
        "\n",
        "class Vocabulary(object):\n",
        "\n",
        "    def __init__(self, vocabulary_file, padding=None):\n",
        "        \"\"\"\n",
        "            Creates a vocabulary from a file\n",
        "            :param vocabulary_file: the path to the vocabulary\n",
        "        \"\"\"\n",
        "        print('vocabulary_file:', vocabulary_file)\n",
        "        self.vocabulary_file = vocabulary_file\n",
        "        with open(vocabulary_file, 'r') as f:\n",
        "            self.vocabulary = json.load(f)\n",
        "\n",
        "        self.padding = padding\n",
        "        self.reverse_vocabulary = {v: k for k, v in self.vocabulary.items()}\n",
        "\n",
        "    def size(self):\n",
        "        \"\"\"\n",
        "            Gets the size of the vocabulary\n",
        "        \"\"\"\n",
        "        #print('Vocabulary.size: ', self.vocabulary.keys())\n",
        "        return len(self.vocabulary.keys())\n",
        "\n",
        "    def string_to_int(self, text):\n",
        "        \"\"\"\n",
        "            Converts a string into it's character integer \n",
        "            representation\n",
        "            :param text: text to convert\n",
        "        \"\"\"\n",
        "        characters = list(text)\n",
        "\n",
        "        integers = []\n",
        "\n",
        "        if self.padding and len(characters) >= self.padding:\n",
        "            # truncate if too long\n",
        "            characters = characters[:self.padding - 1]\n",
        "\n",
        "        characters.append('<eot>')\n",
        "\n",
        "        for c in characters:\n",
        "            if c in self.vocabulary:\n",
        "                integers.append(self.vocabulary[c])\n",
        "            else:\n",
        "                integers.append(self.vocabulary['<unk>'])\n",
        "\n",
        "\n",
        "        # pad:\n",
        "        if self.padding and len(integers) < self.padding:\n",
        "            integers.extend([self.vocabulary['<unk>']]\n",
        "                            * (self.padding - len(integers)))\n",
        "\n",
        "        if len(integers) != self.padding:\n",
        "            print(text)\n",
        "            raise AttributeError('Length of text was not padding.')\n",
        "        #print('Vocabulary.string_to_int: ', len(integers))\n",
        "        return integers\n",
        "\n",
        "    def int_to_string(self, integers):\n",
        "        \"\"\"\n",
        "            Decodes a list of integers\n",
        "            into it's string representation\n",
        "        \"\"\"\n",
        "        characters = []\n",
        "        for i in integers:\n",
        "            characters.append(self.reverse_vocabulary[i])\n",
        "        #print('Vocabulary.int_to_string: ', len(characters))\n",
        "        return characters\n",
        "\n",
        "\n",
        "class Data(object):\n",
        "\n",
        "    def __init__(self, file_name, input_vocabulary, output_vocabulary):\n",
        "        \"\"\"\n",
        "            Creates an object that gets data from a file\n",
        "            :param file_name: name of the file to read from\n",
        "            :param vocabulary: the Vocabulary object to use\n",
        "            :param batch_size: the number of datapoints to return\n",
        "            :param padding: the amount of padding to apply to \n",
        "                            a short string\n",
        "        \"\"\"\n",
        "\n",
        "        self.input_vocabulary = input_vocabulary\n",
        "        self.output_vocabulary = output_vocabulary\n",
        "        self.file_name = file_name\n",
        "\n",
        "    def load(self):\n",
        "        \"\"\"\n",
        "            Loads data from a file\n",
        "        \"\"\"\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "\n",
        "        with open(self.file_name, 'r') as f:\n",
        "            reader = csv.reader(f)\n",
        "            for row in reader:\n",
        "                self.inputs.append(row[0])\n",
        "                self.targets.append(row[1])\n",
        "\n",
        "        print('Data.load len(inputs): ', len(self.inputs))\n",
        "        print('Data.load len(targets): ', len(self.targets))\n",
        "        print('Data.load (inputs[0]): ', self.inputs[0])\n",
        "        print('Data.load (targets[0]): ', self.targets[0])\n",
        "\n",
        "    def transform(self):\n",
        "        \"\"\"\n",
        "            Transforms the data as necessary\n",
        "        \"\"\"\n",
        "        # @TODO: use `pool.map_async` here?\n",
        "        self.inputs = np.array(list(\n",
        "            map(self.input_vocabulary.string_to_int, self.inputs)))\n",
        "        self.targets = map(self.output_vocabulary.string_to_int, self.targets)\n",
        "        self.targets = np.array(\n",
        "            list(map(\n",
        "                lambda x: to_categorical(\n",
        "                    x,\n",
        "                    num_classes=self.output_vocabulary.size()),\n",
        "                self.targets)))\n",
        "        print('Data.transform len(inputs): ', len(self.inputs))\n",
        "        print('Data.transform len(targets): ', len(self.targets))\n",
        "        print('Data.transform np.array(inputs): ', np.array(self.inputs))\n",
        "        print('Data.transform np.array(targets): ', np.array(self.targets))\n",
        "        print('Data.transform np.array(inputs)shape: ', np.array(self.inputs).shape)\n",
        "        print('Data.transform np.array(targets)shape: ', np.array(self.targets).shape)\n",
        "        print('Data.transform (inputs[0]): ', self.inputs[0])\n",
        "        print('Data.transform (targets[0]): ', self.targets[0])\n",
        "        assert len(self.inputs.shape) == 2, 'Inputs could not properly be encoded'\n",
        "        assert len(self.targets.shape) == 3, 'Targets could not properly be encoded'\n",
        "\n",
        "    def generator(self, batch_size):\n",
        "        \"\"\"\n",
        "            Creates a generator that can be used in `model.fit_generator()`\n",
        "            Batches are generated randomly.\n",
        "            :param batch_size: the number of instances to include per batch\n",
        "        \"\"\"\n",
        "        instance_id = range(len(self.inputs))\n",
        "        while True:\n",
        "            try:\n",
        "                batch_ids = random.sample(instance_id, batch_size)\n",
        "                #print('Data.generator: batch_ids:', batch_ids+',instance_id:', instance_id+ ',batch_size:', batch_size)\n",
        "                yield (np.array(self.inputs[batch_ids], dtype=int),\n",
        "                       np.array(self.targets[batch_ids]))\n",
        "            except Exception as e:\n",
        "                print('EXCEPTION OMG')\n",
        "                print(e)\n",
        "                yield None, None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD6vt_bLEvxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "EXAMPLES = ['26th January 2016', '3 April 1989', '5 Dec 09', 'Sat 8 Jun 2017']\n",
        "\n",
        "def run_example(model, input_vocabulary, output_vocabulary, text, idx):\n",
        "    encoded = input_vocabulary.string_to_int(text)\n",
        "    prediction = model.predict(np.array([encoded]))\n",
        "    prediction = np.argmax(prediction[0], axis=-1)    \n",
        "\n",
        "    decoded = output_vocabulary.int_to_string(prediction)\n",
        "    if(idx==0):\n",
        "        print('text: ', text, ',encoded:', encoded)\n",
        "        print('run_example.prediction:', prediction)\n",
        "        print('run_example.decoded:', decoded , ',prediction:', prediction)\n",
        "    return decoded\n",
        "\n",
        "def run_examples(model, input_vocabulary, output_vocabulary, examples=EXAMPLES):\n",
        "    predicted = []\n",
        "    idx = 0\n",
        "    for example in examples:\n",
        "        print('~~~~~')\n",
        "        predicted.append(''.join(run_example(model, input_vocabulary, output_vocabulary, example, idx)))\n",
        "        print('input:',example)\n",
        "        print('output:',predicted[-1])\n",
        "        idx = idx + 1\n",
        "    return predicted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWDDxe6p1Zu5",
        "colab_type": "code",
        "outputId": "b9d11778-3ab2-4b09-ce89-50095c501df6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "ls -alrt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 20\n",
            "drwxr-xr-x 1 root root 4096 Oct 25 16:58 \u001b[0m\u001b[01;34msample_data\u001b[0m/\n",
            "drwxr-xr-x 1 root root 4096 Nov  4 16:14 \u001b[01;34m.config\u001b[0m/\n",
            "drwxr-xr-x 1 root root 4096 Nov  6 22:24 \u001b[01;34m..\u001b[0m/\n",
            "drwxr-xr-x 1 root root 4096 Nov  6 22:25 \u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x 7 root root 4096 Nov  6 22:25 \u001b[01;34mkeras-attention\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLsxZ_fh1cuu",
        "colab_type": "code",
        "outputId": "20e0d885-daea-4932-91ca-e5bacb2056df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd keras-attention"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/keras-attention\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Spnc32GD2sSx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Embedding, Activation, Permute\n",
        "from keras.layers import Input, Flatten, Dropout\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
        "from models.custom_recurrents import AttentionDecoder\n",
        "\n",
        "\n",
        "def simpleNMT(pad_length=100,\n",
        "              n_chars=105,\n",
        "              n_labels=6,\n",
        "              embedding_learnable=False,\n",
        "              encoder_units=256,\n",
        "              decoder_units=256,\n",
        "              trainable=True,\n",
        "              return_probabilities=False):\n",
        "    \"\"\"\n",
        "    Builds a Neural Machine Translator that has alignment attention\n",
        "    :param pad_length: the size of the input sequence\n",
        "    :param n_chars: the number of characters in the vocabulary\n",
        "    :param n_labels: the number of possible labelings for each character\n",
        "    :param embedding_learnable: decides if the one hot embedding should be refinable.\n",
        "    :return: keras.models.Model that can be compiled and fit'ed\n",
        "    *** REFERENCES ***\n",
        "    Lee, Jason, Kyunghyun Cho, and Thomas Hofmann. \n",
        "    \"Neural Machine Translation By Jointly Learning To Align and Translate\" \n",
        "    \"\"\"\n",
        "    input_ = Input(shape=(pad_length,), dtype='float32')\n",
        "    print('input_.shape: ', input_.shape)\n",
        "\n",
        "    input_embed = Embedding(n_chars, n_chars,\n",
        "                            input_length=pad_length,\n",
        "                            trainable=embedding_learnable,\n",
        "                            weights=[np.eye(n_chars)],\n",
        "                            name='OneHot')(input_)\n",
        "    print('simpleNMT n_chars:', n_chars,',pad_length:',pad_length,',embedding_learnable:',embedding_learnable,',weights:',[np.eye(n_chars)])\n",
        "\n",
        "    rnn_encoded = Bidirectional(LSTM(encoder_units, return_sequences=True),\n",
        "                                name='bidirectional_1',\n",
        "                                merge_mode='concat',\n",
        "                                trainable=trainable)(input_embed)\n",
        "    print('simpleNMT rnn_encoded:', rnn_encoded)\n",
        "\n",
        "    y_hat = AttentionDecoder(decoder_units,\n",
        "                             name='attention_decoder_1',\n",
        "                             output_dim=n_labels,\n",
        "                             return_probabilities=return_probabilities,\n",
        "                             trainable=trainable)(rnn_encoded)\n",
        "    print('simpleNMT decoder_units:', decoder_units, 'n_labels:', n_labels, ',return_probabilities:',return_probabilities, ',trainable:',trainable)\n",
        "    print('simpleNMT y_hat:', y_hat)\n",
        "\n",
        "    model = Model(inputs=input_, outputs=y_hat)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHu0QkevDpIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "cp = ModelCheckpoint(\"./weights/NMT.{epoch:02d}-{val_loss:.2f}.hdf5\",\n",
        "                     monitor='val_loss',\n",
        "                     verbose=0,\n",
        "                     save_best_only=True,\n",
        "                     save_weights_only=True,\n",
        "                     mode='auto')\n",
        "\n",
        "# create a directory if it doesn't already exist\n",
        "if not os.path.exists('./weights'):\n",
        "    os.makedirs('./weights/')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP1R2hLpFYWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def main():\n",
        "    print('main!')\n",
        "\n",
        "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "    # Dataset functions\n",
        "    #input_vocab = Vocabulary('./data/human_vocab.json', padding=50)\n",
        "    #output_vocab = Vocabulary('./data/machine_vocab.json',padding=50)\n",
        "    input_vocab = Vocabulary('/content/keras-attention/data/human_vocab.json', padding=50)\n",
        "    output_vocab = Vocabulary('/content/keras-attention/data/machine_vocab.json',padding=50)\n",
        "    print('Loading datasets.')\n",
        "\n",
        "    #training = Data('./data/training.csv', input_vocab, output_vocab)\n",
        "    #validation = Data('./data/validation.csv', input_vocab, output_vocab)\n",
        "    training = Data('/content/keras-attention/data/training.csv', input_vocab, output_vocab)\n",
        "    validation = Data('/content/keras-attention/data/validation.csv', input_vocab, output_vocab)\n",
        "    \n",
        "    training.load()\n",
        "    validation.load()\n",
        "    training.transform()\n",
        "    validation.transform()\n",
        "\n",
        "    print('Datasets Loaded.')\n",
        "    print('Compiling Model.')\n",
        "    model = simpleNMT(pad_length=50,\n",
        "                      n_chars=input_vocab.size(),\n",
        "                      n_labels=output_vocab.size(),\n",
        "                      embedding_learnable=False,\n",
        "                      encoder_units=256,\n",
        "                      decoder_units=256,\n",
        "                      trainable=True,\n",
        "                      return_probabilities=False)\n",
        "\n",
        "    model.summary()\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy', all_acc])\n",
        "    print('Model Compiled.')\n",
        "    print('Training. Ctrl+C to end early.')\n",
        "\n",
        "    try:\n",
        "        model.fit_generator(generator=training.generator(32),\n",
        "                            steps_per_epoch=100,\n",
        "                            validation_data=validation.generator(32),\n",
        "                            validation_steps=100,\n",
        "                            callbacks=[cp],\n",
        "                            workers=1,\n",
        "                            verbose=1,\n",
        "                            epochs=10)\n",
        "\n",
        "    except KeyboardInterrupt as e:\n",
        "        print('Model training stopped early.')\n",
        "\n",
        "    print('Model training complete.')\n",
        "\n",
        "    predicted = run_examples(model, input_vocab, output_vocab)\n",
        "    print('len(predicted): ', len(predicted))\n",
        "    print('np.array(predicted).shape: ', np.array(predicted).shape)\n",
        "    print('predicted[0]: ', predicted[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJvPDxN9DpNS",
        "colab_type": "code",
        "outputId": "e10b13c8-9f2c-4fd3-e23f-cb15dd062f23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    main()\n",
        "    # parser = argparse.ArgumentParser()\n",
        "    # named_args = parser.add_argument_group('named arguments')\n",
        "    \n",
        "    # DEF_EPOCHS = 5 #50\n",
        "\n",
        "    # named_args.add_argument('-e', '--epochs', metavar='|',\n",
        "    #                         help=\"\"\"Number of Epochs to Run\"\"\",\n",
        "    #                         required=False, default=DEF_EPOCHS, type=int)\n",
        "\n",
        "    # named_args.add_argument('-g', '--gpu', metavar='|',\n",
        "    #                         help=\"\"\"GPU to use\"\"\",\n",
        "    #                         required=False, default='0', type=str)\n",
        "\n",
        "    # named_args.add_argument('-p', '--padding', metavar='|',\n",
        "    #                         help=\"\"\"Amount of padding to use\"\"\",\n",
        "    #                         required=False, default=DEF_EPOCHS, type=int)\n",
        "\n",
        "    # named_args.add_argument('-t', '--training-data', metavar='|',\n",
        "    #                         help=\"\"\"Location of training data\"\"\",\n",
        "    #                         required=False, default='./data/training.csv')\n",
        "\n",
        "    # named_args.add_argument('-v', '--validation-data', metavar='|',\n",
        "    #                         help=\"\"\"Location of validation data\"\"\",\n",
        "    #                         required=False, default='./data/validation.csv')\n",
        "\n",
        "    # named_args.add_argument('-b', '--batch-size', metavar='|',\n",
        "    #                         help=\"\"\"Location of validation data\"\"\",\n",
        "    #                         required=False, default=32, type=int)\n",
        "    # args = parser.parse_args()\n",
        "    # print(args)\n",
        "    \n",
        "    #main(args)\n",
        "    "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "main!\n",
            "vocabulary_file: /content/keras-attention/data/human_vocab.json\n",
            "vocabulary_file: /content/keras-attention/data/machine_vocab.json\n",
            "Loading datasets.\n",
            "Data.load len(inputs):  50000\n",
            "Data.load len(targets):  50000\n",
            "Data.load (inputs[0]):  12, sept. 2010\n",
            "Data.load (targets[0]):  2010-09-12\n",
            "Data.load len(inputs):  100\n",
            "Data.load len(targets):  100\n",
            "Data.load (inputs[0]):  18 mar. 1990\n",
            "Data.load (targets[0]):  1990-03-18\n",
            "Data.transform len(inputs):  50000\n",
            "Data.transform len(targets):  50000\n",
            "Data.transform np.array(inputs):  [[ 552  277 1131 ... 1333 1333 1333]\n",
            " [ 278 1166   74 ... 1333 1333 1333]\n",
            " [1141 1331  278 ... 1333 1333 1333]\n",
            " ...\n",
            " [ 552  896 1331 ... 1333 1333 1333]\n",
            " [1062   43  856 ... 1333 1333 1333]\n",
            " [ 552 1331  456 ... 1333 1333 1333]]\n",
            "Data.transform np.array(targets):  [[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]]]\n",
            "Data.transform np.array(inputs)shape:  (50000, 50)\n",
            "Data.transform np.array(targets)shape:  (50000, 50, 13)\n",
            "Data.transform (inputs[0]):  [ 552  277 1131 1331   45  378 1252 1093  786 1331  277  456  552  456\n",
            " 1334 1333 1333 1333 1333 1333 1333 1333 1333 1333 1333 1333 1333 1333\n",
            " 1333 1333 1333 1333 1333 1333 1333 1333 1333 1333 1333 1333 1333 1333\n",
            " 1333 1333 1333 1333 1333 1333 1333 1333]\n",
            "Data.transform (targets[0]):  [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "Data.transform len(inputs):  100\n",
            "Data.transform len(targets):  100\n",
            "Data.transform np.array(inputs):  [[ 552 1141 1331 ... 1333 1333 1333]\n",
            " [ 552  277 1331 ... 1333 1333 1333]\n",
            " [ 552  637 1331 ... 1333 1333 1333]\n",
            " ...\n",
            " [ 552 1281 1331 ... 1333 1333 1333]\n",
            " [ 792 1331 1233 ... 1333 1333 1333]\n",
            " [ 277  637 1331 ... 1333 1333 1333]]\n",
            "Data.transform np.array(targets):  [[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 1. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]]]\n",
            "Data.transform np.array(inputs)shape:  (100, 50)\n",
            "Data.transform np.array(targets)shape:  (100, 50, 13)\n",
            "Data.transform (inputs[0]):  [ 552 1141 1331   75  332   72  786 1331  552  896  896  456 1334 1333\n",
            " 1333 1333 1333 1333 1333 1333 1333 1333 1333 1333 1333 1333 1333 1333\n",
            " 1333 1333 1333 1333 1333 1333 1333 1333 1333 1333 1333 1333 1333 1333\n",
            " 1333 1333 1333 1333 1333 1333 1333 1333]\n",
            "Data.transform (targets[0]):  [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "Datasets Loaded.\n",
            "Compiling Model.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "input_.shape:  (?, 50)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "simpleNMT n_chars: 1335 ,pad_length: 50 ,embedding_learnable: False ,weights: [array([[1., 0., 0., ..., 0., 0., 0.],\n",
            "       [0., 1., 0., ..., 0., 0., 0.],\n",
            "       [0., 0., 1., ..., 0., 0., 0.],\n",
            "       ...,\n",
            "       [0., 0., 0., ..., 1., 0., 0.],\n",
            "       [0., 0., 0., ..., 0., 1., 0.],\n",
            "       [0., 0., 0., ..., 0., 0., 1.]])]\n",
            "simpleNMT rnn_encoded: Tensor(\"bidirectional_1/concat:0\", shape=(?, ?, 512), dtype=float32)\n",
            "AttentionDecoder init\n",
            "AttentionDecoder build:\n",
            "AttentionDecoder call:\n",
            "AttentionDecoder get_initial_state:\n",
            "AttentionDecoder step:\n",
            "AttentionDecoder step:\n",
            "AttentionDecoder compute_output_shape:\n",
            "simpleNMT decoder_units: 256 n_labels: 13 ,return_probabilities: False ,trainable: True\n",
            "simpleNMT y_hat: Tensor(\"attention_decoder_1/transpose_4:0\", shape=(?, ?, 13), dtype=float32)\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "OneHot (Embedding)           (None, 50, 1335)          1782225   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 50, 512)           3260416   \n",
            "_________________________________________________________________\n",
            "attention_decoder_1 (Attenti (None, 50, 13)            938934    \n",
            "=================================================================\n",
            "Total params: 5,981,575\n",
            "Trainable params: 4,199,350\n",
            "Non-trainable params: 1,782,225\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model Compiled.\n",
            "Training. Ctrl+C to end early.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 61s 614ms/step - loss: 0.5407 - acc: 0.8363 - all_acc: 0.0000e+00 - val_loss: 0.3316 - val_acc: 0.8734 - val_all_acc: 0.0000e+00\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 58s 582ms/step - loss: 0.3075 - acc: 0.8849 - all_acc: 0.0000e+00 - val_loss: 0.2486 - val_acc: 0.8970 - val_all_acc: 0.0000e+00\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 58s 583ms/step - loss: 0.2382 - acc: 0.8976 - all_acc: 0.0000e+00 - val_loss: 0.2422 - val_acc: 0.8943 - val_all_acc: 0.0000e+00\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 57s 575ms/step - loss: 0.2360 - acc: 0.8967 - all_acc: 0.0000e+00 - val_loss: 0.2376 - val_acc: 0.8947 - val_all_acc: 0.0000e+00\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 58s 581ms/step - loss: 0.2343 - acc: 0.8986 - all_acc: 0.0000e+00 - val_loss: 0.2365 - val_acc: 0.8998 - val_all_acc: 0.0000e+00\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 58s 582ms/step - loss: 0.2284 - acc: 0.9033 - all_acc: 0.0000e+00 - val_loss: 0.2277 - val_acc: 0.9040 - val_all_acc: 0.0000e+00\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 58s 579ms/step - loss: 0.2262 - acc: 0.9039 - all_acc: 0.0000e+00 - val_loss: 0.2244 - val_acc: 0.9049 - val_all_acc: 0.0000e+00\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 58s 582ms/step - loss: 0.2234 - acc: 0.9058 - all_acc: 0.0000e+00 - val_loss: 0.2262 - val_acc: 0.9069 - val_all_acc: 0.0000e+00\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 59s 587ms/step - loss: 0.2141 - acc: 0.9144 - all_acc: 0.0000e+00 - val_loss: 0.2049 - val_acc: 0.9216 - val_all_acc: 0.0000e+00\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 58s 578ms/step - loss: 0.1981 - acc: 0.9235 - all_acc: 0.0000e+00 - val_loss: 0.1895 - val_acc: 0.9271 - val_all_acc: 0.0000e+00\n",
            "Model training complete.\n",
            "~~~~~\n",
            "text:  26th January 2016 ,encoded: [277, 240, 1093, 705, 1331, 662, 332, 751, 43, 332, 72, 523, 1331, 277, 456, 552, 240, 1334, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333, 1333]\n",
            "run_example.prediction: [ 5  7  8  1  6  7  5  6  5  5 12 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
            " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
            " 11 11]\n",
            "run_example.decoded: ['2', '0', '1', '7', '-', '0', '2', '-', '2', '2', '<eot>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>'] ,prediction: [ 5  7  8  1  6  7  5  6  5  5 12 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
            " 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11 11\n",
            " 11 11]\n",
            "input: 26th January 2016\n",
            "output: 2017-02-22<eot><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
            "~~~~~\n",
            "input: 3 April 1989\n",
            "output: 1997-03-03<eot><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
            "~~~~~\n",
            "input: 5 Dec 09\n",
            "output: 1987-02-03<eot><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
            "~~~~~\n",
            "input: Sat 8 Jun 2017\n",
            "output: 2017-02-05<eot><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
            "len(predicted):  4\n",
            "np.array(predicted).shape:  (4,)\n",
            "predicted[0]:  2017-02-22<eot><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}