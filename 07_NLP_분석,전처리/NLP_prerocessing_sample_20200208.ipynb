{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_prerocessing_sample_20200208.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z-EOq2SlAAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#참고: https://programmers.co.kr/learn/courses/21/lessons/1694\n",
        "\n",
        "#!pip show BeautifulSoup4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKtDKMUiRhYq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "78278895-d5aa-4933-aee8-6a83b14afb60"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "train=[]\n",
        "train.append([\"<a>gd test 1234</a>\"])\n",
        "train.append([\"<p>going to bed, it`s good-time.\"])\n",
        "train.append([\"I WAS SLEEPED!!!\"])\n",
        "train.append([\"this breeding horse breeding horses. +\"])\n",
        "train.append([\"coho eggs eyed\"])\n",
        "\n",
        "train_data = np.array(train)\n",
        "print(train_data)\n",
        "#for idx, data in enumerate(train_data) : print('train_data',idx, ':', data)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['<a>gd test 1234</a>']\n",
            " ['<p>going to bed, it`s good-time.']\n",
            " ['I WAS SLEEPED!!!']\n",
            " ['this breeding horse breeding horses. +']\n",
            " ['coho eggs eyed']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQOluFt5OGEl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "897b7738-8302-4604-f993-a4133dee836b"
      },
      "source": [
        "#1. html 제거\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#example1 = BeautifulSoup(train['review'][0], \"html5lib\")\n",
        "#example1.get_text()[:700]\n",
        "\n",
        "for idx, words in enumerate(train_data) : \n",
        "    html_removed = BeautifulSoup(''.join(words)).get_text()\n",
        "    train_data[idx] = [html_removed]\n",
        "\n",
        "train_data_h = train_data.copy()\n",
        "print(train_data_h)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['gd test 1234']\n",
            " ['going to bed, it`s good-time.']\n",
            " ['I WAS SLEEPED!!!']\n",
            " ['this breeding horse breeding horses. +']\n",
            " ['coho eggs eyed']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps_nspjLOGJ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "8e46b29a-ef96-4081-aec1-aab9ee0f9f25"
      },
      "source": [
        "#2. 정규표현식을 사용해서 특수문자를 제거, 소문자변환\n",
        "import re\n",
        "\n",
        "for idx, words in enumerate(train_data_h) : \n",
        "    # 소문자와 대문자가 아닌 것은 공백으로 대체한다.\n",
        "    #letters_only = re.sub('[^a-zA-Z]', ' ', ''.join(data))\n",
        "    \n",
        "    p = re.compile('[a-zA-Z0-9.,-]*')   \n",
        "    word_list = p.findall(''.join(words))\n",
        "\n",
        "    new_word_list = []\n",
        "    for word in word_list:\n",
        "        if(word.strip()==''): continue\n",
        "        new_word_list.append(word.strip())\n",
        "    if(len(new_word_list)<=1): new_word_list = []\n",
        "\n",
        "    train_data_h[idx] = [' '.join(new_word_list).lower()]\n",
        "\n",
        "train_data_r = train_data_h.copy()\n",
        "print(train_data_r)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['gd test 1234']\n",
            " ['going to bed, it s good-time.']\n",
            " ['i was sleeped']\n",
            " ['this breeding horse breeding horses.']\n",
            " ['coho eggs eyed']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2ubXSLoOGCd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "e447f672-1e49-4726-878e-3f323f72bfa9"
      },
      "source": [
        "#3. 불용어 제거\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "print('stopwords.words: ', stops)\n",
        "print()\n",
        "\n",
        "# stopwords 를 제거한 토큰들\n",
        "#words = [w for w in train_data if not w in stopwords.words('english')]\n",
        "\n",
        "for idx, words in enumerate(train_data_r) :\n",
        "    new_words = []\n",
        "    #new_words = [w for w in words if not w in stops]\n",
        "    for word in ''.join(words).split():\n",
        "        if not word in stops: \n",
        "          new_words.append(word)\n",
        "        else: print('stops word:', word)\n",
        "    train_data_r[idx] = [' '.join(new_words)]\n",
        "\n",
        "train_data_s = train_data_r.copy()\n",
        "print(train_data_s)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "stopwords.words:  {'through', 'you', 'such', 'hasn', 'once', \"shan't\", 'herself', \"needn't\", 'of', \"don't\", 're', 'wasn', 'this', 'it', 'i', 'had', 'after', 'on', 'what', 'can', 'being', 'from', 'too', 'both', \"couldn't\", 'some', 'isn', \"you'd\", \"should've\", 'weren', 'and', 'so', 'an', 'for', 'their', \"shouldn't\", 'which', 'were', 'all', 'doing', 'before', \"mustn't\", 'be', 'further', 'few', 'during', 'o', 'her', 'should', 'y', 'couldn', \"it's\", 'that', 'into', 'ain', 'than', \"that'll\", 'while', \"didn't\", 'they', 'needn', 'then', 'hadn', 'm', \"you'll\", 'his', 'whom', 'them', 'yours', \"won't\", 'he', 'yourself', 'those', 'by', 'over', 'at', 'will', 'any', 'down', 'between', 'here', 'these', \"hadn't\", 'more', 'each', 'mustn', 'my', 'does', 'did', 't', 'until', 'ma', 'a', 'nor', \"aren't\", 'the', 'our', 'other', 'am', 'again', 'if', 'just', 'have', \"mightn't\", 'been', 'didn', \"you're\", \"hasn't\", 'off', 'but', \"wouldn't\", 'ourselves', 've', 'no', 'how', 'under', 'aren', 'don', 'below', 'having', \"wasn't\", 'wouldn', 'up', 'theirs', 'who', 'now', 'about', 's', 'its', 'has', 'himself', 'we', \"you've\", 'is', 'do', 'same', 'against', 'above', 'she', 'themselves', \"doesn't\", 'in', 'with', \"haven't\", 'as', 'very', 'shan', \"isn't\", 'him', \"she's\", 'because', \"weren't\", 'most', 'there', 'd', 'or', 'when', 'your', 'hers', 'doesn', 'shouldn', 'myself', 'only', 'won', 'll', 'not', 'why', 'yourselves', 'where', 'mightn', 'itself', 'was', 'ours', 'own', 'me', 'haven', 'to', 'are', 'out'}\n",
            "\n",
            "stops word: to\n",
            "stops word: it\n",
            "stops word: s\n",
            "stops word: i\n",
            "stops word: was\n",
            "stops word: this\n",
            "[['gd test 1234']\n",
            " ['going bed, good-time.']\n",
            " ['sleeped']\n",
            " ['breeding horse breeding horses.']\n",
            " ['coho eggs eyed']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlMlihUOfLYP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "05bb93d6-2396-4cbb-b325-ab4dc5291058"
      },
      "source": [
        "#4. 원형추출(어간추출)\n",
        "#horse -> hors 바뀌는 버그있음?\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "for idx, words in enumerate(train_data_s) :\n",
        "    new_words = []\n",
        "    for word in ''.join(words).split():\n",
        "        new_w = stemmer.stem(word)\n",
        "        new_words.append(new_w)\n",
        "    train_data_s[idx] = [' '.join(new_words)]\n",
        "\n",
        "train_data_st = train_data_s.copy()\n",
        "print(train_data_st)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['gd test 1234']\n",
            " ['go bed, good-time.']\n",
            " ['sleep']\n",
            " ['breed hors breed horses.']\n",
            " ['coho egg eye']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvFpmPZVgD9o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "b5584318-afb0-43ca-8355-a7240219e0d6"
      },
      "source": [
        "#5. Lemmatization 음소표기법\n",
        "# 언어학에서 음소 표기법 (또는 lemmatization)은 단어의 보조 정리 또는 사전 형식에 의해 식별되는 단일 항목으로 분석될 수 있도록 굴절된 형태의 단어를 그룹화하는 과정이다.\n",
        "# 예를 들어 동음이의어가 문맥에 따라 다른 의미가 있는데\n",
        "\n",
        "# 1) 배가 맛있다.\n",
        "# 2) 배를 타는 것이 재미있다.\n",
        "# 3) 평소보다 두 배로 많이 먹어서 배가 아프다.\n",
        "\n",
        "# 위에 있는 3개의 문장에 있는 배는 모두 다른 의미가 있다.\n",
        "\n",
        "# 레마타이제이션은 이때 앞뒤 문맥을 보고 단어의 의미를 식별하는 것이다.\n",
        "# 영어에서 meet는 meeting으로 쓰였을 때 회의를 뜻하지만, meet일 때는 만나다는 뜻을 갖는데 그 단어가 명사로 쓰였는지 동사로 쓰였는지에 따라 적합한 의미가 있도록 추출하는 것이다.\n",
        "\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(wordnet_lemmatizer.lemmatize('fly'))\n",
        "print(wordnet_lemmatizer.lemmatize('flies'))\n",
        "\n",
        "for idx, words in enumerate(train_data_st) :\n",
        "    new_words = []\n",
        "    for word in ''.join(words).split():\n",
        "        new_w = wordnet_lemmatizer.lemmatize(word)\n",
        "        new_words.append(new_w)\n",
        "    train_data_st[idx] = [' '.join(new_words)]\n",
        "\n",
        "train_data_l = train_data_st.copy()\n",
        "print(train_data_l)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "fly\n",
            "fly\n",
            "[['gd test 1234']\n",
            " ['go bed, good-time.']\n",
            " ['sleep']\n",
            " ['breed hors']\n",
            " ['coho egg eye']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeEZetIRhOr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def review_to_words( raw_review ):\n",
        "    # 1. HTML 제거\n",
        "    review_text = BeautifulSoup(raw_review, 'html.parser').get_text()\n",
        "    # 2. 영문자가 아닌 문자는 공백으로 변환\n",
        "    letters_only = re.sub('[^a-zA-Z]', ' ', review_text)\n",
        "    # 3. 소문자 변환\n",
        "    words = letters_only.lower().split()\n",
        "    # 4. 파이썬에서는 리스트보다 세트로 찾는 게 훨씬 빠르다.\n",
        "    # stopwords 를 세트로 변환한다.\n",
        "    stops = set(stopwords.words('english'))\n",
        "    # 5. Stopwords 불용어 제거\n",
        "    meaningful_words = [w for w in words if not w in stops]\n",
        "    # 6. 어간추출\n",
        "    stemming_words = [stemmer.stem(w) for w in meaningful_words]\n",
        "    # 7. 공백으로 구분된 문자열로 결합하여 결과를 반환\n",
        "    return( ' '.join(stemming_words) )"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}