{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clustering_sentence_sample01_20200121.ipynb.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oaKVuk-8X7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#참고: http://www.lumenai.fr/blog/quick-review-on-text-clustering-and-text-similarity-approaches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMnc5woP8eNb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "762d24d8-d2ac-4c5c-8370-ac09748325a8"
      },
      "source": [
        "import collections\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "def word_tokenizer(text):\n",
        "    #tokenizes and stems the text\n",
        "    tokens = word_tokenize(text)\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(t) for t in tokens if t not in stopwords.words('english')]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def cluster_sentences(sentences, nb_of_clusters=5):\n",
        "    tfidf_vectorizer = TfidfVectorizer(tokenizer=word_tokenizer,\n",
        "                                    stop_words=stopwords.words('english'),\n",
        "                                    max_df=0.9,\n",
        "                                    min_df=0.1,\n",
        "                                    lowercase=True)\n",
        "    #builds a tf-idf matrix for the sentences\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
        "\n",
        "    kmeans = KMeans(n_clusters=nb_of_clusters)\n",
        "    kmeans.fit(tfidf_matrix)\n",
        "    \n",
        "    clusters = collections.defaultdict(list)\n",
        "    for i, label in enumerate(kmeans.labels_):\n",
        "            clusters[label].append(i)\n",
        "\n",
        "    return dict(clusters)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofi8v2lS9J8r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "7c183c58-c706-4272-a714-433f91ba2ef4"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    sentences = [\"Nature is beautiful\",\"I like green apples\",\n",
        "            \"We should protect the trees\",\"Fruit trees provide fruits\",\n",
        "            \"Green apples are tasty\"]\n",
        "    nclusters= 3\n",
        "    clusters = cluster_sentences(sentences, nclusters)\n",
        "    for cluster in range(nclusters):\n",
        "            print (\"cluster \",cluster,\":\")\n",
        "            for i,sentence in enumerate(clusters[cluster]):\n",
        "                    print (\"\\tsentence \",i,\": \",sentences[sentence])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cluster  0 :\n",
            "\tsentence  0 :  Nature is beautiful\n",
            "cluster  1 :\n",
            "\tsentence  0 :  I like green apples\n",
            "\tsentence  1 :  Green apples are tasty\n",
            "cluster  2 :\n",
            "\tsentence  0 :  We should protect the trees\n",
            "\tsentence  1 :  Fruit trees provide fruits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}