{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(KOSTA) rnn predict next digit의 사본",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xs6r_3VsiqT",
        "colab_type": "code",
        "outputId": "da25f27b-26d4-4d58-86f6-aa8e39510fa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDu7f8R-t_AU",
        "colab_type": "text"
      },
      "source": [
        "# training set preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Saoxec8VtsuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature = [[[(i+j)] for i in range(5)] for j in range(100)]\n",
        "target = [(i+5) for i in range(100)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKl2GLlUuMdD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "f3c54d20-f048-4c5b-a216-b9a1aa896751"
      },
      "source": [
        "feature = np.array(feature, dtype=float)\n",
        "target = np.array(target, dtype=float)\n",
        "\n",
        "print(target)\n",
        "print(feature[98:,:,:])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  5.   6.   7.   8.   9.  10.  11.  12.  13.  14.  15.  16.  17.  18.\n",
            "  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.  29.  30.  31.  32.\n",
            "  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.  43.  44.  45.  46.\n",
            "  47.  48.  49.  50.  51.  52.  53.  54.  55.  56.  57.  58.  59.  60.\n",
            "  61.  62.  63.  64.  65.  66.  67.  68.  69.  70.  71.  72.  73.  74.\n",
            "  75.  76.  77.  78.  79.  80.  81.  82.  83.  84.  85.  86.  87.  88.\n",
            "  89.  90.  91.  92.  93.  94.  95.  96.  97.  98.  99. 100. 101. 102.\n",
            " 103. 104.]\n",
            "[[[ 98.]\n",
            "  [ 99.]\n",
            "  [100.]\n",
            "  [101.]\n",
            "  [102.]]\n",
            "\n",
            " [[ 99.]\n",
            "  [100.]\n",
            "  [101.]\n",
            "  [102.]\n",
            "  [103.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyyyEwOcBq5e",
        "colab_type": "text"
      },
      "source": [
        "### normalization of the input is needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKU89a3w9hOg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "71f5ba7b-63ff-442d-bedf-39d46392eb3a"
      },
      "source": [
        "feature = feature/100\n",
        "target = target/100\n",
        "\n",
        "print(target)\n",
        "print(feature[98:,:,:])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13 0.14 0.15 0.16 0.17 0.18\n",
            " 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27 0.28 0.29 0.3  0.31 0.32\n",
            " 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4  0.41 0.42 0.43 0.44 0.45 0.46\n",
            " 0.47 0.48 0.49 0.5  0.51 0.52 0.53 0.54 0.55 0.56 0.57 0.58 0.59 0.6\n",
            " 0.61 0.62 0.63 0.64 0.65 0.66 0.67 0.68 0.69 0.7  0.71 0.72 0.73 0.74\n",
            " 0.75 0.76 0.77 0.78 0.79 0.8  0.81 0.82 0.83 0.84 0.85 0.86 0.87 0.88\n",
            " 0.89 0.9  0.91 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99 1.   1.01 1.02\n",
            " 1.03 1.04]\n",
            "[[[0.98]\n",
            "  [0.99]\n",
            "  [1.  ]\n",
            "  [1.01]\n",
            "  [1.02]]\n",
            "\n",
            " [[0.99]\n",
            "  [1.  ]\n",
            "  [1.01]\n",
            "  [1.02]\n",
            "  [1.03]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1TARhfWuTva",
        "colab_type": "code",
        "outputId": "995e70aa-3a26-45bd-e8be-0a732e0562e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(feature.shape)\n",
        "print(target.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 5, 1)\n",
            "(100,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IP2qvs-uado",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "cb86bc1a-d7d7-4629-c09b-00d411fcdd73"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(feature, target, test_size=0.2)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(80, 5, 1)\n",
            "(20, 5, 1)\n",
            "(80,)\n",
            "(20,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUWpExgazKDQ",
        "colab_type": "text"
      },
      "source": [
        "### batch_input_shape = batch_size, timestamp, data_dim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTyW8L7rujzp",
        "colab_type": "code",
        "outputId": "ef1cb8c8-b8e0-4dd8-e7d6-9dc8516fc78e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM((1), batch_input_shape=(None,5,1), return_sequences=False))\n",
        "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0621 15:58:52.107197 140359198967680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0621 15:58:52.160257 140359198967680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0621 15:58:52.167212 140359198967680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0621 15:58:52.389448 140359198967680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 1)                 12        \n",
            "=================================================================\n",
            "Total params: 12\n",
            "Trainable params: 12\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R2ijpr1wNFq",
        "colab_type": "code",
        "outputId": "4f603d4a-ab76-4d5b-aaea-04f94f09c3fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 14728
        }
      },
      "source": [
        "# 50 to 100 to 200\n",
        "history = model.fit(x_train, y_train, epochs=400, validation_split=0.10)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0621 15:59:00.484411 140359198967680 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0621 15:59:00.884555 140359198967680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "W0621 15:59:01.073270 140359198967680 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 72 samples, validate on 8 samples\n",
            "Epoch 1/400\n",
            "72/72 [==============================] - 4s 60ms/step - loss: 0.5088 - acc: 0.0000e+00 - val_loss: 0.5321 - val_acc: 0.0000e+00\n",
            "Epoch 2/400\n",
            "72/72 [==============================] - 0s 516us/step - loss: 0.5064 - acc: 0.0000e+00 - val_loss: 0.5297 - val_acc: 0.0000e+00\n",
            "Epoch 3/400\n",
            "72/72 [==============================] - 0s 535us/step - loss: 0.5040 - acc: 0.0000e+00 - val_loss: 0.5272 - val_acc: 0.0000e+00\n",
            "Epoch 4/400\n",
            "72/72 [==============================] - 0s 531us/step - loss: 0.5016 - acc: 0.0000e+00 - val_loss: 0.5247 - val_acc: 0.0000e+00\n",
            "Epoch 5/400\n",
            "72/72 [==============================] - 0s 575us/step - loss: 0.4991 - acc: 0.0000e+00 - val_loss: 0.5221 - val_acc: 0.0000e+00\n",
            "Epoch 6/400\n",
            "72/72 [==============================] - 0s 546us/step - loss: 0.4966 - acc: 0.0000e+00 - val_loss: 0.5196 - val_acc: 0.0000e+00\n",
            "Epoch 7/400\n",
            "72/72 [==============================] - 0s 565us/step - loss: 0.4941 - acc: 0.0000e+00 - val_loss: 0.5169 - val_acc: 0.0000e+00\n",
            "Epoch 8/400\n",
            "72/72 [==============================] - 0s 518us/step - loss: 0.4915 - acc: 0.0000e+00 - val_loss: 0.5142 - val_acc: 0.0000e+00\n",
            "Epoch 9/400\n",
            "72/72 [==============================] - 0s 571us/step - loss: 0.4888 - acc: 0.0000e+00 - val_loss: 0.5115 - val_acc: 0.0000e+00\n",
            "Epoch 10/400\n",
            "72/72 [==============================] - 0s 540us/step - loss: 0.4861 - acc: 0.0000e+00 - val_loss: 0.5087 - val_acc: 0.0000e+00\n",
            "Epoch 11/400\n",
            "72/72 [==============================] - 0s 573us/step - loss: 0.4834 - acc: 0.0000e+00 - val_loss: 0.5059 - val_acc: 0.0000e+00\n",
            "Epoch 12/400\n",
            "72/72 [==============================] - 0s 551us/step - loss: 0.4806 - acc: 0.0000e+00 - val_loss: 0.5030 - val_acc: 0.0000e+00\n",
            "Epoch 13/400\n",
            "72/72 [==============================] - 0s 544us/step - loss: 0.4777 - acc: 0.0000e+00 - val_loss: 0.5000 - val_acc: 0.0000e+00\n",
            "Epoch 14/400\n",
            "72/72 [==============================] - 0s 521us/step - loss: 0.4748 - acc: 0.0000e+00 - val_loss: 0.4970 - val_acc: 0.0000e+00\n",
            "Epoch 15/400\n",
            "72/72 [==============================] - 0s 485us/step - loss: 0.4719 - acc: 0.0000e+00 - val_loss: 0.4939 - val_acc: 0.0000e+00\n",
            "Epoch 16/400\n",
            "72/72 [==============================] - 0s 579us/step - loss: 0.4688 - acc: 0.0000e+00 - val_loss: 0.4908 - val_acc: 0.0000e+00\n",
            "Epoch 17/400\n",
            "72/72 [==============================] - 0s 652us/step - loss: 0.4657 - acc: 0.0000e+00 - val_loss: 0.4876 - val_acc: 0.0000e+00\n",
            "Epoch 18/400\n",
            "72/72 [==============================] - 0s 534us/step - loss: 0.4625 - acc: 0.0000e+00 - val_loss: 0.4843 - val_acc: 0.0000e+00\n",
            "Epoch 19/400\n",
            "72/72 [==============================] - 0s 514us/step - loss: 0.4593 - acc: 0.0000e+00 - val_loss: 0.4810 - val_acc: 0.0000e+00\n",
            "Epoch 20/400\n",
            "72/72 [==============================] - 0s 517us/step - loss: 0.4561 - acc: 0.0000e+00 - val_loss: 0.4776 - val_acc: 0.0000e+00\n",
            "Epoch 21/400\n",
            "72/72 [==============================] - 0s 515us/step - loss: 0.4527 - acc: 0.0000e+00 - val_loss: 0.4741 - val_acc: 0.0000e+00\n",
            "Epoch 22/400\n",
            "72/72 [==============================] - 0s 536us/step - loss: 0.4493 - acc: 0.0000e+00 - val_loss: 0.4705 - val_acc: 0.0000e+00\n",
            "Epoch 23/400\n",
            "72/72 [==============================] - 0s 586us/step - loss: 0.4458 - acc: 0.0000e+00 - val_loss: 0.4669 - val_acc: 0.0000e+00\n",
            "Epoch 24/400\n",
            "72/72 [==============================] - 0s 603us/step - loss: 0.4423 - acc: 0.0000e+00 - val_loss: 0.4632 - val_acc: 0.0000e+00\n",
            "Epoch 25/400\n",
            "72/72 [==============================] - 0s 539us/step - loss: 0.4386 - acc: 0.0000e+00 - val_loss: 0.4595 - val_acc: 0.0000e+00\n",
            "Epoch 26/400\n",
            "72/72 [==============================] - 0s 527us/step - loss: 0.4350 - acc: 0.0000e+00 - val_loss: 0.4556 - val_acc: 0.0000e+00\n",
            "Epoch 27/400\n",
            "72/72 [==============================] - 0s 661us/step - loss: 0.4313 - acc: 0.0000e+00 - val_loss: 0.4517 - val_acc: 0.0000e+00\n",
            "Epoch 28/400\n",
            "72/72 [==============================] - 0s 520us/step - loss: 0.4275 - acc: 0.0000e+00 - val_loss: 0.4477 - val_acc: 0.0000e+00\n",
            "Epoch 29/400\n",
            "72/72 [==============================] - 0s 545us/step - loss: 0.4237 - acc: 0.0000e+00 - val_loss: 0.4436 - val_acc: 0.0000e+00\n",
            "Epoch 30/400\n",
            "72/72 [==============================] - 0s 531us/step - loss: 0.4199 - acc: 0.0000e+00 - val_loss: 0.4395 - val_acc: 0.0000e+00\n",
            "Epoch 31/400\n",
            "72/72 [==============================] - 0s 532us/step - loss: 0.4160 - acc: 0.0000e+00 - val_loss: 0.4353 - val_acc: 0.0000e+00\n",
            "Epoch 32/400\n",
            "72/72 [==============================] - 0s 556us/step - loss: 0.4120 - acc: 0.0000e+00 - val_loss: 0.4311 - val_acc: 0.0000e+00\n",
            "Epoch 33/400\n",
            "72/72 [==============================] - 0s 525us/step - loss: 0.4080 - acc: 0.0000e+00 - val_loss: 0.4267 - val_acc: 0.0000e+00\n",
            "Epoch 34/400\n",
            "72/72 [==============================] - 0s 588us/step - loss: 0.4041 - acc: 0.0000e+00 - val_loss: 0.4223 - val_acc: 0.0000e+00\n",
            "Epoch 35/400\n",
            "72/72 [==============================] - 0s 531us/step - loss: 0.3999 - acc: 0.0000e+00 - val_loss: 0.4178 - val_acc: 0.0000e+00\n",
            "Epoch 36/400\n",
            "72/72 [==============================] - 0s 553us/step - loss: 0.3958 - acc: 0.0000e+00 - val_loss: 0.4132 - val_acc: 0.0000e+00\n",
            "Epoch 37/400\n",
            "72/72 [==============================] - 0s 563us/step - loss: 0.3917 - acc: 0.0000e+00 - val_loss: 0.4086 - val_acc: 0.0000e+00\n",
            "Epoch 38/400\n",
            "72/72 [==============================] - 0s 547us/step - loss: 0.3874 - acc: 0.0000e+00 - val_loss: 0.4046 - val_acc: 0.0000e+00\n",
            "Epoch 39/400\n",
            "72/72 [==============================] - 0s 558us/step - loss: 0.3830 - acc: 0.0000e+00 - val_loss: 0.4006 - val_acc: 0.0000e+00\n",
            "Epoch 40/400\n",
            "72/72 [==============================] - 0s 509us/step - loss: 0.3786 - acc: 0.0000e+00 - val_loss: 0.3964 - val_acc: 0.0000e+00\n",
            "Epoch 41/400\n",
            "72/72 [==============================] - 0s 603us/step - loss: 0.3743 - acc: 0.0000e+00 - val_loss: 0.3922 - val_acc: 0.0000e+00\n",
            "Epoch 42/400\n",
            "72/72 [==============================] - 0s 652us/step - loss: 0.3698 - acc: 0.0000e+00 - val_loss: 0.3880 - val_acc: 0.0000e+00\n",
            "Epoch 43/400\n",
            "72/72 [==============================] - 0s 577us/step - loss: 0.3653 - acc: 0.0000e+00 - val_loss: 0.3836 - val_acc: 0.0000e+00\n",
            "Epoch 44/400\n",
            "72/72 [==============================] - 0s 564us/step - loss: 0.3607 - acc: 0.0000e+00 - val_loss: 0.3793 - val_acc: 0.0000e+00\n",
            "Epoch 45/400\n",
            "72/72 [==============================] - 0s 510us/step - loss: 0.3562 - acc: 0.0000e+00 - val_loss: 0.3749 - val_acc: 0.0000e+00\n",
            "Epoch 46/400\n",
            "72/72 [==============================] - 0s 513us/step - loss: 0.3516 - acc: 0.0000e+00 - val_loss: 0.3704 - val_acc: 0.0000e+00\n",
            "Epoch 47/400\n",
            "72/72 [==============================] - 0s 563us/step - loss: 0.3469 - acc: 0.0000e+00 - val_loss: 0.3658 - val_acc: 0.0000e+00\n",
            "Epoch 48/400\n",
            "72/72 [==============================] - 0s 740us/step - loss: 0.3423 - acc: 0.0000e+00 - val_loss: 0.3612 - val_acc: 0.0000e+00\n",
            "Epoch 49/400\n",
            "72/72 [==============================] - 0s 516us/step - loss: 0.3375 - acc: 0.0000e+00 - val_loss: 0.3565 - val_acc: 0.0000e+00\n",
            "Epoch 50/400\n",
            "72/72 [==============================] - 0s 581us/step - loss: 0.3326 - acc: 0.0000e+00 - val_loss: 0.3518 - val_acc: 0.0000e+00\n",
            "Epoch 51/400\n",
            "72/72 [==============================] - 0s 510us/step - loss: 0.3277 - acc: 0.0000e+00 - val_loss: 0.3470 - val_acc: 0.0000e+00\n",
            "Epoch 52/400\n",
            "72/72 [==============================] - 0s 516us/step - loss: 0.3230 - acc: 0.0000e+00 - val_loss: 0.3421 - val_acc: 0.0000e+00\n",
            "Epoch 53/400\n",
            "72/72 [==============================] - 0s 508us/step - loss: 0.3183 - acc: 0.0000e+00 - val_loss: 0.3372 - val_acc: 0.0000e+00\n",
            "Epoch 54/400\n",
            "72/72 [==============================] - 0s 525us/step - loss: 0.3133 - acc: 0.0000e+00 - val_loss: 0.3324 - val_acc: 0.0000e+00\n",
            "Epoch 55/400\n",
            "72/72 [==============================] - 0s 555us/step - loss: 0.3087 - acc: 0.0000e+00 - val_loss: 0.3275 - val_acc: 0.0000e+00\n",
            "Epoch 56/400\n",
            "72/72 [==============================] - 0s 576us/step - loss: 0.3041 - acc: 0.0000e+00 - val_loss: 0.3226 - val_acc: 0.0000e+00\n",
            "Epoch 57/400\n",
            "72/72 [==============================] - 0s 542us/step - loss: 0.2996 - acc: 0.0000e+00 - val_loss: 0.3177 - val_acc: 0.0000e+00\n",
            "Epoch 58/400\n",
            "72/72 [==============================] - 0s 522us/step - loss: 0.2950 - acc: 0.0000e+00 - val_loss: 0.3128 - val_acc: 0.0000e+00\n",
            "Epoch 59/400\n",
            "72/72 [==============================] - 0s 521us/step - loss: 0.2906 - acc: 0.0000e+00 - val_loss: 0.3079 - val_acc: 0.0000e+00\n",
            "Epoch 60/400\n",
            "72/72 [==============================] - 0s 552us/step - loss: 0.2862 - acc: 0.0000e+00 - val_loss: 0.3030 - val_acc: 0.0000e+00\n",
            "Epoch 61/400\n",
            "72/72 [==============================] - 0s 590us/step - loss: 0.2821 - acc: 0.0000e+00 - val_loss: 0.2982 - val_acc: 0.0000e+00\n",
            "Epoch 62/400\n",
            "72/72 [==============================] - 0s 525us/step - loss: 0.2779 - acc: 0.0000e+00 - val_loss: 0.2934 - val_acc: 0.0000e+00\n",
            "Epoch 63/400\n",
            "72/72 [==============================] - 0s 511us/step - loss: 0.2737 - acc: 0.0000e+00 - val_loss: 0.2887 - val_acc: 0.0000e+00\n",
            "Epoch 64/400\n",
            "72/72 [==============================] - 0s 555us/step - loss: 0.2700 - acc: 0.0000e+00 - val_loss: 0.2840 - val_acc: 0.0000e+00\n",
            "Epoch 65/400\n",
            "72/72 [==============================] - 0s 563us/step - loss: 0.2661 - acc: 0.0000e+00 - val_loss: 0.2793 - val_acc: 0.0000e+00\n",
            "Epoch 66/400\n",
            "72/72 [==============================] - 0s 677us/step - loss: 0.2623 - acc: 0.0000e+00 - val_loss: 0.2746 - val_acc: 0.0000e+00\n",
            "Epoch 67/400\n",
            "72/72 [==============================] - 0s 566us/step - loss: 0.2584 - acc: 0.0000e+00 - val_loss: 0.2699 - val_acc: 0.0000e+00\n",
            "Epoch 68/400\n",
            "72/72 [==============================] - 0s 502us/step - loss: 0.2549 - acc: 0.0000e+00 - val_loss: 0.2651 - val_acc: 0.0000e+00\n",
            "Epoch 69/400\n",
            "72/72 [==============================] - 0s 546us/step - loss: 0.2510 - acc: 0.0000e+00 - val_loss: 0.2605 - val_acc: 0.0000e+00\n",
            "Epoch 70/400\n",
            "72/72 [==============================] - 0s 550us/step - loss: 0.2476 - acc: 0.0000e+00 - val_loss: 0.2558 - val_acc: 0.0000e+00\n",
            "Epoch 71/400\n",
            "72/72 [==============================] - 0s 530us/step - loss: 0.2445 - acc: 0.0000e+00 - val_loss: 0.2511 - val_acc: 0.0000e+00\n",
            "Epoch 72/400\n",
            "72/72 [==============================] - 0s 546us/step - loss: 0.2409 - acc: 0.0000e+00 - val_loss: 0.2466 - val_acc: 0.0000e+00\n",
            "Epoch 73/400\n",
            "72/72 [==============================] - 0s 579us/step - loss: 0.2377 - acc: 0.0000e+00 - val_loss: 0.2421 - val_acc: 0.0000e+00\n",
            "Epoch 74/400\n",
            "72/72 [==============================] - 0s 549us/step - loss: 0.2348 - acc: 0.0000e+00 - val_loss: 0.2377 - val_acc: 0.0000e+00\n",
            "Epoch 75/400\n",
            "72/72 [==============================] - 0s 539us/step - loss: 0.2319 - acc: 0.0000e+00 - val_loss: 0.2334 - val_acc: 0.0000e+00\n",
            "Epoch 76/400\n",
            "72/72 [==============================] - 0s 513us/step - loss: 0.2292 - acc: 0.0000e+00 - val_loss: 0.2293 - val_acc: 0.0000e+00\n",
            "Epoch 77/400\n",
            "72/72 [==============================] - 0s 597us/step - loss: 0.2265 - acc: 0.0000e+00 - val_loss: 0.2255 - val_acc: 0.0000e+00\n",
            "Epoch 78/400\n",
            "72/72 [==============================] - 0s 550us/step - loss: 0.2241 - acc: 0.0000e+00 - val_loss: 0.2224 - val_acc: 0.0000e+00\n",
            "Epoch 79/400\n",
            "72/72 [==============================] - 0s 575us/step - loss: 0.2217 - acc: 0.0000e+00 - val_loss: 0.2197 - val_acc: 0.0000e+00\n",
            "Epoch 80/400\n",
            "72/72 [==============================] - 0s 548us/step - loss: 0.2193 - acc: 0.0000e+00 - val_loss: 0.2170 - val_acc: 0.0000e+00\n",
            "Epoch 81/400\n",
            "72/72 [==============================] - 0s 519us/step - loss: 0.2171 - acc: 0.0000e+00 - val_loss: 0.2141 - val_acc: 0.0000e+00\n",
            "Epoch 82/400\n",
            "72/72 [==============================] - 0s 547us/step - loss: 0.2147 - acc: 0.0000e+00 - val_loss: 0.2114 - val_acc: 0.0000e+00\n",
            "Epoch 83/400\n",
            "72/72 [==============================] - 0s 528us/step - loss: 0.2125 - acc: 0.0000e+00 - val_loss: 0.2087 - val_acc: 0.0000e+00\n",
            "Epoch 84/400\n",
            "72/72 [==============================] - 0s 610us/step - loss: 0.2102 - acc: 0.0000e+00 - val_loss: 0.2062 - val_acc: 0.0000e+00\n",
            "Epoch 85/400\n",
            "72/72 [==============================] - 0s 532us/step - loss: 0.2083 - acc: 0.0000e+00 - val_loss: 0.2037 - val_acc: 0.0000e+00\n",
            "Epoch 86/400\n",
            "72/72 [==============================] - 0s 525us/step - loss: 0.2063 - acc: 0.0000e+00 - val_loss: 0.2013 - val_acc: 0.0000e+00\n",
            "Epoch 87/400\n",
            "72/72 [==============================] - 0s 544us/step - loss: 0.2046 - acc: 0.0000e+00 - val_loss: 0.1987 - val_acc: 0.0000e+00\n",
            "Epoch 88/400\n",
            "72/72 [==============================] - 0s 567us/step - loss: 0.2025 - acc: 0.0000e+00 - val_loss: 0.1963 - val_acc: 0.0000e+00\n",
            "Epoch 89/400\n",
            "72/72 [==============================] - 0s 538us/step - loss: 0.2009 - acc: 0.0000e+00 - val_loss: 0.1940 - val_acc: 0.0000e+00\n",
            "Epoch 90/400\n",
            "72/72 [==============================] - 0s 665us/step - loss: 0.1992 - acc: 0.0000e+00 - val_loss: 0.1918 - val_acc: 0.0000e+00\n",
            "Epoch 91/400\n",
            "72/72 [==============================] - 0s 666us/step - loss: 0.1976 - acc: 0.0000e+00 - val_loss: 0.1896 - val_acc: 0.0000e+00\n",
            "Epoch 92/400\n",
            "72/72 [==============================] - 0s 534us/step - loss: 0.1960 - acc: 0.0139 - val_loss: 0.1873 - val_acc: 0.0000e+00\n",
            "Epoch 93/400\n",
            "72/72 [==============================] - 0s 515us/step - loss: 0.1945 - acc: 0.0139 - val_loss: 0.1850 - val_acc: 0.0000e+00\n",
            "Epoch 94/400\n",
            "72/72 [==============================] - 0s 519us/step - loss: 0.1932 - acc: 0.0139 - val_loss: 0.1827 - val_acc: 0.0000e+00\n",
            "Epoch 95/400\n",
            "72/72 [==============================] - 0s 549us/step - loss: 0.1917 - acc: 0.0139 - val_loss: 0.1805 - val_acc: 0.0000e+00\n",
            "Epoch 96/400\n",
            "72/72 [==============================] - 0s 537us/step - loss: 0.1902 - acc: 0.0139 - val_loss: 0.1782 - val_acc: 0.0000e+00\n",
            "Epoch 97/400\n",
            "72/72 [==============================] - 0s 564us/step - loss: 0.1889 - acc: 0.0139 - val_loss: 0.1760 - val_acc: 0.0000e+00\n",
            "Epoch 98/400\n",
            "72/72 [==============================] - 0s 535us/step - loss: 0.1876 - acc: 0.0139 - val_loss: 0.1739 - val_acc: 0.0000e+00\n",
            "Epoch 99/400\n",
            "72/72 [==============================] - 0s 532us/step - loss: 0.1862 - acc: 0.0139 - val_loss: 0.1720 - val_acc: 0.0000e+00\n",
            "Epoch 100/400\n",
            "72/72 [==============================] - 0s 510us/step - loss: 0.1850 - acc: 0.0139 - val_loss: 0.1701 - val_acc: 0.0000e+00\n",
            "Epoch 101/400\n",
            "72/72 [==============================] - 0s 568us/step - loss: 0.1839 - acc: 0.0139 - val_loss: 0.1683 - val_acc: 0.0000e+00\n",
            "Epoch 102/400\n",
            "72/72 [==============================] - 0s 600us/step - loss: 0.1827 - acc: 0.0139 - val_loss: 0.1667 - val_acc: 0.0000e+00\n",
            "Epoch 103/400\n",
            "72/72 [==============================] - 0s 552us/step - loss: 0.1815 - acc: 0.0139 - val_loss: 0.1649 - val_acc: 0.0000e+00\n",
            "Epoch 104/400\n",
            "72/72 [==============================] - 0s 542us/step - loss: 0.1805 - acc: 0.0139 - val_loss: 0.1631 - val_acc: 0.0000e+00\n",
            "Epoch 105/400\n",
            "72/72 [==============================] - 0s 504us/step - loss: 0.1794 - acc: 0.0139 - val_loss: 0.1615 - val_acc: 0.0000e+00\n",
            "Epoch 106/400\n",
            "72/72 [==============================] - 0s 562us/step - loss: 0.1783 - acc: 0.0139 - val_loss: 0.1600 - val_acc: 0.0000e+00\n",
            "Epoch 107/400\n",
            "72/72 [==============================] - 0s 508us/step - loss: 0.1772 - acc: 0.0139 - val_loss: 0.1585 - val_acc: 0.0000e+00\n",
            "Epoch 108/400\n",
            "72/72 [==============================] - 0s 599us/step - loss: 0.1762 - acc: 0.0139 - val_loss: 0.1571 - val_acc: 0.0000e+00\n",
            "Epoch 109/400\n",
            "72/72 [==============================] - 0s 519us/step - loss: 0.1752 - acc: 0.0139 - val_loss: 0.1558 - val_acc: 0.0000e+00\n",
            "Epoch 110/400\n",
            "72/72 [==============================] - 0s 518us/step - loss: 0.1741 - acc: 0.0139 - val_loss: 0.1545 - val_acc: 0.0000e+00\n",
            "Epoch 111/400\n",
            "72/72 [==============================] - 0s 514us/step - loss: 0.1731 - acc: 0.0139 - val_loss: 0.1533 - val_acc: 0.0000e+00\n",
            "Epoch 112/400\n",
            "72/72 [==============================] - 0s 506us/step - loss: 0.1720 - acc: 0.0139 - val_loss: 0.1519 - val_acc: 0.0000e+00\n",
            "Epoch 113/400\n",
            "72/72 [==============================] - 0s 530us/step - loss: 0.1710 - acc: 0.0139 - val_loss: 0.1505 - val_acc: 0.0000e+00\n",
            "Epoch 114/400\n",
            "72/72 [==============================] - 0s 605us/step - loss: 0.1700 - acc: 0.0139 - val_loss: 0.1489 - val_acc: 0.0000e+00\n",
            "Epoch 115/400\n",
            "72/72 [==============================] - 0s 546us/step - loss: 0.1689 - acc: 0.0139 - val_loss: 0.1474 - val_acc: 0.0000e+00\n",
            "Epoch 116/400\n",
            "72/72 [==============================] - 0s 710us/step - loss: 0.1679 - acc: 0.0139 - val_loss: 0.1459 - val_acc: 0.0000e+00\n",
            "Epoch 117/400\n",
            "72/72 [==============================] - 0s 528us/step - loss: 0.1668 - acc: 0.0139 - val_loss: 0.1446 - val_acc: 0.0000e+00\n",
            "Epoch 118/400\n",
            "72/72 [==============================] - 0s 500us/step - loss: 0.1658 - acc: 0.0139 - val_loss: 0.1435 - val_acc: 0.0000e+00\n",
            "Epoch 119/400\n",
            "72/72 [==============================] - 0s 650us/step - loss: 0.1648 - acc: 0.0139 - val_loss: 0.1425 - val_acc: 0.0000e+00\n",
            "Epoch 120/400\n",
            "72/72 [==============================] - 0s 537us/step - loss: 0.1637 - acc: 0.0139 - val_loss: 0.1417 - val_acc: 0.0000e+00\n",
            "Epoch 121/400\n",
            "72/72 [==============================] - 0s 502us/step - loss: 0.1626 - acc: 0.0139 - val_loss: 0.1410 - val_acc: 0.0000e+00\n",
            "Epoch 122/400\n",
            "72/72 [==============================] - 0s 513us/step - loss: 0.1615 - acc: 0.0139 - val_loss: 0.1402 - val_acc: 0.0000e+00\n",
            "Epoch 123/400\n",
            "72/72 [==============================] - 0s 534us/step - loss: 0.1605 - acc: 0.0139 - val_loss: 0.1394 - val_acc: 0.0000e+00\n",
            "Epoch 124/400\n",
            "72/72 [==============================] - 0s 574us/step - loss: 0.1594 - acc: 0.0139 - val_loss: 0.1383 - val_acc: 0.0000e+00\n",
            "Epoch 125/400\n",
            "72/72 [==============================] - 0s 552us/step - loss: 0.1583 - acc: 0.0139 - val_loss: 0.1373 - val_acc: 0.0000e+00\n",
            "Epoch 126/400\n",
            "72/72 [==============================] - 0s 543us/step - loss: 0.1572 - acc: 0.0139 - val_loss: 0.1364 - val_acc: 0.0000e+00\n",
            "Epoch 127/400\n",
            "72/72 [==============================] - 0s 568us/step - loss: 0.1561 - acc: 0.0139 - val_loss: 0.1355 - val_acc: 0.0000e+00\n",
            "Epoch 128/400\n",
            "72/72 [==============================] - 0s 567us/step - loss: 0.1549 - acc: 0.0139 - val_loss: 0.1346 - val_acc: 0.0000e+00\n",
            "Epoch 129/400\n",
            "72/72 [==============================] - 0s 543us/step - loss: 0.1538 - acc: 0.0139 - val_loss: 0.1336 - val_acc: 0.0000e+00\n",
            "Epoch 130/400\n",
            "72/72 [==============================] - 0s 559us/step - loss: 0.1526 - acc: 0.0139 - val_loss: 0.1328 - val_acc: 0.0000e+00\n",
            "Epoch 131/400\n",
            "72/72 [==============================] - 0s 535us/step - loss: 0.1515 - acc: 0.0139 - val_loss: 0.1321 - val_acc: 0.0000e+00\n",
            "Epoch 132/400\n",
            "72/72 [==============================] - 0s 521us/step - loss: 0.1503 - acc: 0.0139 - val_loss: 0.1315 - val_acc: 0.0000e+00\n",
            "Epoch 133/400\n",
            "72/72 [==============================] - 0s 577us/step - loss: 0.1493 - acc: 0.0139 - val_loss: 0.1310 - val_acc: 0.0000e+00\n",
            "Epoch 134/400\n",
            "72/72 [==============================] - 0s 562us/step - loss: 0.1481 - acc: 0.0139 - val_loss: 0.1300 - val_acc: 0.0000e+00\n",
            "Epoch 135/400\n",
            "72/72 [==============================] - 0s 548us/step - loss: 0.1469 - acc: 0.0139 - val_loss: 0.1290 - val_acc: 0.0000e+00\n",
            "Epoch 136/400\n",
            "72/72 [==============================] - 0s 605us/step - loss: 0.1457 - acc: 0.0139 - val_loss: 0.1279 - val_acc: 0.0000e+00\n",
            "Epoch 137/400\n",
            "72/72 [==============================] - 0s 523us/step - loss: 0.1445 - acc: 0.0139 - val_loss: 0.1266 - val_acc: 0.0000e+00\n",
            "Epoch 138/400\n",
            "72/72 [==============================] - 0s 550us/step - loss: 0.1433 - acc: 0.0139 - val_loss: 0.1251 - val_acc: 0.0000e+00\n",
            "Epoch 139/400\n",
            "72/72 [==============================] - 0s 554us/step - loss: 0.1420 - acc: 0.0139 - val_loss: 0.1237 - val_acc: 0.0000e+00\n",
            "Epoch 140/400\n",
            "72/72 [==============================] - 0s 564us/step - loss: 0.1408 - acc: 0.0139 - val_loss: 0.1222 - val_acc: 0.0000e+00\n",
            "Epoch 141/400\n",
            "72/72 [==============================] - 0s 739us/step - loss: 0.1396 - acc: 0.0139 - val_loss: 0.1206 - val_acc: 0.0000e+00\n",
            "Epoch 142/400\n",
            "72/72 [==============================] - 0s 519us/step - loss: 0.1384 - acc: 0.0139 - val_loss: 0.1190 - val_acc: 0.0000e+00\n",
            "Epoch 143/400\n",
            "72/72 [==============================] - 0s 541us/step - loss: 0.1371 - acc: 0.0139 - val_loss: 0.1173 - val_acc: 0.0000e+00\n",
            "Epoch 144/400\n",
            "72/72 [==============================] - 0s 608us/step - loss: 0.1358 - acc: 0.0139 - val_loss: 0.1159 - val_acc: 0.0000e+00\n",
            "Epoch 145/400\n",
            "72/72 [==============================] - 0s 514us/step - loss: 0.1345 - acc: 0.0139 - val_loss: 0.1146 - val_acc: 0.0000e+00\n",
            "Epoch 146/400\n",
            "72/72 [==============================] - 0s 538us/step - loss: 0.1332 - acc: 0.0139 - val_loss: 0.1136 - val_acc: 0.0000e+00\n",
            "Epoch 147/400\n",
            "72/72 [==============================] - 0s 547us/step - loss: 0.1320 - acc: 0.0139 - val_loss: 0.1128 - val_acc: 0.0000e+00\n",
            "Epoch 148/400\n",
            "72/72 [==============================] - 0s 544us/step - loss: 0.1307 - acc: 0.0139 - val_loss: 0.1116 - val_acc: 0.0000e+00\n",
            "Epoch 149/400\n",
            "72/72 [==============================] - 0s 519us/step - loss: 0.1293 - acc: 0.0139 - val_loss: 0.1106 - val_acc: 0.0000e+00\n",
            "Epoch 150/400\n",
            "72/72 [==============================] - 0s 538us/step - loss: 0.1280 - acc: 0.0139 - val_loss: 0.1095 - val_acc: 0.0000e+00\n",
            "Epoch 151/400\n",
            "72/72 [==============================] - 0s 561us/step - loss: 0.1267 - acc: 0.0139 - val_loss: 0.1085 - val_acc: 0.0000e+00\n",
            "Epoch 152/400\n",
            "72/72 [==============================] - 0s 599us/step - loss: 0.1253 - acc: 0.0139 - val_loss: 0.1073 - val_acc: 0.0000e+00\n",
            "Epoch 153/400\n",
            "72/72 [==============================] - 0s 527us/step - loss: 0.1240 - acc: 0.0139 - val_loss: 0.1063 - val_acc: 0.0000e+00\n",
            "Epoch 154/400\n",
            "72/72 [==============================] - 0s 509us/step - loss: 0.1227 - acc: 0.0139 - val_loss: 0.1050 - val_acc: 0.0000e+00\n",
            "Epoch 155/400\n",
            "72/72 [==============================] - 0s 536us/step - loss: 0.1213 - acc: 0.0139 - val_loss: 0.1039 - val_acc: 0.0000e+00\n",
            "Epoch 156/400\n",
            "72/72 [==============================] - 0s 514us/step - loss: 0.1200 - acc: 0.0139 - val_loss: 0.1029 - val_acc: 0.0000e+00\n",
            "Epoch 157/400\n",
            "72/72 [==============================] - 0s 627us/step - loss: 0.1187 - acc: 0.0139 - val_loss: 0.1019 - val_acc: 0.0000e+00\n",
            "Epoch 158/400\n",
            "72/72 [==============================] - 0s 616us/step - loss: 0.1172 - acc: 0.0139 - val_loss: 0.1007 - val_acc: 0.0000e+00\n",
            "Epoch 159/400\n",
            "72/72 [==============================] - 0s 513us/step - loss: 0.1159 - acc: 0.0139 - val_loss: 0.0994 - val_acc: 0.0000e+00\n",
            "Epoch 160/400\n",
            "72/72 [==============================] - 0s 552us/step - loss: 0.1144 - acc: 0.0139 - val_loss: 0.0982 - val_acc: 0.0000e+00\n",
            "Epoch 161/400\n",
            "72/72 [==============================] - 0s 514us/step - loss: 0.1130 - acc: 0.0139 - val_loss: 0.0970 - val_acc: 0.0000e+00\n",
            "Epoch 162/400\n",
            "72/72 [==============================] - 0s 549us/step - loss: 0.1116 - acc: 0.0139 - val_loss: 0.0960 - val_acc: 0.0000e+00\n",
            "Epoch 163/400\n",
            "72/72 [==============================] - 0s 543us/step - loss: 0.1102 - acc: 0.0139 - val_loss: 0.0952 - val_acc: 0.0000e+00\n",
            "Epoch 164/400\n",
            "72/72 [==============================] - 0s 575us/step - loss: 0.1088 - acc: 0.0139 - val_loss: 0.0942 - val_acc: 0.0000e+00\n",
            "Epoch 165/400\n",
            "72/72 [==============================] - 0s 571us/step - loss: 0.1074 - acc: 0.0139 - val_loss: 0.0933 - val_acc: 0.0000e+00\n",
            "Epoch 166/400\n",
            "72/72 [==============================] - 0s 678us/step - loss: 0.1059 - acc: 0.0139 - val_loss: 0.0919 - val_acc: 0.0000e+00\n",
            "Epoch 167/400\n",
            "72/72 [==============================] - 0s 624us/step - loss: 0.1044 - acc: 0.0139 - val_loss: 0.0904 - val_acc: 0.0000e+00\n",
            "Epoch 168/400\n",
            "72/72 [==============================] - 0s 509us/step - loss: 0.1029 - acc: 0.0139 - val_loss: 0.0887 - val_acc: 0.0000e+00\n",
            "Epoch 169/400\n",
            "72/72 [==============================] - 0s 591us/step - loss: 0.1014 - acc: 0.0139 - val_loss: 0.0868 - val_acc: 0.0000e+00\n",
            "Epoch 170/400\n",
            "72/72 [==============================] - 0s 504us/step - loss: 0.0998 - acc: 0.0139 - val_loss: 0.0852 - val_acc: 0.0000e+00\n",
            "Epoch 171/400\n",
            "72/72 [==============================] - 0s 496us/step - loss: 0.0983 - acc: 0.0139 - val_loss: 0.0836 - val_acc: 0.0000e+00\n",
            "Epoch 172/400\n",
            "72/72 [==============================] - 0s 522us/step - loss: 0.0967 - acc: 0.0139 - val_loss: 0.0819 - val_acc: 0.0000e+00\n",
            "Epoch 173/400\n",
            "72/72 [==============================] - 0s 538us/step - loss: 0.0952 - acc: 0.0139 - val_loss: 0.0804 - val_acc: 0.0000e+00\n",
            "Epoch 174/400\n",
            "72/72 [==============================] - 0s 532us/step - loss: 0.0936 - acc: 0.0139 - val_loss: 0.0790 - val_acc: 0.0000e+00\n",
            "Epoch 175/400\n",
            "72/72 [==============================] - 0s 595us/step - loss: 0.0921 - acc: 0.0139 - val_loss: 0.0773 - val_acc: 0.0000e+00\n",
            "Epoch 176/400\n",
            "72/72 [==============================] - 0s 583us/step - loss: 0.0905 - acc: 0.0139 - val_loss: 0.0756 - val_acc: 0.0000e+00\n",
            "Epoch 177/400\n",
            "72/72 [==============================] - 0s 526us/step - loss: 0.0889 - acc: 0.0139 - val_loss: 0.0740 - val_acc: 0.0000e+00\n",
            "Epoch 178/400\n",
            "72/72 [==============================] - 0s 539us/step - loss: 0.0872 - acc: 0.0139 - val_loss: 0.0722 - val_acc: 0.0000e+00\n",
            "Epoch 179/400\n",
            "72/72 [==============================] - 0s 546us/step - loss: 0.0856 - acc: 0.0139 - val_loss: 0.0702 - val_acc: 0.0000e+00\n",
            "Epoch 180/400\n",
            "72/72 [==============================] - 0s 555us/step - loss: 0.0840 - acc: 0.0139 - val_loss: 0.0686 - val_acc: 0.0000e+00\n",
            "Epoch 181/400\n",
            "72/72 [==============================] - 0s 540us/step - loss: 0.0823 - acc: 0.0139 - val_loss: 0.0671 - val_acc: 0.0000e+00\n",
            "Epoch 182/400\n",
            "72/72 [==============================] - 0s 570us/step - loss: 0.0806 - acc: 0.0139 - val_loss: 0.0657 - val_acc: 0.0000e+00\n",
            "Epoch 183/400\n",
            "72/72 [==============================] - 0s 537us/step - loss: 0.0789 - acc: 0.0139 - val_loss: 0.0642 - val_acc: 0.0000e+00\n",
            "Epoch 184/400\n",
            "72/72 [==============================] - 0s 552us/step - loss: 0.0772 - acc: 0.0139 - val_loss: 0.0629 - val_acc: 0.0000e+00\n",
            "Epoch 185/400\n",
            "72/72 [==============================] - 0s 534us/step - loss: 0.0755 - acc: 0.0139 - val_loss: 0.0617 - val_acc: 0.0000e+00\n",
            "Epoch 186/400\n",
            "72/72 [==============================] - 0s 545us/step - loss: 0.0737 - acc: 0.0139 - val_loss: 0.0608 - val_acc: 0.0000e+00\n",
            "Epoch 187/400\n",
            "72/72 [==============================] - 0s 518us/step - loss: 0.0720 - acc: 0.0139 - val_loss: 0.0600 - val_acc: 0.0000e+00\n",
            "Epoch 188/400\n",
            "72/72 [==============================] - 0s 533us/step - loss: 0.0702 - acc: 0.0139 - val_loss: 0.0591 - val_acc: 0.0000e+00\n",
            "Epoch 189/400\n",
            "72/72 [==============================] - 0s 658us/step - loss: 0.0685 - acc: 0.0139 - val_loss: 0.0579 - val_acc: 0.0000e+00\n",
            "Epoch 190/400\n",
            "72/72 [==============================] - 0s 546us/step - loss: 0.0667 - acc: 0.0139 - val_loss: 0.0566 - val_acc: 0.0000e+00\n",
            "Epoch 191/400\n",
            "72/72 [==============================] - 0s 718us/step - loss: 0.0649 - acc: 0.0139 - val_loss: 0.0547 - val_acc: 0.0000e+00\n",
            "Epoch 192/400\n",
            "72/72 [==============================] - 0s 520us/step - loss: 0.0630 - acc: 0.0139 - val_loss: 0.0525 - val_acc: 0.0000e+00\n",
            "Epoch 193/400\n",
            "72/72 [==============================] - 0s 517us/step - loss: 0.0611 - acc: 0.0139 - val_loss: 0.0503 - val_acc: 0.0000e+00\n",
            "Epoch 194/400\n",
            "72/72 [==============================] - 0s 528us/step - loss: 0.0592 - acc: 0.0139 - val_loss: 0.0485 - val_acc: 0.0000e+00\n",
            "Epoch 195/400\n",
            "72/72 [==============================] - 0s 540us/step - loss: 0.0574 - acc: 0.0139 - val_loss: 0.0468 - val_acc: 0.0000e+00\n",
            "Epoch 196/400\n",
            "72/72 [==============================] - 0s 508us/step - loss: 0.0556 - acc: 0.0139 - val_loss: 0.0455 - val_acc: 0.0000e+00\n",
            "Epoch 197/400\n",
            "72/72 [==============================] - 0s 631us/step - loss: 0.0536 - acc: 0.0139 - val_loss: 0.0443 - val_acc: 0.0000e+00\n",
            "Epoch 198/400\n",
            "72/72 [==============================] - 0s 535us/step - loss: 0.0518 - acc: 0.0139 - val_loss: 0.0429 - val_acc: 0.0000e+00\n",
            "Epoch 199/400\n",
            "72/72 [==============================] - 0s 535us/step - loss: 0.0498 - acc: 0.0139 - val_loss: 0.0415 - val_acc: 0.0000e+00\n",
            "Epoch 200/400\n",
            "72/72 [==============================] - 0s 559us/step - loss: 0.0479 - acc: 0.0139 - val_loss: 0.0399 - val_acc: 0.0000e+00\n",
            "Epoch 201/400\n",
            "72/72 [==============================] - 0s 507us/step - loss: 0.0462 - acc: 0.0139 - val_loss: 0.0383 - val_acc: 0.0000e+00\n",
            "Epoch 202/400\n",
            "72/72 [==============================] - 0s 561us/step - loss: 0.0445 - acc: 0.0139 - val_loss: 0.0374 - val_acc: 0.0000e+00\n",
            "Epoch 203/400\n",
            "72/72 [==============================] - 0s 527us/step - loss: 0.0429 - acc: 0.0139 - val_loss: 0.0368 - val_acc: 0.0000e+00\n",
            "Epoch 204/400\n",
            "72/72 [==============================] - 0s 564us/step - loss: 0.0416 - acc: 0.0139 - val_loss: 0.0366 - val_acc: 0.0000e+00\n",
            "Epoch 205/400\n",
            "72/72 [==============================] - 0s 561us/step - loss: 0.0403 - acc: 0.0139 - val_loss: 0.0363 - val_acc: 0.0000e+00\n",
            "Epoch 206/400\n",
            "72/72 [==============================] - 0s 544us/step - loss: 0.0393 - acc: 0.0139 - val_loss: 0.0361 - val_acc: 0.0000e+00\n",
            "Epoch 207/400\n",
            "72/72 [==============================] - 0s 560us/step - loss: 0.0383 - acc: 0.0139 - val_loss: 0.0361 - val_acc: 0.0000e+00\n",
            "Epoch 208/400\n",
            "72/72 [==============================] - 0s 538us/step - loss: 0.0374 - acc: 0.0139 - val_loss: 0.0360 - val_acc: 0.0000e+00\n",
            "Epoch 209/400\n",
            "72/72 [==============================] - 0s 577us/step - loss: 0.0367 - acc: 0.0139 - val_loss: 0.0361 - val_acc: 0.0000e+00\n",
            "Epoch 210/400\n",
            "72/72 [==============================] - 0s 529us/step - loss: 0.0360 - acc: 0.0139 - val_loss: 0.0361 - val_acc: 0.0000e+00\n",
            "Epoch 211/400\n",
            "72/72 [==============================] - 0s 539us/step - loss: 0.0356 - acc: 0.0139 - val_loss: 0.0363 - val_acc: 0.0000e+00\n",
            "Epoch 212/400\n",
            "72/72 [==============================] - 0s 568us/step - loss: 0.0350 - acc: 0.0139 - val_loss: 0.0356 - val_acc: 0.0000e+00\n",
            "Epoch 213/400\n",
            "72/72 [==============================] - 0s 623us/step - loss: 0.0345 - acc: 0.0139 - val_loss: 0.0349 - val_acc: 0.0000e+00\n",
            "Epoch 214/400\n",
            "72/72 [==============================] - 0s 540us/step - loss: 0.0343 - acc: 0.0139 - val_loss: 0.0347 - val_acc: 0.0000e+00\n",
            "Epoch 215/400\n",
            "72/72 [==============================] - 0s 522us/step - loss: 0.0340 - acc: 0.0139 - val_loss: 0.0351 - val_acc: 0.0000e+00\n",
            "Epoch 216/400\n",
            "72/72 [==============================] - 0s 674us/step - loss: 0.0336 - acc: 0.0139 - val_loss: 0.0352 - val_acc: 0.0000e+00\n",
            "Epoch 217/400\n",
            "72/72 [==============================] - 0s 532us/step - loss: 0.0334 - acc: 0.0139 - val_loss: 0.0353 - val_acc: 0.0000e+00\n",
            "Epoch 218/400\n",
            "72/72 [==============================] - 0s 548us/step - loss: 0.0332 - acc: 0.0139 - val_loss: 0.0350 - val_acc: 0.0000e+00\n",
            "Epoch 219/400\n",
            "72/72 [==============================] - 0s 601us/step - loss: 0.0329 - acc: 0.0139 - val_loss: 0.0345 - val_acc: 0.0000e+00\n",
            "Epoch 220/400\n",
            "72/72 [==============================] - 0s 541us/step - loss: 0.0327 - acc: 0.0139 - val_loss: 0.0341 - val_acc: 0.0000e+00\n",
            "Epoch 221/400\n",
            "72/72 [==============================] - 0s 565us/step - loss: 0.0326 - acc: 0.0139 - val_loss: 0.0336 - val_acc: 0.0000e+00\n",
            "Epoch 222/400\n",
            "72/72 [==============================] - 0s 512us/step - loss: 0.0323 - acc: 0.0139 - val_loss: 0.0338 - val_acc: 0.0000e+00\n",
            "Epoch 223/400\n",
            "72/72 [==============================] - 0s 522us/step - loss: 0.0322 - acc: 0.0139 - val_loss: 0.0339 - val_acc: 0.0000e+00\n",
            "Epoch 224/400\n",
            "72/72 [==============================] - 0s 579us/step - loss: 0.0319 - acc: 0.0139 - val_loss: 0.0335 - val_acc: 0.0000e+00\n",
            "Epoch 225/400\n",
            "72/72 [==============================] - 0s 559us/step - loss: 0.0317 - acc: 0.0139 - val_loss: 0.0331 - val_acc: 0.0000e+00\n",
            "Epoch 226/400\n",
            "72/72 [==============================] - 0s 602us/step - loss: 0.0315 - acc: 0.0139 - val_loss: 0.0329 - val_acc: 0.0000e+00\n",
            "Epoch 227/400\n",
            "72/72 [==============================] - 0s 506us/step - loss: 0.0313 - acc: 0.0139 - val_loss: 0.0324 - val_acc: 0.0000e+00\n",
            "Epoch 228/400\n",
            "72/72 [==============================] - 0s 523us/step - loss: 0.0312 - acc: 0.0139 - val_loss: 0.0320 - val_acc: 0.0000e+00\n",
            "Epoch 229/400\n",
            "72/72 [==============================] - 0s 529us/step - loss: 0.0310 - acc: 0.0139 - val_loss: 0.0315 - val_acc: 0.0000e+00\n",
            "Epoch 230/400\n",
            "72/72 [==============================] - 0s 525us/step - loss: 0.0309 - acc: 0.0139 - val_loss: 0.0313 - val_acc: 0.0000e+00\n",
            "Epoch 231/400\n",
            "72/72 [==============================] - 0s 552us/step - loss: 0.0307 - acc: 0.0139 - val_loss: 0.0314 - val_acc: 0.0000e+00\n",
            "Epoch 232/400\n",
            "72/72 [==============================] - 0s 531us/step - loss: 0.0305 - acc: 0.0139 - val_loss: 0.0317 - val_acc: 0.0000e+00\n",
            "Epoch 233/400\n",
            "72/72 [==============================] - 0s 530us/step - loss: 0.0303 - acc: 0.0139 - val_loss: 0.0323 - val_acc: 0.0000e+00\n",
            "Epoch 234/400\n",
            "72/72 [==============================] - 0s 538us/step - loss: 0.0302 - acc: 0.0139 - val_loss: 0.0326 - val_acc: 0.0000e+00\n",
            "Epoch 235/400\n",
            "72/72 [==============================] - 0s 600us/step - loss: 0.0301 - acc: 0.0139 - val_loss: 0.0321 - val_acc: 0.0000e+00\n",
            "Epoch 236/400\n",
            "72/72 [==============================] - 0s 507us/step - loss: 0.0299 - acc: 0.0139 - val_loss: 0.0313 - val_acc: 0.0000e+00\n",
            "Epoch 237/400\n",
            "72/72 [==============================] - 0s 574us/step - loss: 0.0297 - acc: 0.0139 - val_loss: 0.0311 - val_acc: 0.0000e+00\n",
            "Epoch 238/400\n",
            "72/72 [==============================] - 0s 552us/step - loss: 0.0295 - acc: 0.0139 - val_loss: 0.0312 - val_acc: 0.0000e+00\n",
            "Epoch 239/400\n",
            "72/72 [==============================] - 0s 536us/step - loss: 0.0293 - acc: 0.0139 - val_loss: 0.0307 - val_acc: 0.0000e+00\n",
            "Epoch 240/400\n",
            "72/72 [==============================] - 0s 546us/step - loss: 0.0292 - acc: 0.0139 - val_loss: 0.0302 - val_acc: 0.0000e+00\n",
            "Epoch 241/400\n",
            "72/72 [==============================] - 0s 740us/step - loss: 0.0290 - acc: 0.0139 - val_loss: 0.0301 - val_acc: 0.0000e+00\n",
            "Epoch 242/400\n",
            "72/72 [==============================] - 0s 512us/step - loss: 0.0289 - acc: 0.0139 - val_loss: 0.0301 - val_acc: 0.0000e+00\n",
            "Epoch 243/400\n",
            "72/72 [==============================] - 0s 578us/step - loss: 0.0287 - acc: 0.0139 - val_loss: 0.0304 - val_acc: 0.0000e+00\n",
            "Epoch 244/400\n",
            "72/72 [==============================] - 0s 519us/step - loss: 0.0287 - acc: 0.0139 - val_loss: 0.0306 - val_acc: 0.0000e+00\n",
            "Epoch 245/400\n",
            "72/72 [==============================] - 0s 512us/step - loss: 0.0284 - acc: 0.0139 - val_loss: 0.0301 - val_acc: 0.0000e+00\n",
            "Epoch 246/400\n",
            "72/72 [==============================] - 0s 546us/step - loss: 0.0283 - acc: 0.0139 - val_loss: 0.0297 - val_acc: 0.0000e+00\n",
            "Epoch 247/400\n",
            "72/72 [==============================] - 0s 534us/step - loss: 0.0281 - acc: 0.0139 - val_loss: 0.0292 - val_acc: 0.0000e+00\n",
            "Epoch 248/400\n",
            "72/72 [==============================] - 0s 565us/step - loss: 0.0280 - acc: 0.0139 - val_loss: 0.0287 - val_acc: 0.0000e+00\n",
            "Epoch 249/400\n",
            "72/72 [==============================] - 0s 602us/step - loss: 0.0278 - acc: 0.0139 - val_loss: 0.0284 - val_acc: 0.0000e+00\n",
            "Epoch 250/400\n",
            "72/72 [==============================] - 0s 512us/step - loss: 0.0277 - acc: 0.0139 - val_loss: 0.0279 - val_acc: 0.0000e+00\n",
            "Epoch 251/400\n",
            "72/72 [==============================] - 0s 553us/step - loss: 0.0276 - acc: 0.0139 - val_loss: 0.0274 - val_acc: 0.0000e+00\n",
            "Epoch 252/400\n",
            "72/72 [==============================] - 0s 607us/step - loss: 0.0275 - acc: 0.0139 - val_loss: 0.0276 - val_acc: 0.0000e+00\n",
            "Epoch 253/400\n",
            "72/72 [==============================] - 0s 536us/step - loss: 0.0272 - acc: 0.0139 - val_loss: 0.0283 - val_acc: 0.0000e+00\n",
            "Epoch 254/400\n",
            "72/72 [==============================] - 0s 516us/step - loss: 0.0271 - acc: 0.0139 - val_loss: 0.0284 - val_acc: 0.0000e+00\n",
            "Epoch 255/400\n",
            "72/72 [==============================] - 0s 503us/step - loss: 0.0270 - acc: 0.0139 - val_loss: 0.0280 - val_acc: 0.0000e+00\n",
            "Epoch 256/400\n",
            "72/72 [==============================] - 0s 531us/step - loss: 0.0268 - acc: 0.0139 - val_loss: 0.0275 - val_acc: 0.0000e+00\n",
            "Epoch 257/400\n",
            "72/72 [==============================] - 0s 629us/step - loss: 0.0266 - acc: 0.0139 - val_loss: 0.0274 - val_acc: 0.0000e+00\n",
            "Epoch 258/400\n",
            "72/72 [==============================] - 0s 571us/step - loss: 0.0265 - acc: 0.0139 - val_loss: 0.0274 - val_acc: 0.0000e+00\n",
            "Epoch 259/400\n",
            "72/72 [==============================] - 0s 587us/step - loss: 0.0264 - acc: 0.0139 - val_loss: 0.0270 - val_acc: 0.0000e+00\n",
            "Epoch 260/400\n",
            "72/72 [==============================] - 0s 510us/step - loss: 0.0263 - acc: 0.0139 - val_loss: 0.0265 - val_acc: 0.0000e+00\n",
            "Epoch 261/400\n",
            "72/72 [==============================] - 0s 554us/step - loss: 0.0262 - acc: 0.0139 - val_loss: 0.0257 - val_acc: 0.0000e+00\n",
            "Epoch 262/400\n",
            "72/72 [==============================] - 0s 530us/step - loss: 0.0262 - acc: 0.0139 - val_loss: 0.0254 - val_acc: 0.0000e+00\n",
            "Epoch 263/400\n",
            "72/72 [==============================] - 0s 635us/step - loss: 0.0260 - acc: 0.0139 - val_loss: 0.0256 - val_acc: 0.0000e+00\n",
            "Epoch 264/400\n",
            "72/72 [==============================] - 0s 546us/step - loss: 0.0258 - acc: 0.0139 - val_loss: 0.0260 - val_acc: 0.0000e+00\n",
            "Epoch 265/400\n",
            "72/72 [==============================] - 0s 518us/step - loss: 0.0256 - acc: 0.0139 - val_loss: 0.0264 - val_acc: 0.0000e+00\n",
            "Epoch 266/400\n",
            "72/72 [==============================] - 0s 648us/step - loss: 0.0255 - acc: 0.0139 - val_loss: 0.0262 - val_acc: 0.0000e+00\n",
            "Epoch 267/400\n",
            "72/72 [==============================] - 0s 541us/step - loss: 0.0254 - acc: 0.0139 - val_loss: 0.0257 - val_acc: 0.0000e+00\n",
            "Epoch 268/400\n",
            "72/72 [==============================] - 0s 553us/step - loss: 0.0252 - acc: 0.0139 - val_loss: 0.0253 - val_acc: 0.0000e+00\n",
            "Epoch 269/400\n",
            "72/72 [==============================] - 0s 531us/step - loss: 0.0251 - acc: 0.0139 - val_loss: 0.0250 - val_acc: 0.0000e+00\n",
            "Epoch 270/400\n",
            "72/72 [==============================] - 0s 532us/step - loss: 0.0250 - acc: 0.0139 - val_loss: 0.0252 - val_acc: 0.0000e+00\n",
            "Epoch 271/400\n",
            "72/72 [==============================] - 0s 538us/step - loss: 0.0247 - acc: 0.0139 - val_loss: 0.0258 - val_acc: 0.0000e+00\n",
            "Epoch 272/400\n",
            "72/72 [==============================] - 0s 560us/step - loss: 0.0247 - acc: 0.0139 - val_loss: 0.0265 - val_acc: 0.0000e+00\n",
            "Epoch 273/400\n",
            "72/72 [==============================] - 0s 612us/step - loss: 0.0249 - acc: 0.0139 - val_loss: 0.0268 - val_acc: 0.0000e+00\n",
            "Epoch 274/400\n",
            "72/72 [==============================] - 0s 536us/step - loss: 0.0248 - acc: 0.0139 - val_loss: 0.0261 - val_acc: 0.0000e+00\n",
            "Epoch 275/400\n",
            "72/72 [==============================] - 0s 509us/step - loss: 0.0245 - acc: 0.0139 - val_loss: 0.0251 - val_acc: 0.0000e+00\n",
            "Epoch 276/400\n",
            "72/72 [==============================] - 0s 552us/step - loss: 0.0243 - acc: 0.0139 - val_loss: 0.0243 - val_acc: 0.0000e+00\n",
            "Epoch 277/400\n",
            "72/72 [==============================] - 0s 537us/step - loss: 0.0241 - acc: 0.0139 - val_loss: 0.0240 - val_acc: 0.0000e+00\n",
            "Epoch 278/400\n",
            "72/72 [==============================] - 0s 513us/step - loss: 0.0240 - acc: 0.0139 - val_loss: 0.0238 - val_acc: 0.0000e+00\n",
            "Epoch 279/400\n",
            "72/72 [==============================] - 0s 584us/step - loss: 0.0239 - acc: 0.0139 - val_loss: 0.0232 - val_acc: 0.0000e+00\n",
            "Epoch 280/400\n",
            "72/72 [==============================] - 0s 540us/step - loss: 0.0239 - acc: 0.0139 - val_loss: 0.0230 - val_acc: 0.0000e+00\n",
            "Epoch 281/400\n",
            "72/72 [==============================] - 0s 559us/step - loss: 0.0237 - acc: 0.0139 - val_loss: 0.0234 - val_acc: 0.0000e+00\n",
            "Epoch 282/400\n",
            "72/72 [==============================] - 0s 571us/step - loss: 0.0236 - acc: 0.0139 - val_loss: 0.0238 - val_acc: 0.0000e+00\n",
            "Epoch 283/400\n",
            "72/72 [==============================] - 0s 540us/step - loss: 0.0234 - acc: 0.0139 - val_loss: 0.0238 - val_acc: 0.0000e+00\n",
            "Epoch 284/400\n",
            "72/72 [==============================] - 0s 578us/step - loss: 0.0233 - acc: 0.0139 - val_loss: 0.0238 - val_acc: 0.0000e+00\n",
            "Epoch 285/400\n",
            "72/72 [==============================] - 0s 555us/step - loss: 0.0232 - acc: 0.0139 - val_loss: 0.0237 - val_acc: 0.0000e+00\n",
            "Epoch 286/400\n",
            "72/72 [==============================] - 0s 531us/step - loss: 0.0231 - acc: 0.0139 - val_loss: 0.0235 - val_acc: 0.0000e+00\n",
            "Epoch 287/400\n",
            "72/72 [==============================] - 0s 524us/step - loss: 0.0230 - acc: 0.0139 - val_loss: 0.0236 - val_acc: 0.0000e+00\n",
            "Epoch 288/400\n",
            "72/72 [==============================] - 0s 523us/step - loss: 0.0229 - acc: 0.0139 - val_loss: 0.0238 - val_acc: 0.0000e+00\n",
            "Epoch 289/400\n",
            "72/72 [==============================] - 0s 628us/step - loss: 0.0229 - acc: 0.0139 - val_loss: 0.0240 - val_acc: 0.0000e+00\n",
            "Epoch 290/400\n",
            "72/72 [==============================] - 0s 542us/step - loss: 0.0228 - acc: 0.0139 - val_loss: 0.0234 - val_acc: 0.0000e+00\n",
            "Epoch 291/400\n",
            "72/72 [==============================] - 0s 673us/step - loss: 0.0226 - acc: 0.0139 - val_loss: 0.0226 - val_acc: 0.0000e+00\n",
            "Epoch 292/400\n",
            "72/72 [==============================] - 0s 521us/step - loss: 0.0224 - acc: 0.0139 - val_loss: 0.0222 - val_acc: 0.0000e+00\n",
            "Epoch 293/400\n",
            "72/72 [==============================] - 0s 514us/step - loss: 0.0223 - acc: 0.0139 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
            "Epoch 294/400\n",
            "72/72 [==============================] - 0s 544us/step - loss: 0.0222 - acc: 0.0139 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
            "Epoch 295/400\n",
            "72/72 [==============================] - 0s 613us/step - loss: 0.0221 - acc: 0.0139 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
            "Epoch 296/400\n",
            "72/72 [==============================] - 0s 535us/step - loss: 0.0220 - acc: 0.0139 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
            "Epoch 297/400\n",
            "72/72 [==============================] - 0s 541us/step - loss: 0.0219 - acc: 0.0139 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
            "Epoch 298/400\n",
            "72/72 [==============================] - 0s 513us/step - loss: 0.0219 - acc: 0.0139 - val_loss: 0.0223 - val_acc: 0.0000e+00\n",
            "Epoch 299/400\n",
            "72/72 [==============================] - 0s 671us/step - loss: 0.0218 - acc: 0.0139 - val_loss: 0.0219 - val_acc: 0.0000e+00\n",
            "Epoch 300/400\n",
            "72/72 [==============================] - 0s 717us/step - loss: 0.0216 - acc: 0.0139 - val_loss: 0.0210 - val_acc: 0.0000e+00\n",
            "Epoch 301/400\n",
            "72/72 [==============================] - 0s 520us/step - loss: 0.0214 - acc: 0.0139 - val_loss: 0.0206 - val_acc: 0.0000e+00\n",
            "Epoch 302/400\n",
            "72/72 [==============================] - 0s 513us/step - loss: 0.0213 - acc: 0.0139 - val_loss: 0.0204 - val_acc: 0.0000e+00\n",
            "Epoch 303/400\n",
            "72/72 [==============================] - 0s 513us/step - loss: 0.0213 - acc: 0.0139 - val_loss: 0.0204 - val_acc: 0.0000e+00\n",
            "Epoch 304/400\n",
            "72/72 [==============================] - 0s 526us/step - loss: 0.0212 - acc: 0.0139 - val_loss: 0.0206 - val_acc: 0.0000e+00\n",
            "Epoch 305/400\n",
            "72/72 [==============================] - 0s 517us/step - loss: 0.0211 - acc: 0.0139 - val_loss: 0.0208 - val_acc: 0.0000e+00\n",
            "Epoch 306/400\n",
            "72/72 [==============================] - 0s 607us/step - loss: 0.0210 - acc: 0.0139 - val_loss: 0.0207 - val_acc: 0.0000e+00\n",
            "Epoch 307/400\n",
            "72/72 [==============================] - 0s 578us/step - loss: 0.0209 - acc: 0.0139 - val_loss: 0.0204 - val_acc: 0.0000e+00\n",
            "Epoch 308/400\n",
            "72/72 [==============================] - 0s 522us/step - loss: 0.0208 - acc: 0.0139 - val_loss: 0.0204 - val_acc: 0.0000e+00\n",
            "Epoch 309/400\n",
            "72/72 [==============================] - 0s 624us/step - loss: 0.0207 - acc: 0.0139 - val_loss: 0.0199 - val_acc: 0.0000e+00\n",
            "Epoch 310/400\n",
            "72/72 [==============================] - 0s 520us/step - loss: 0.0206 - acc: 0.0139 - val_loss: 0.0191 - val_acc: 0.0000e+00\n",
            "Epoch 311/400\n",
            "72/72 [==============================] - 0s 564us/step - loss: 0.0206 - acc: 0.0139 - val_loss: 0.0188 - val_acc: 0.0000e+00\n",
            "Epoch 312/400\n",
            "72/72 [==============================] - 0s 562us/step - loss: 0.0206 - acc: 0.0139 - val_loss: 0.0187 - val_acc: 0.0000e+00\n",
            "Epoch 313/400\n",
            "72/72 [==============================] - 0s 516us/step - loss: 0.0205 - acc: 0.0139 - val_loss: 0.0193 - val_acc: 0.0000e+00\n",
            "Epoch 314/400\n",
            "72/72 [==============================] - 0s 528us/step - loss: 0.0203 - acc: 0.0139 - val_loss: 0.0195 - val_acc: 0.0000e+00\n",
            "Epoch 315/400\n",
            "72/72 [==============================] - 0s 634us/step - loss: 0.0202 - acc: 0.0139 - val_loss: 0.0193 - val_acc: 0.0000e+00\n",
            "Epoch 316/400\n",
            "72/72 [==============================] - 0s 599us/step - loss: 0.0201 - acc: 0.0139 - val_loss: 0.0193 - val_acc: 0.0000e+00\n",
            "Epoch 317/400\n",
            "72/72 [==============================] - 0s 551us/step - loss: 0.0200 - acc: 0.0139 - val_loss: 0.0195 - val_acc: 0.0000e+00\n",
            "Epoch 318/400\n",
            "72/72 [==============================] - 0s 503us/step - loss: 0.0200 - acc: 0.0139 - val_loss: 0.0194 - val_acc: 0.0000e+00\n",
            "Epoch 319/400\n",
            "72/72 [==============================] - 0s 553us/step - loss: 0.0199 - acc: 0.0139 - val_loss: 0.0188 - val_acc: 0.0000e+00\n",
            "Epoch 320/400\n",
            "72/72 [==============================] - 0s 575us/step - loss: 0.0198 - acc: 0.0139 - val_loss: 0.0180 - val_acc: 0.0000e+00\n",
            "Epoch 321/400\n",
            "72/72 [==============================] - 0s 528us/step - loss: 0.0198 - acc: 0.0139 - val_loss: 0.0175 - val_acc: 0.0000e+00\n",
            "Epoch 322/400\n",
            "72/72 [==============================] - 0s 577us/step - loss: 0.0198 - acc: 0.0139 - val_loss: 0.0172 - val_acc: 0.0000e+00\n",
            "Epoch 323/400\n",
            "72/72 [==============================] - 0s 512us/step - loss: 0.0198 - acc: 0.0139 - val_loss: 0.0173 - val_acc: 0.0000e+00\n",
            "Epoch 324/400\n",
            "72/72 [==============================] - 0s 536us/step - loss: 0.0197 - acc: 0.0139 - val_loss: 0.0178 - val_acc: 0.0000e+00\n",
            "Epoch 325/400\n",
            "72/72 [==============================] - 0s 524us/step - loss: 0.0195 - acc: 0.0139 - val_loss: 0.0184 - val_acc: 0.0000e+00\n",
            "Epoch 326/400\n",
            "72/72 [==============================] - 0s 538us/step - loss: 0.0193 - acc: 0.0139 - val_loss: 0.0189 - val_acc: 0.0000e+00\n",
            "Epoch 327/400\n",
            "72/72 [==============================] - 0s 610us/step - loss: 0.0194 - acc: 0.0139 - val_loss: 0.0188 - val_acc: 0.0000e+00\n",
            "Epoch 328/400\n",
            "72/72 [==============================] - 0s 548us/step - loss: 0.0193 - acc: 0.0139 - val_loss: 0.0182 - val_acc: 0.0000e+00\n",
            "Epoch 329/400\n",
            "72/72 [==============================] - 0s 514us/step - loss: 0.0192 - acc: 0.0139 - val_loss: 0.0174 - val_acc: 0.0000e+00\n",
            "Epoch 330/400\n",
            "72/72 [==============================] - 0s 568us/step - loss: 0.0191 - acc: 0.0139 - val_loss: 0.0171 - val_acc: 0.0000e+00\n",
            "Epoch 331/400\n",
            "72/72 [==============================] - 0s 532us/step - loss: 0.0192 - acc: 0.0139 - val_loss: 0.0167 - val_acc: 0.0000e+00\n",
            "Epoch 332/400\n",
            "72/72 [==============================] - 0s 551us/step - loss: 0.0191 - acc: 0.0139 - val_loss: 0.0168 - val_acc: 0.0000e+00\n",
            "Epoch 333/400\n",
            "72/72 [==============================] - 0s 555us/step - loss: 0.0190 - acc: 0.0139 - val_loss: 0.0168 - val_acc: 0.0000e+00\n",
            "Epoch 334/400\n",
            "72/72 [==============================] - 0s 508us/step - loss: 0.0189 - acc: 0.0139 - val_loss: 0.0167 - val_acc: 0.0000e+00\n",
            "Epoch 335/400\n",
            "72/72 [==============================] - 0s 557us/step - loss: 0.0189 - acc: 0.0139 - val_loss: 0.0165 - val_acc: 0.0000e+00\n",
            "Epoch 336/400\n",
            "72/72 [==============================] - 0s 539us/step - loss: 0.0187 - acc: 0.0139 - val_loss: 0.0169 - val_acc: 0.0000e+00\n",
            "Epoch 337/400\n",
            "72/72 [==============================] - 0s 531us/step - loss: 0.0186 - acc: 0.0139 - val_loss: 0.0173 - val_acc: 0.0000e+00\n",
            "Epoch 338/400\n",
            "72/72 [==============================] - 0s 635us/step - loss: 0.0185 - acc: 0.0139 - val_loss: 0.0171 - val_acc: 0.0000e+00\n",
            "Epoch 339/400\n",
            "72/72 [==============================] - 0s 524us/step - loss: 0.0185 - acc: 0.0139 - val_loss: 0.0167 - val_acc: 0.0000e+00\n",
            "Epoch 340/400\n",
            "72/72 [==============================] - 0s 612us/step - loss: 0.0184 - acc: 0.0139 - val_loss: 0.0162 - val_acc: 0.0000e+00\n",
            "Epoch 341/400\n",
            "72/72 [==============================] - 0s 586us/step - loss: 0.0185 - acc: 0.0139 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 342/400\n",
            "72/72 [==============================] - 0s 542us/step - loss: 0.0184 - acc: 0.0139 - val_loss: 0.0156 - val_acc: 0.0000e+00\n",
            "Epoch 343/400\n",
            "72/72 [==============================] - 0s 623us/step - loss: 0.0183 - acc: 0.0139 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 344/400\n",
            "72/72 [==============================] - 0s 522us/step - loss: 0.0182 - acc: 0.0139 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 345/400\n",
            "72/72 [==============================] - 0s 530us/step - loss: 0.0181 - acc: 0.0139 - val_loss: 0.0158 - val_acc: 0.0000e+00\n",
            "Epoch 346/400\n",
            "72/72 [==============================] - 0s 572us/step - loss: 0.0180 - acc: 0.0139 - val_loss: 0.0160 - val_acc: 0.0000e+00\n",
            "Epoch 347/400\n",
            "72/72 [==============================] - 0s 553us/step - loss: 0.0180 - acc: 0.0139 - val_loss: 0.0160 - val_acc: 0.0000e+00\n",
            "Epoch 348/400\n",
            "72/72 [==============================] - 0s 541us/step - loss: 0.0179 - acc: 0.0139 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 349/400\n",
            "72/72 [==============================] - 0s 519us/step - loss: 0.0178 - acc: 0.0139 - val_loss: 0.0151 - val_acc: 0.0000e+00\n",
            "Epoch 350/400\n",
            "72/72 [==============================] - 0s 578us/step - loss: 0.0178 - acc: 0.0139 - val_loss: 0.0151 - val_acc: 0.0000e+00\n",
            "Epoch 351/400\n",
            "72/72 [==============================] - 0s 529us/step - loss: 0.0178 - acc: 0.0139 - val_loss: 0.0153 - val_acc: 0.0000e+00\n",
            "Epoch 352/400\n",
            "72/72 [==============================] - 0s 540us/step - loss: 0.0177 - acc: 0.0139 - val_loss: 0.0152 - val_acc: 0.0000e+00\n",
            "Epoch 353/400\n",
            "72/72 [==============================] - 0s 590us/step - loss: 0.0177 - acc: 0.0139 - val_loss: 0.0151 - val_acc: 0.0000e+00\n",
            "Epoch 354/400\n",
            "72/72 [==============================] - 0s 508us/step - loss: 0.0176 - acc: 0.0139 - val_loss: 0.0151 - val_acc: 0.0000e+00\n",
            "Epoch 355/400\n",
            "72/72 [==============================] - 0s 506us/step - loss: 0.0176 - acc: 0.0139 - val_loss: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 356/400\n",
            "72/72 [==============================] - 0s 556us/step - loss: 0.0175 - acc: 0.0139 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
            "Epoch 357/400\n",
            "72/72 [==============================] - 0s 544us/step - loss: 0.0175 - acc: 0.0139 - val_loss: 0.0157 - val_acc: 0.0000e+00\n",
            "Epoch 358/400\n",
            "72/72 [==============================] - 0s 542us/step - loss: 0.0175 - acc: 0.0139 - val_loss: 0.0161 - val_acc: 0.0000e+00\n",
            "Epoch 359/400\n",
            "72/72 [==============================] - 0s 598us/step - loss: 0.0176 - acc: 0.0139 - val_loss: 0.0162 - val_acc: 0.0000e+00\n",
            "Epoch 360/400\n",
            "72/72 [==============================] - 0s 515us/step - loss: 0.0176 - acc: 0.0139 - val_loss: 0.0158 - val_acc: 0.0000e+00\n",
            "Epoch 361/400\n",
            "72/72 [==============================] - 0s 549us/step - loss: 0.0174 - acc: 0.0139 - val_loss: 0.0150 - val_acc: 0.0000e+00\n",
            "Epoch 362/400\n",
            "72/72 [==============================] - 0s 541us/step - loss: 0.0172 - acc: 0.0139 - val_loss: 0.0143 - val_acc: 0.0000e+00\n",
            "Epoch 363/400\n",
            "72/72 [==============================] - 0s 503us/step - loss: 0.0171 - acc: 0.0139 - val_loss: 0.0137 - val_acc: 0.0000e+00\n",
            "Epoch 364/400\n",
            "72/72 [==============================] - 0s 601us/step - loss: 0.0171 - acc: 0.0139 - val_loss: 0.0136 - val_acc: 0.0000e+00\n",
            "Epoch 365/400\n",
            "72/72 [==============================] - 0s 638us/step - loss: 0.0171 - acc: 0.0139 - val_loss: 0.0135 - val_acc: 0.0000e+00\n",
            "Epoch 366/400\n",
            "72/72 [==============================] - 0s 598us/step - loss: 0.0170 - acc: 0.0139 - val_loss: 0.0136 - val_acc: 0.0000e+00\n",
            "Epoch 367/400\n",
            "72/72 [==============================] - 0s 500us/step - loss: 0.0169 - acc: 0.0139 - val_loss: 0.0138 - val_acc: 0.0000e+00\n",
            "Epoch 368/400\n",
            "72/72 [==============================] - 0s 528us/step - loss: 0.0169 - acc: 0.0139 - val_loss: 0.0138 - val_acc: 0.0000e+00\n",
            "Epoch 369/400\n",
            "72/72 [==============================] - 0s 532us/step - loss: 0.0168 - acc: 0.0139 - val_loss: 0.0136 - val_acc: 0.0000e+00\n",
            "Epoch 370/400\n",
            "72/72 [==============================] - 0s 593us/step - loss: 0.0168 - acc: 0.0139 - val_loss: 0.0136 - val_acc: 0.0000e+00\n",
            "Epoch 371/400\n",
            "72/72 [==============================] - 0s 519us/step - loss: 0.0167 - acc: 0.0139 - val_loss: 0.0136 - val_acc: 0.0000e+00\n",
            "Epoch 372/400\n",
            "72/72 [==============================] - 0s 544us/step - loss: 0.0167 - acc: 0.0139 - val_loss: 0.0138 - val_acc: 0.0000e+00\n",
            "Epoch 373/400\n",
            "72/72 [==============================] - 0s 539us/step - loss: 0.0167 - acc: 0.0139 - val_loss: 0.0137 - val_acc: 0.0000e+00\n",
            "Epoch 374/400\n",
            "72/72 [==============================] - 0s 561us/step - loss: 0.0166 - acc: 0.0139 - val_loss: 0.0136 - val_acc: 0.0000e+00\n",
            "Epoch 375/400\n",
            "72/72 [==============================] - 0s 503us/step - loss: 0.0165 - acc: 0.0139 - val_loss: 0.0134 - val_acc: 0.0000e+00\n",
            "Epoch 376/400\n",
            "72/72 [==============================] - 0s 558us/step - loss: 0.0165 - acc: 0.0139 - val_loss: 0.0131 - val_acc: 0.0000e+00\n",
            "Epoch 377/400\n",
            "72/72 [==============================] - 0s 574us/step - loss: 0.0164 - acc: 0.0139 - val_loss: 0.0129 - val_acc: 0.0000e+00\n",
            "Epoch 378/400\n",
            "72/72 [==============================] - 0s 527us/step - loss: 0.0164 - acc: 0.0139 - val_loss: 0.0128 - val_acc: 0.0000e+00\n",
            "Epoch 379/400\n",
            "72/72 [==============================] - 0s 565us/step - loss: 0.0163 - acc: 0.0139 - val_loss: 0.0127 - val_acc: 0.0000e+00\n",
            "Epoch 380/400\n",
            "72/72 [==============================] - 0s 522us/step - loss: 0.0163 - acc: 0.0139 - val_loss: 0.0126 - val_acc: 0.0000e+00\n",
            "Epoch 381/400\n",
            "72/72 [==============================] - 0s 538us/step - loss: 0.0165 - acc: 0.0139 - val_loss: 0.0125 - val_acc: 0.0000e+00\n",
            "Epoch 382/400\n",
            "72/72 [==============================] - 0s 534us/step - loss: 0.0164 - acc: 0.0139 - val_loss: 0.0125 - val_acc: 0.0000e+00\n",
            "Epoch 383/400\n",
            "72/72 [==============================] - 0s 554us/step - loss: 0.0162 - acc: 0.0139 - val_loss: 0.0124 - val_acc: 0.0000e+00\n",
            "Epoch 384/400\n",
            "72/72 [==============================] - 0s 550us/step - loss: 0.0162 - acc: 0.0139 - val_loss: 0.0124 - val_acc: 0.0000e+00\n",
            "Epoch 385/400\n",
            "72/72 [==============================] - 0s 527us/step - loss: 0.0161 - acc: 0.0139 - val_loss: 0.0123 - val_acc: 0.0000e+00\n",
            "Epoch 386/400\n",
            "72/72 [==============================] - 0s 511us/step - loss: 0.0161 - acc: 0.0139 - val_loss: 0.0123 - val_acc: 0.0000e+00\n",
            "Epoch 387/400\n",
            "72/72 [==============================] - 0s 624us/step - loss: 0.0160 - acc: 0.0139 - val_loss: 0.0122 - val_acc: 0.0000e+00\n",
            "Epoch 388/400\n",
            "72/72 [==============================] - 0s 546us/step - loss: 0.0160 - acc: 0.0139 - val_loss: 0.0121 - val_acc: 0.0000e+00\n",
            "Epoch 389/400\n",
            "72/72 [==============================] - 0s 529us/step - loss: 0.0160 - acc: 0.0139 - val_loss: 0.0121 - val_acc: 0.0000e+00\n",
            "Epoch 390/400\n",
            "72/72 [==============================] - 0s 528us/step - loss: 0.0160 - acc: 0.0139 - val_loss: 0.0121 - val_acc: 0.0000e+00\n",
            "Epoch 391/400\n",
            "72/72 [==============================] - 0s 669us/step - loss: 0.0160 - acc: 0.0139 - val_loss: 0.0124 - val_acc: 0.0000e+00\n",
            "Epoch 392/400\n",
            "72/72 [==============================] - 0s 583us/step - loss: 0.0160 - acc: 0.0139 - val_loss: 0.0127 - val_acc: 0.0000e+00\n",
            "Epoch 393/400\n",
            "72/72 [==============================] - 0s 537us/step - loss: 0.0161 - acc: 0.0139 - val_loss: 0.0127 - val_acc: 0.0000e+00\n",
            "Epoch 394/400\n",
            "72/72 [==============================] - 0s 525us/step - loss: 0.0160 - acc: 0.0139 - val_loss: 0.0123 - val_acc: 0.0000e+00\n",
            "Epoch 395/400\n",
            "72/72 [==============================] - 0s 504us/step - loss: 0.0158 - acc: 0.0139 - val_loss: 0.0122 - val_acc: 0.0000e+00\n",
            "Epoch 396/400\n",
            "72/72 [==============================] - 0s 534us/step - loss: 0.0161 - acc: 0.0139 - val_loss: 0.0122 - val_acc: 0.0000e+00\n",
            "Epoch 397/400\n",
            "72/72 [==============================] - 0s 582us/step - loss: 0.0162 - acc: 0.0139 - val_loss: 0.0122 - val_acc: 0.0000e+00\n",
            "Epoch 398/400\n",
            "72/72 [==============================] - 0s 537us/step - loss: 0.0159 - acc: 0.0139 - val_loss: 0.0122 - val_acc: 0.0000e+00\n",
            "Epoch 399/400\n",
            "72/72 [==============================] - 0s 599us/step - loss: 0.0159 - acc: 0.0139 - val_loss: 0.0122 - val_acc: 0.0000e+00\n",
            "Epoch 400/400\n",
            "72/72 [==============================] - 0s 521us/step - loss: 0.0158 - acc: 0.0139 - val_loss: 0.0122 - val_acc: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8scoJgV_w2VV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hDEK7Ri0foc",
        "colab_type": "code",
        "outputId": "2bf8fa33-e985-412c-b1ba-928488ef122f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 5, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-AyYJdBw-uG",
        "colab_type": "code",
        "outputId": "e29bfb99-b652-4df9-b765-f951e4d7277f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "plt.scatter(range(20), results, c='r')\n",
        "plt.scatter(range(20), y_test, c='g')\n",
        "plt.show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U2uPmrNxN0t",
        "colab_type": "code",
        "outputId": "24d3cba8-a112-4682-f6da-fbd59af7a858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.show()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUpDKXUQkBf1/BHAN4rwzts0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUR3sN-uzX3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_input = [[[10], [11], [12], [13], [14]]]\n",
        "norm_test_input = np.array(test_input, dtype=float)\n",
        "norm_test_input = norm_test_input/100\n",
        "results = model.predict(norm_test_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WzbLqWY0zJ0",
        "colab_type": "code",
        "outputId": "788bc8e5-d80f-4133-d307-0051bd50498d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(results*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[14.86734]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}