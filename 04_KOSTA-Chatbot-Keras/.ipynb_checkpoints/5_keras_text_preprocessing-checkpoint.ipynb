{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "eoOlIj_3k_E0",
    "outputId": "4fb70017-a0cf-40c8-ef84-e8c7aacea685"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfrom keras.preprocessing.text import Tokenizer\\ntokenizer = Tokenizer(oov_token=\"<UNK>\") \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(oov_token=\"<UNK>\")  # num_words parameter, oov_token=\"<UNK>\"\n",
    "\n",
    "\"\"\"\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(oov_token=\"<UNK>\") \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "wHOdCETflYZA",
    "outputId": "c743bbbc-4fda-4361-8940-76f6354e20ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 6, 2], [3, 7, 4], [2, 8, 4, 9, 3]]\n"
     ]
    }
   ],
   "source": [
    "text = [\"I am Tom\", \"You are Jane\", \"Tom and Jane love you\"]\n",
    "tokenizer.fit_on_texts(text)  # lower boolean parameter\n",
    "sequences = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ONj1Oe7gmZvZ",
    "outputId": "349889c0-c9d3-4a4d-e179-46d9e73b9a0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 9, 1]]\n"
     ]
    }
   ],
   "source": [
    "sample = tokenizer.texts_to_sequences([\"I love him\"])\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jD_0_h9ynJMu",
    "outputId": "e23d44c9-4ac5-4dd9-d528-5a3c18988d96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<UNK>': 1, 'tom': 2, 'you': 3, 'jane': 4, 'i': 5, 'am': 6, 'are': 7, 'and': 8, 'love': 9}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "KWRpLOe7oVXo",
    "outputId": "7d8b8084-fdc3-438c-e9b6-1300b4a030ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '<UNK>', 2: 'tom', 3: 'you', 4: 'jane', 5: 'i', 6: 'am', 7: 'are', 8: 'and', 9: 'love'}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "tCP0jYHuobUF",
    "outputId": "a4658de2-d6da-4196-e409-3bdb25301384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i love <UNK>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.sequences_to_texts(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "p2pYqLpToqRP",
    "outputId": "f29abf18-3d74-4434-d13a-7ada20a2245e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 6, 2, 0, 0],\n",
       "       [3, 7, 4, 0, 0],\n",
       "       [2, 8, 4, 9, 3]], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.preprocessing.sequence.pad_sequences(sequences, padding='post', maxlen=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qJvT4ZAQREV4"
   },
   "source": [
    "## One Hot Encoding Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "s7AFVjB_etSP",
    "outputId": "1e55f074-c20b-4dec-d42b-bc07da507a7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 9, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E-jnp-51RDz4"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "sample_encoded = to_categorical(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "5K11IC2SRoGr",
    "outputId": "0bb49a3b-253f-45d8-e6ac-136fad63851e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 9, 1]]\n",
      "[[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# print sample array\n",
    "print(sample)\n",
    "\n",
    "# print one-hot encoded sample array \n",
    "print(sample_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K62rUROeBqwi"
   },
   "source": [
    "이렇게 만들어진 tokenizer는 매번 새로 계산하기 보다는 결과를 저장하고 다음에 필요할때 로드해서 쓰는 것이 일반적인 패턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ky1fWra_BqCE"
   },
   "outputs": [],
   "source": [
    "# tokenizer의 내용을 json으로 받아서 디스크에 저장\n",
    "text_to_save = tokenizer.to_json()\n",
    "\n",
    "# 디스크에 저장된 json을 읽어서 tokenizer_from_json로 지정해서 tokenizer 생성\n",
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "tokenizer = tokenizer_from_json(text_to_save)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "(KOSTA) keras text preprocessing의 사본",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
