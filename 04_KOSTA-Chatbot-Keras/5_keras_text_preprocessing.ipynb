{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(KOSTA) keras text preprocessing의 사본",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoOlIj_3k_E0",
        "colab_type": "code",
        "outputId": "4fb70017-a0cf-40c8-ef84-e8c7aacea685",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import keras\n",
        "\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(oov_token=\"<UNK>\")  # num_words parameter, oov_token=\"<UNK>\"\n",
        "\n",
        "\"\"\"\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer(oov_token=\"<UNK>\") \n",
        "\"\"\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom keras.preprocessing.text import Tokenizer\\ntokenizer = Tokenizer(oov_token=\"<UNK>\") \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHOdCETflYZA",
        "colab_type": "code",
        "outputId": "c743bbbc-4fda-4361-8940-76f6354e20ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "text = [\"I am Tom\", \"You are Jane\", \"Tom and Jane love you\"]\n",
        "tokenizer.fit_on_texts(text)  # lower boolean parameter\n",
        "sequences = tokenizer.texts_to_sequences(text)\n",
        "\n",
        "print(sequences)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5, 6, 2], [3, 7, 4], [2, 8, 4, 9, 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONj1Oe7gmZvZ",
        "colab_type": "code",
        "outputId": "349889c0-c9d3-4a4d-e179-46d9e73b9a0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sample = tokenizer.texts_to_sequences([\"I love him\"])\n",
        "print(sample)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5, 9, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD_0_h9ynJMu",
        "colab_type": "code",
        "outputId": "e23d44c9-4ac5-4dd9-d528-5a3c18988d96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<UNK>': 1, 'tom': 2, 'you': 3, 'jane': 4, 'i': 5, 'am': 6, 'are': 7, 'and': 8, 'love': 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWRpLOe7oVXo",
        "colab_type": "code",
        "outputId": "7d8b8084-fdc3-438c-e9b6-1300b4a030ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(tokenizer.index_word)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: '<UNK>', 2: 'tom', 3: 'you', 4: 'jane', 5: 'i', 6: 'am', 7: 'are', 8: 'and', 9: 'love'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCP0jYHuobUF",
        "colab_type": "code",
        "outputId": "a4658de2-d6da-4196-e409-3bdb25301384",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(tokenizer.sequences_to_texts(sample))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i love <UNK>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2pYqLpToqRP",
        "colab_type": "code",
        "outputId": "f29abf18-3d74-4434-d13a-7ada20a2245e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "keras.preprocessing.sequence.pad_sequences(sequences, padding='post', maxlen=5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5, 6, 2, 0, 0],\n",
              "       [3, 7, 4, 0, 0],\n",
              "       [2, 8, 4, 9, 3]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJvT4ZAQREV4",
        "colab_type": "text"
      },
      "source": [
        "## One Hot Encoding Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7AFVjB_etSP",
        "colab_type": "code",
        "outputId": "1e55f074-c20b-4dec-d42b-bc07da507a7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(sample)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5, 9, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-jnp-51RDz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "sample_encoded = to_categorical(sample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K11IC2SRoGr",
        "colab_type": "code",
        "outputId": "0bb49a3b-253f-45d8-e6ac-136fad63851e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# print sample array\n",
        "print(sample)\n",
        "\n",
        "# print one-hot encoded sample array \n",
        "print(sample_encoded)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5, 9, 1]]\n",
            "[[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K62rUROeBqwi",
        "colab_type": "text"
      },
      "source": [
        "이렇게 만들어진 tokenizer는 매번 새로 계산하기 보다는 결과를 저장하고 다음에 필요할때 로드해서 쓰는 것이 일반적인 패턴"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky1fWra_BqCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenizer의 내용을 json으로 받아서 디스크에 저장\n",
        "text_to_save = tokenizer.to_json()\n",
        "\n",
        "# 디스크에 저장된 json을 읽어서 tokenizer_from_json로 지정해서 tokenizer 생성\n",
        "from keras_preprocessing.text import tokenizer_from_json\n",
        "tokenizer = tokenizer_from_json(text_to_save)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}