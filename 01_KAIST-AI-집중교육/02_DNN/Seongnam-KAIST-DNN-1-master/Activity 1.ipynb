{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation systems with Deep neural network\n",
    "\n",
    "### **2019/3/22 성남-KAIST 인공지능 집중교육과정**\n",
    "\n",
    "***Tip> shotcuts for Jupyter Notebook***\n",
    "* Shift + Enter : run cell and select below\n",
    "\n",
    "#### Objective> Train deep neural network (autoencoder) to complete movie rating matrix\n",
    "<img src=\"img/fig1.png\" alt=\"fig1\" width=\"700\"/>\n",
    "<center>Fig. 1 Item-based autoencoder </center>\n",
    "\n",
    "- #### Loss function\n",
    "\n",
    "$$L(M, \\hat{M})=\\sum_{(i,j)\\in E}(M_{ij}-\\hat{M}_{ij})^2 + \\lambda\\sum_{i=1}^{3}\\lVert W_i\\rVert^2_2$$\n",
    "<br/>\n",
    "- #### Update weight and bias\n",
    "$$\\underset{W, b}{\\text{argmin}}\\hspace{0.2em} L(M, \\hat{M})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-16c26608d072>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare data\n",
    "### MovieLens Dataset (<a href=https://grouplens.org/datasets/movielens/>ref.</a>)\n",
    "We use \"MovieLens Latest Datasets\" consisting of 100,000 ratings applied to 9,000 movies by 600 users. Last updated 9/2018.\n",
    "### Upload the data to Google server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch MovieLens data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = pd.read_csv('ratings.csv')\n",
    "rating.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings statistics\n",
    "Count the number of movies with identical rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating.set_index([\"userId\", \"timestamp\",\"rating\"]).count(level=\"rating\").rename({'movieId': 'The number of movies'}, axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of users and movies and check the sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_user = len(rating['userId'].unique())\n",
    "n_movie = len(rating['movieId'].unique())\n",
    "n_rating = len(rating['rating'])\n",
    "print(\"[*] %d users & %d movies\" % (n_user, n_movie))\n",
    "print(\"[*] Sparsity: %.2f%%\" % (n_rating / (n_user * n_movie) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie list\n",
    "See the movie list including movies' title and genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movielist = pd.read_csv('movies.csv')\n",
    "movielist.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop **\"timestamp\"** which looks useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating.drop(['timestamp'], axis=1, inplace=True)\n",
    "rating.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale **\"movieId\"** in between 0 and 9741, **\"userId\"** in between 0 and 609"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating['movieId'], _ = pd.factorize(rating['movieId'])\n",
    "rating['userId'], _ = pd.factorize(rating['userId'])\n",
    "rating.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-based autoencoder\n",
    "Transpose the rating matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = rating[['movieId', 'userId', 'rating']]\n",
    "rating.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the ratings for training and test\n",
    "Training : Test = 9 : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIdx = np.random.choice(range(n_rating), int(n_rating * 0.9), replace=False)\n",
    "dataTrain = rating.iloc[trainIdx]\n",
    "\n",
    "testIdx = np.setdiff1d(range(n_rating), trainIdx)\n",
    "dataTest = rating.iloc[testIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingTrain = np.asarray(dataTrain)\n",
    "ratingTest = np.asarray(dataTest)\n",
    "d1, d2 = np.max(ratingTrain[:, 0]) + 1, np.max(ratingTrain[:, 1] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a Graph\n",
    "We use \"tf.sparse_tensor_dense_matmul()\" function instead of  \"tf.layers.dense( )\" function, because of the sparse input and regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(_X, _units, _l2_lambda, _n_ratings):\n",
    "    w_init = w_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    b_init = tf.constant_initializer(0.)\n",
    "    \n",
    "    ## Encoder\n",
    "    '1st Hidden layer'\n",
    "    w1 = tf.get_variable('weight1', [d2, _units[0]], initializer=w_init)\n",
    "    b1 = tf.get_variable('biases1', [_units[0]], initializer=b_init)\n",
    "    h1 = tf.sparse_tensor_dense_matmul(_X, w1) + b1\n",
    "    h1 = tf.nn.relu(h1)\n",
    "\n",
    "    '2nd Hidden layer'\n",
    "    w2 = tf.get_variable('weight2', [_units[0], _units[1]], initializer=w_init)\n",
    "    b2 = tf.get_variable('biases2', [_units[1]], initializer=b_init)\n",
    "    h2 = tf.matmul(h1, w2) + b2\n",
    "    h2 = tf.nn.sigmoid(h2)\n",
    "    \n",
    "    ## Decoder\n",
    "    w3 = tf.get_variable('weight3', [_units[1], d2], initializer=w_init)\n",
    "    \n",
    "    yhat = tf.matmul(h2, w3)\n",
    "    out = tf.gather_nd(yhat, _X.indices)\n",
    "\n",
    "    loss = tf.reduce_sum(tf.pow(out - _X.values, 2)) / _n_ratings\n",
    "    \n",
    "    \n",
    "    ''' L2 regularization '''\n",
    "    all_var = [var for var in tf.trainable_variables() ]\n",
    "    l2_losses = []\n",
    "    for var in all_var:\n",
    "        if var.op.name.find('weight') == 0:\n",
    "            l2_losses.append(tf.nn.l2_loss(var))\n",
    "    \n",
    "    losses = loss + _l2_lambda * tf.reduce_sum(l2_losses)\n",
    "    \n",
    "    return yhat, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set hyperparameters\n",
    "- ***n_epochs*** : The number of epochs\n",
    "- ***lr*** : Learning rate for gradient descent\n",
    "- ***l2_lambda*** : regularization parameter\n",
    "- ***n_units*** : The number of units for each hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"parameters\"\"\"\n",
    "n_epochs = 1000\n",
    "lr = 0.1\n",
    "l2_lambda = 0.003\n",
    "n_units = [100, 50]\n",
    "n_ratings = len(ratingTrain)\n",
    "display_step = n_epochs / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder for sparse input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "X = tf.sparse_placeholder(dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the GradientDescentOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, cost = autoencoder(X, n_units, l2_lambda, n_ratings)\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr).minimize(cost, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a tensorflow session\n",
    "Tensorflow operations must be executed in the session. The only one session is activated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"START OPTIMIZATION\\n\")\n",
    "start_time = time.time()\n",
    "losses = []\n",
    "for epoch in  range(n_epochs + 1):\n",
    "    feed = {X: (ratingTrain[:, 0:2], ratingTrain[:, 2], [d1, d2])}\n",
    "    _, avg_cost = sess.run((optimizer, cost), feed_dict = feed)\n",
    "    losses.append(np.sqrt(avg_cost))\n",
    "\n",
    "    # DISPLAY\n",
    "    if epoch % display_step == 0:\n",
    "        duration = float(time.time() - start_time)\n",
    "        print(\" [*] Epoch: %05d/%05d cost: %2e (duration: %.3fs)\" % (epoch, n_epochs, np.sqrt(avg_cost), duration))\n",
    "        start_time = time.time()\n",
    "print(\"\\nOptimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.title(\"Learning curve\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Epochs\", fontsize=14, fontweight='bold')\n",
    "plt.ylabel(\"RMSE of training set\", fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed = {X: (ratingTrain[:, 0:2], ratingTrain[:, 2], [d1, d2])}\n",
    "Pred = sess.run(pred, feed_dict=feed)\n",
    "\n",
    "idxTest = (ratingTest[:, 0].astype(int), ratingTest[:, 1].astype(int))\n",
    "idxTrain = (ratingTrain[:, 0].astype(int), ratingTrain[:, 1].astype(int))\n",
    "\n",
    "RMSE_Test = np.sqrt(np.sum((Pred[idxTest] - ratingTest[:, 2]) ** 2) / len(ratingTest[:, 0]))\n",
    "RMSE_Train = np.sqrt(np.linalg.norm(Pred[idxTrain] - ratingTrain[:, 2]) ** 2 / len(ratingTrain[:, 0]))\n",
    "\n",
    "print(\"[*] RMSE Test: %.4e\" % RMSE_Test)\n",
    "print(\"[*] RMSE Train %.4e\" % RMSE_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report</br>\n",
    "\n",
    "### 1. Momentum Optimizer\n",
    "Use the \"MomentumOptimizer( )\" instaed of the GradientDescentOptimizer and compare the RMSE learning curves of the two optimizers. When you use MomentumOptimizer, set the momuentum at 0.9 and adjust the learning rate.\n",
    "\n",
    "### 2. Batch normalization\n",
    "Apply \"batch normalization\" to the 1st and 2nd hidden layers, and compare the resulting RMSE learning curves with those obtained above.<br/>\n",
    "*Hint)* tf.layers.batch_normalization( )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
